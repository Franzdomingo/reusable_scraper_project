[
  {
    "name": "qwen3-next-80b-a3b-thinking",
    "nvidia_url": "https://build.nvidia.com/qwen/qwen3-next-80b-a3b-thinking",
    "scraped_on": "2025-10-15T21:38:20.613477",
    "tags": [
      "reasoning",
      "text-to-text"
    ],
    "model_card": "Qwen3-Next-80B-A3B-Thinking =========================== Description =========== Qwen3-Next-80B-A3B-Thinkingis a part of the Qwen3-Next series that features the following key enchancements: Qwen3-Next-80B-A3B-Thinking • Hybrid Attention: Replaces standard attention with the combination ofGated DeltaNetandGated Attention, enabling efficient context modeling for ultra-long context length. Hybrid Attention Gated DeltaNet Gated Attention • High-Sparsity Mixture-of-Experts (MoE): Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity. High-Sparsity Mixture-of-Experts (MoE) • Stability Optimizations: Includes techniques such aszero-centered and weight-decayed layernorm, and other stabilizing enhancements for robust pre-training and post-training. Stability Optimizations zero-centered and weight-decayed layernorm • Multi-Token Prediction (MTP): Boosts pretraining model performance and accelerates inference. Multi-Token Prediction (MTP) For more details, please refer to theQwen3-Nextblog post. Qwen3-Next (https://qwen.ai/blog?id=e34c4305036ce60d55a0791b170337c2b70ae51d&from=home.latest-research-list) This model is ready for commercial/non-commercial use. Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIA model card here:Qwen3-Next-80B-A3B-Thinking. Qwen3-Next-80B-A3B-Thinking (https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking) License and Terms of Use: ========================= GOVERNING TERMS:The trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:Apache 2.0 License. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache 2.0 License (https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking/blob/main/LICENSE) Deployment Geography: ===================== Global Use Case: ========= Qwen3-Next-80B-A3B-Thinking excels in tool calling capabilities and highly complex reasoning tasks. Release Date: ============= build.nvidia.com: September 11, 2025 viaQwen3-Next-80B-A3B-ThinkingHugging Face: September 11, 2025 viaQwen3-Next-80B-A3B-Thinking build.nvidia.com Qwen3-Next-80B-A3B-Thinking (https://build.nvidia.com/qwen/qwen3-next-80b-a3b-thinking) Hugging Face Qwen3-Next-80B-A3B-Thinking (https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking) Reference(s): ============= References: • Qwen3-Next-80B-A3B-Thinking Qwen3-Next-80B-A3B-Thinking (https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking) • Qwen3-Next-80B-A3B-Instruct Qwen3-Next-80B-A3B-Instruct (https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct) • Qwen3-Next: Towards Ultimate Training & Inference Efficiency Qwen3-Next: Towards Ultimate Training & Inference Efficiency (https://qwen.ai/blog?id=e34c4305036ce60d55a0791b170337c2b70ae51d&from=home.latest-research-list) • Gated Delta Networks: Improving Mamba2 with Delta Rule Gated Delta Networks: Improving Mamba2 with Delta Rule (https://arxiv.org/abs/2412.06464) • Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free (https://arxiv.org/pdf/2505.06708) • DeepSeek-V3 Technical Report DeepSeek-V3 Technical Report (https://arxiv.org/pdf/2412.19437) • Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models (https://arxiv.org/abs/2501.11873) • Efficient Streaming Language Models with Attention Sinks Efficient Streaming Language Models with Attention Sinks (https://qwen.ai/blog?id=e34c4305036ce60d55a0791b170337c2b70ae51d&from=home.latest-research-list) • Massive Activations in Large Language Models Massive Activations in Large Language Models (https://arxiv.org/abs/2402.17762) • Gemma: Open Models Based on Gemini Research and Technology Gemma: Open Models Based on Gemini Research and Technology (https://arxiv.org/abs/2403.08295) • Approximating Two-Layer Feedforward Networks for Efficient Transformers Approximating Two-Layer Feedforward Networks for Efficient Transformers (https://arxiv.org/abs/2310.10837) • Better & Faster Large Language Models via Multi-token Prediction Better & Faster Large Language Models via Multi-token Prediction (https://arxiv.org/abs/2404.19737) Model Architecture: =================== Architecture Type:Other (Hybrid Transformer-Mamba)Network Architecture:Qwen3-NextTotal Parameters:80BActive Parameters:3.9BVocabulary Size:151,936 Architecture Type: Network Architecture: Total Parameters: Active Parameters: Vocabulary Size: Input: ------ Input Types:TextInput Formats:StringInput Parameters:One Dimensional (1D)Other Input Properties:Qwen3-Next natively supports context lengths of up to 262,144 tokens Input Types: Input Formats: Input Parameters: Other Input Properties: Qwen3-Next-80B-A3B-Thinkingsupports only thinking mode. To enforce model thinking, the default chat template automatically includes<think>. Therefore, it is normal for the model's output to contain only</think>without an explicit opening<think>tag. Qwen3-Next-80B-A3B-Thinking Output: ------- Output Types:TextOutput Format:StringOutput Parameters:One Dimensional (1D)Other Output Properties:Qwen3-Next natively supports context lengths of up to 262,144 tokens Output Types: Output Format: Output Parameters: Other Output Properties: Qwen3-Next-80B-A3B-Thinkingmay generate thinking content longer than its predecessor. Alibaba strongly recommends its use in highly complex reasoning tasks.Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Qwen3-Next-80B-A3B-Thinking Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration: ===================== Runtime Engines: • Inference:SGLang, vLLM Inference: Supported Hardware: • NVIDIA Hopper:H20, H100 NVIDIA Hopper: Operating Systems:Linux Operating Systems: Model Version(s) ================ Qwen3-Next-80B-A3B-Thinking v1.0 (September 11, 2025) Training, Testing, and Evaluation Datasets: =========================================== Training Dataset ---------------- Training Data Collection:UndisclosedTraining Labeling:UndisclosedTraining Properties:Undisclosed Training Data Collection: Training Labeling: Training Properties: Testing Dataset --------------- Testing Data Collection:UndisclosedTesting Labeling:UndisclosedTesting Properties:Undisclosed Testing Data Collection: Testing Labeling: Testing Properties: Evaluation Dataset ------------------ Evaluation Data Collection:UndisclosedEvaluation Labeling:UndisclosedEvaluation Properties:UndisclosedEvaluation Benchmarks: Evaluation Data Collection: Evaluation Labeling: Evaluation Properties: Evaluation Benchmarks: Knowledge 84.4 93.8 82.8 64.9 Reasoning 92.3 83.9 78.4 Coding 74.1 2134 32.5 Alignment 89.8 79.7 88.3 Agent 72.4 69.6 54.0 71.9 60.5 45.6 Multilingualism 80.6 81.0 83.9 60.1 *For reproducibility, Alibaba reports the win rates evaluated by GPT-4.1. Inference ========= Acceleration Engine:SGLangTest Hardware:NVIDIA H100 Acceleration Engine: Test Hardware: Additional Details ================== Qwen3-Next-80B-A3B-Thinkinghas the following features: Qwen3-Next-80B-A3B-Thinking • Type: Causal Language Models • Training Stage: Pre-training (15T tokens) & Post-training • Number of Parameters: 80B in total and 3.9B activated • Number of Paramaters (Non-Embedding): 79B • Number of Layers: 48 • Hidden Dimension: 2048 • Hybrid Layout: 12 * (3 * (Gated DeltaNet -> MoE) -> (Gated Attention -> MoE)) • Gated Attention:Number of Attention Heads: 16 for Q and 2 for KVHead Dimension: 256Rotary Position Embedding Dimension: 64 • Number of Attention Heads: 16 for Q and 2 for KV • Head Dimension: 256 • Rotary Position Embedding Dimension: 64 • Gated DeltaNet:Number of Linear Attention Heads: 32 for V and 16 for QKHead Dimension: 128 • Number of Linear Attention Heads: 32 for V and 16 for QK • Head Dimension: 128 • Mixture of Experts:Number of Experts: 512Number of Activated Experts: 10Number of Shared Experts: 1Expert Intermediate Dimension: 512 • Number of Experts: 512 • Number of Activated Experts: 10 • Number of Shared Experts: 1 • Expert Intermediate Dimension: 512 • Context Length: 262,144 natively and extensible up to 1,010,000 tokens Ethical Considerations ====================== NVIDIA believes Trustworthy Al is a shared responsibility and we have established policies and practices to enable development for a wide array of Al applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA Al Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "gemma-3-27b-it",
    "nvidia_url": "https://build.nvidia.com/google/gemma-3-27b-it",
    "scraped_on": "2025-10-15T21:40:40.560897",
    "tags": [
      "google",
      "image-to-text",
      "language generation",
      "vision assistant",
      "visual question answering"
    ],
    "model_card": "Gemma 3 model ============= Description =========== Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. This model is ready for commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAGemma 3model card. Gemma 3 (https://ai.google.dev/gemma/docs/core) License/Terms of Use ==================== GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service; and the use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:Gemma Terms of Use. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Gemma Terms of Use (https://ai.google.dev/gemma/terms) Deployment Geography ==================== Global Use Case ======== Models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Benefits ======== At the time of release, this family of models provides high-performance open vision-language model implementations designed from the ground up for responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown to provide superior performance to other, comparably-sized open model alternatives. Release Date ============ • Build.Nvidia.com- 3/11/2025 viahttps://build.nvidia.com/google/gemma-3-1b-itandhttps://build.nvidia.com/google/gemma-3-27b-it Build.Nvidia.com https://build.nvidia.com/google/gemma-3-1b-it https://build.nvidia.com/google/gemma-3-27b-it References ========== Model Page:GemmaAuthors: Google DeepMind Model Page Gemma (https://ai.google.dev/gemma/docs/core) Authors • Gemma 3 Technical Report Gemma 3 Technical Report (https://goo.gle/Gemma3Report) • Responsible Generative AI Toolkit Responsible Generative AI Toolkit (https://ai.google.dev/responsible) • Gemma on Kaggle Gemma on Kaggle (https://www.kaggle.com/models/google/gemma-3) • Gemma on Vertex Model Garden Gemma on Vertex Model Garden (https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3) Model Architecture ================== Architecture Type: Dense decoder-only Transformer model Architecture Type Inputs and outputs ------------------ Input ----- Input Type(s): Text, Text+ImageInput Format(s): Input Type(s) Input Format(s) • String • Image: jpg Input Parameters: Input Parameters • Text: One-dimensional (1D) Text • Image: Two-dimensional (2D) Image Other Properties Related to Input: Other Properties Related to Input • Text string, such as a question, a prompt, or a document to be summarized • Images, normalized to 896 x 896 resolution and encoded to 256 tokens each • Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and 32K tokens for the 1B size Output ------ Output Type(s): TextOutput Format: StringOutput Parameters: (1D)Other Properties Related to Output: Output Type(s) Output Format Output Parameters Other Properties Related to Output • Generated text in response to the input, such as an answer to a question, analysis of image content, or a summary of a document • Total output context of 8192 tokens Software Integration ==================== Runtime Engine(s): TRT-LLMSupported Hardware Microarchitecture Compatibility: NVIDIA Ampere, NVIDIA Blackwell, NVIDIA Jetson, NVIDIA Hopper, NVIDIA Lovelace, NVIDIA Pascal, NVIDIA Turing, and NVIDIA Volta architectures[Preferred/Supported] Operating System(s): Linux Runtime Engine(s) Supported Hardware Microarchitecture Compatibility [Preferred/Supported] Operating System(s) Model Version(s): ================= • Gemma 3 IT 1B: 1.0 (3/12/2025) Gemma 3 IT 1B • Gemma 3 IT 4B: 1.0 (3/12/2025) Gemma 3 IT 4B • Gemma 3 IT 12B: 1.0 (3/12/2025) Gemma 3 IT 12B • Gemma 3 IT 27B: 1.0 (3/12/2025) Gemma 3 IT 27B Software -------- Training was done usingJAXandML Pathways. JAX (https://github.com/jax-ml/jax) ML Pathways (https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/) JAX allows researchers to take advantage of the latest generation of hardware, including TPUs, for faster and more efficient training of large models. ML Pathways is Google's latest effort to build artificially intelligent systems capable of generalizing across multiple tasks. This is especially suitable for foundation models, including large language models like these ones. Together, JAX and ML Pathways are used as described in thepaper about the Gemini family of models;\"the 'single controller' programming model of Jax and Pathways allows a single Python process to orchestrate the entire training run, dramatically simplifying the development workflow.\" paper about the Gemini family of models (https://arxiv.org/abs/2312.11805) \"the 'single controller' programming model of Jax and Pathways allows a single Python process to orchestrate the entire training run, dramatically simplifying the development workflow.\" Training, Testing, and Evaluation Datasets ========================================== Training Dataset ---------------- Data Collection Method by dataset: Hybrid: Human, AutomatedLabeling Method by dataset: Hybrid: Human, Automated Data Collection Method by dataset Labeling Method by dataset These models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 14 trillion tokens, the 12B model was trained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and 1B with 2 trillion tokens. Here are the key components: • Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 140 languages. • Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code and understand code-related questions. • Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and address mathematical queries. • Images: A wide range of images enables the model to perform image analysis and visual data extraction tasks. The combination of these diverse data sources is crucial for training a powerful multimodal model that can handle a wide variety of different tasks and data formats. Data Preprocessing ------------------ Here are the key data cleaning and filtering methods applied to the training data: • CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content. • Sensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets. • Additional methods: Filtering based on content quality and safety in line withGoogle Responsible AI policies. Google Responsible AI policies (https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf) Testing Dataset --------------- Data Collection Method by dataset: Hybrid: Human, AutomatedLabeling Method by dataset: Hybrid: Human, Automated Data Collection Method by dataset Labeling Method by dataset Evaluation Dataset ------------------ Data Collection Method by dataset: Hybrid: Human, AutomatedLabeling Method by dataset: Hybrid: Human, Automated Data Collection Method by dataset Labeling Method by dataset Evaluation ========== Model evaluation metrics and results are highlighted below. Benchmark Results ----------------- These models were evaluated against a large collection of different datasets and metrics to cover different aspects of text generation: Reasoning and factuality ------------------------ HellaSwag (https://arxiv.org/abs/1905.07830) BoolQ (https://arxiv.org/abs/1905.10044) PIQA (https://arxiv.org/abs/1911.11641) SocialIQA (https://arxiv.org/abs/1904.09728) TriviaQA (https://arxiv.org/abs/1705.03551) Natural Questions (https://github.com/google-research-datasets/natural-questions) ARC-c (https://arxiv.org/abs/1911.01547) ARC-e (https://arxiv.org/abs/1911.01547) WinoGrande (https://arxiv.org/abs/1907.10641) BIG-Bench Hard (https://paperswithcode.com/dataset/bbh) DROP (https://arxiv.org/abs/1903.00161) STEM and code ------------- MMLU (https://arxiv.org/abs/2009.03300) AGIEval (https://arxiv.org/abs/2304.06364) MATH (https://arxiv.org/abs/2103.03874) GSM8K (https://arxiv.org/abs/2110.14168) GPQA (https://arxiv.org/abs/2311.12022) MBPP (https://arxiv.org/abs/2108.07732) HumanEval (https://arxiv.org/abs/2107.03374) Multilingual ------------ MGSM (https://arxiv.org/abs/2210.03057) Global-MMLU-Lite (https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite) WMT24++ (https://arxiv.org/abs/2502.12404v1) FloRes (https://arxiv.org/abs/2106.03193) XQuAD (https://arxiv.org/abs/1910.11856v3) ECLeKTic (https://arxiv.org/abs/2502.21228) IndicGenBench (https://arxiv.org/abs/2404.16816) Multimodal ---------- COCOcap (https://cocodataset.org/#home) DocVQA (https://www.docvqa.org/) InfoVQA (https://arxiv.org/abs/2104.12756) MMMU (https://arxiv.org/abs/2311.16502) TextVQA (https://textvqa.org/) RealWorldQA (https://paperswithcode.com/dataset/realworldqa) ReMI (https://arxiv.org/html/2406.09175v1) AI2D (https://allenai.org/data/diagrams) ChartQA (https://arxiv.org/abs/2203.10244) VQAv2 (https://visualqa.org/index.html) BLINK (https://arxiv.org/abs/2404.12390) OKVQA (https://okvqa.allenai.org/) TallyQA (https://arxiv.org/abs/1810.12440) SpatialSense VQA (https://arxiv.org/abs/1908.02660) CountBenchQA (https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/) Inference ========= Engine: TransformersTest Hardware: NVIDIA Hopper Engine Test Hardware Ethics and Safety ================= Ethics and safety evaluation approach and results are highlighted below. Evaluation Approach ------------------- The evaluation method included structured evaluations and internal red-teaming testing of relevant content policies. Red-teaming was conducted by a number of different teams, each with different goals and human evaluation metrics. These models were evaluated against a number of different categories relevant to ethics and safety, including: • Child Safety: Evaluation of text-to-text and image to text prompts covering child safety policies, including child sexual abuse and exploitation. Child Safety • Content Safety:Evaluation of text-to-text and image to text prompts covering safety policies including, harassment, violence and gore, and hate speech. Content Safety: • Representational Harms: Evaluation of text-to-text and image to text prompts covering safety policies including bias, stereotyping, and harmful associations or inaccuracies. Representational Harms In addition to development level evaluations, assurance evaluations were conducted using the \"arms-length\" internal evaluations for responsibility governance decision making. They are conducted separately from the model development team, to inform decision making about release. High level findings are fed back to the model team, but prompt sets are held-out to prevent overfitting and preserve the results' ability to inform decision making. Assurance evaluation results are reported to the Responsibility & Safety Council as part of release review. Evaluation Results ------------------ For all areas of safety testing, there were major improvements in the categories of child safety, content safety, and representational harms relative to previous Gemma models. All testing was conducted without safety filters to evaluate the model capabilities and behaviors. For both text-to-text and image-to-text, and across all model sizes, the model produced minimal policy violations, and showed significant improvements over previous Gemma models' performance with respect to ungrounded inferences. One limitation of the evaluation was that the models incorporated only English language prompts. Usage and Limitations ===================== The potential limitations for these models are outlined below. Intended Usage -------------- Open vision-language models (VLMs) models have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development. • Content Creation and CommunicationText Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts.Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications.Text Summarization: Generate concise summaries of a text corpus, research papers, or reports.Image Data Extraction: These models can be used to extract, interpret, and summarize visual data for text communications. • Text Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts. • Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications. • Text Summarization: Generate concise summaries of a text corpus, research papers, or reports. • Image Data Extraction: These models can be used to extract, interpret, and summarize visual data for text communications. • Research and EducationNatural Language Processing (NLP) and VLM Research: These models can serve as a foundation for researchers to experiment with VLM and NLP techniques, develop algorithms, and contribute to the advancement of the field.Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice.Knowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics. • Natural Language Processing (NLP) and VLM Research: These models can serve as a foundation for researchers to experiment with VLM and NLP techniques, develop algorithms, and contribute to the advancement of the field. • Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice. • Knowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics. Model Limitations ----------------- • Training DataThe quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses.The scope of the training dataset determines the subject areas the model can handle effectively. • The quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses. • The scope of the training dataset determines the subject areas the model can handle effectively. • Context and Task ComplexityModels are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging.A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • Models are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging. • A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • Language Ambiguity and NuanceNatural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language. • Natural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language. • Factual AccuracyModels generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • Models generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • Common SenseModels rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. • Models rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. Identified risks and mitigations -------------------------------- • Perpetuation of biases: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases. Perpetuation of biases • Generation of harmful content: Mechanisms and guidelines for content safety are essential. Developers are encouraged to exercise caution and implement appropriate content safety safeguards based on their specific product policies and application use cases. Generation of harmful content • Misuse for malicious purposes: Technical limitations and developer and end-user education can help mitigate against malicious applications of VLMs. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in theGemma Prohibited Use Policy. Misuse for malicious purposes Gemma Prohibited Use Policy (https://ai.google.dev/gemma/prohibited_use_policy) • Privacy violations: Models were trained on data filtered for removal of certain personal information and other sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques. Privacy violations Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "openfold2",
    "nvidia_url": "https://build.nvidia.com/openfold/openfold2",
    "scraped_on": "2025-10-15T21:40:37.390397",
    "tags": [
      "biology",
      "bionemo",
      "drug discovery",
      "nim",
      "openfold",
      "protein folding"
    ],
    "model_card": "Model Overview ============== Description: ============ OpenFold2 is a protein structure prediction model from theOpenFold Consortiumand theAlquraishi Laboratory. OpenFold2 is a pytorch re-implementation of Google Deepmind'sAlphaFold2, with support for both training and inference. OpenFold2 demonstrates accuracy parity with AlphaFold2, and improved speed. For more information, please visit the OpenFold repository see the OpenFold repositoryhttps://github.com/aqlaboratory/openfold. OpenFold Consortium (https://openfold.io/) Alquraishi Laboratory (https://www.aqlab.io/) AlphaFold2 (https://github.com/google-deepmind/alphafold) https://github.com/aqlaboratory/openfold This model is available for commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. License/Terms of Use: --------------------- GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service; and the use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:Apache 2.0 License. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache 2.0 License (https://choosealicense.com/licenses/apache-2.0/) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. Deployment Geography: ===================== Global Use Case ======== The OpenFold2 NIM can be used at academic and pharmaceutical industry research labs. The structure prediction functionality supports computer-aided drug design. Release Date ============ • Build.Nvidia.com: 3/18/2025 atbuild.nvidia.com/openfold/openfold2 build.nvidia.com/openfold/openfold2 • NGC: 3/18/2025 References: =========== Model Architecture: =================== Architecture Type:Protein Structure PredictionNetwork Architecture:AlphaFold2 Architecture Type: Network Architecture: Input: ====== Input Type(s):Protein Sequence, Multiple Sequence Alignments, TemplatesInput Format(s):String (less than or equal to 1000), a3m-format strings, hhr-format stringsInput Parameters:One-Dimensional (1D), One-Dimensional (1D), One-Dimensional (1D)Other Properties Related to Input:a3m is a standard file format for storing multiple sequence alignment results. hhr is the file format output by the tool hh-search. For more informaiton seehh-suite Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: hh-suite (https://github.com/soedinglab/hh-suite) Output: ======= Output Type(s):Protein Structure(s) in PDB FormatOutput Format:PDB (text file)Output Parameters:1DOther Properties Related to Output:Pose (num_atm_ x 3) Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s): • PyTorch Supported Hardware Microarchitecture Compatibility: • NVIDIA Hopper • NVIDIA Ampere • NVIDIA Lovelace [Preferred/Supported] Operating System(s): • Linux Model Version(s): ================= • AlphaFold weights 2.3.2 • OpenFold 2.1.0 (pl_upgrade) Training & Evaluation: ====================== Training Dataset: ================= Link:Highly Accurate ... Data Availability Link: Highly Accurate ... Data Availability (https://www.nature.com/articles/s41586-021-03819-2#data-availability) The model parameter sets were trained by Google Deepmind as part of AlphaFold2 development. A description of the training dataset and relevant download links are available atHighly Accurate ... Data Availability. This data was not collected by NVIDIA. Highly Accurate ... Data Availability (https://www.nature.com/articles/s41586-021-03819-2#data-availability) ** Data Collection Method by dataset • Hybrid: Automatic/Sensors, Human • See the description atHighly Accurate ... Data Availability. Highly Accurate ... Data Availability (https://www.nature.com/articles/s41586-021-03819-2#data-availability) ** Labeling Method by dataset • Hybrid: Automatic/Sensors, Human • See the description atHighly Accurate ... Data Availability. Highly Accurate ... Data Availability (https://www.nature.com/articles/s41586-021-03819-2#data-availability) Properties (Quantity, Dataset Descriptions, Sensor(s)):Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set. Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: =================== Link:See the description atHighly Accurate... Sec10. Link: Highly Accurate... Sec10 (https://www.nature.com/articles/s41586-021-03819-2#Sec10) ** Data Collection Method by dataset • Hybrid: Automatic/Sensors, Human • See the description atHighly Accurate ... Data Availability. Highly Accurate ... Data Availability (https://www.nature.com/articles/s41586-021-03819-2#data-availability) ** Labeling Method by dataset • Hybrid: Automatic/Sensors, Human • See the description atHighly Accurate ... Data Availability. Highly Accurate ... Data Availability (https://www.nature.com/articles/s41586-021-03819-2#data-availability) Properties (Quantity, Dataset Descriptions, Sensor(s)):Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set. Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference: ========== Engine:PyTorchTest Hardware: Engine: Test Hardware: • NVIDIA H100 • NVIDIA A100 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "cuopt",
    "nvidia_url": "https://build.nvidia.com/nvidia/nvidia-cuopt",
    "scraped_on": "2025-10-15T21:45:17.198492",
    "tags": [
      "nvidia",
      "route optimization"
    ],
    "model_card": "Model Overview ============== Description =========== NVIDIA cuOptis an AI microservice that optimizes logistics routing to save enterprises money, increase revenue, and reduce carbon emissions. It offers dynamic rerouting, horizontal load-balancing, and robotic simulations, with sub second solver response times. cuOpt enables organizations to easily access world record accelerated optimization capabilities across multi- and hybrid cloud environments. It solves complex routing problems with multiple constraints and delivers new capabilities, empowering teams to make dynamic, data-driven decisions. NVIDIA cuOpt Today, cuOpt focuses primarily on variants of Vehicle Routing Problems (VRP) such as Capacitated Vehicle Routing Problems (CVRP), Capacitated Vehicle Routing with Time Windows (CVRTW), and Pick up and Delivery with Time Windows (PDPTW). To solve these problems, NVIDIA cuOpt uses GPU-accelerated logistics solvers relying on heuristics, metaheuristics, and optimization to calculate complex vehicle-routing-problem variants with a wide range of constraints.Typical Use Cases • Last Mile Delivery (LMD): A fleet of vehicles is tasked with serving customers at multiple locations or demand points, typically with the goal of minimizing total costs, such as fleet size, distance traveled or time taken, while satisfying various constraints and objectives. Each customer has specific demand along with other constraints such as time windows and other service level agreements. Each vehicle may be bound by constraints such as such as maximum load capacity, maximum travel distance, emission types and availability. In Manufacturing, the last mile is the supply of components in a production process, e.g., the delivery of parts to the plant. In Distribution (B2B), the last mile is the supply of stock to physical stores for in-store experiences. In E-Commerce (B2C)/Grocery/Parcel/Quick Service delivery, the last mile is the hand-off point to customer or to a prearranged drop-off location. Last Mile Delivery (LMD): A fleet of vehicles is tasked with serving customers at multiple locations or demand points, typically with the goal of minimizing total costs, such as fleet size, distance traveled or time taken, while satisfying various constraints and objectives. Each customer has specific demand along with other constraints such as time windows and other service level agreements. Each vehicle may be bound by constraints such as such as maximum load capacity, maximum travel distance, emission types and availability. In Manufacturing, the last mile is the supply of components in a production process, e.g., the delivery of parts to the plant. In Distribution (B2B), the last mile is the supply of stock to physical stores for in-store experiences. In E-Commerce (B2C)/Grocery/Parcel/Quick Service delivery, the last mile is the hand-off point to customer or to a prearranged drop-off location. Last Mile Delivery (LMD) • Dispatch Optimization: This involves the allocation and scheduling of field service technicians to fulfill customer orders, service requests or deliveries. Determining the most efficient routes for vehicles or technicians based on factors such as location, capacity, availability, skillset, priority, and regulatory compliance. The objective could be to minimize travel time, distance, fuel consumption, maximize the number of tasks or service calls, or minimize empty space and unnecessary trips. Implement continuous optimization by processing dynamic events to accommodate changes, disruptions, or new requests while increasing productivity and maintaining service level agreements (SLAs). Option available to allocate tasks evenly by balancing workload distribution to prevent overloading or under-utilization of service personnel. Dispatch optimization is essential for any businesses with field service operations, transportation and logistics, emergency response teams, health or hunger drives and any other operations that involve routing of field assets. Dispatch Optimization: This involves the allocation and scheduling of field service technicians to fulfill customer orders, service requests or deliveries. Determining the most efficient routes for vehicles or technicians based on factors such as location, capacity, availability, skillset, priority, and regulatory compliance. The objective could be to minimize travel time, distance, fuel consumption, maximize the number of tasks or service calls, or minimize empty space and unnecessary trips. Implement continuous optimization by processing dynamic events to accommodate changes, disruptions, or new requests while increasing productivity and maintaining service level agreements (SLAs). Option available to allocate tasks evenly by balancing workload distribution to prevent overloading or under-utilization of service personnel. Dispatch optimization is essential for any businesses with field service operations, transportation and logistics, emergency response teams, health or hunger drives and any other operations that involve routing of field assets. Dispatch Optimization • Pickup and Delivery (PDP) –This involves determining the most efficient routes for a fleet of vehicles to pick up items from specified pick-up points and deliver them to specified drop-off points, while satisfying various constraints and objectives. The objective could be to minimize total travel time, minimize fleet size, maximizing vehicle utilization, minimize transportation costs, reduce emissions while ensuring timely delivery and maintaining customer service level agreements (SLAs). Both pick-up and drop-off locations have capacity constraints, geographical proximity/constraints, service durations, time windows, precedence relationships and customer preferences. The fleet also has constraints such as limited capabilities, maximum load, maximum travel distance, or reduced emissions. Pickup and delivery use cases can be applied in various routing scenarios: Quick service food delivery, ride hailing, retail distribution, parcel delivery, intralogistics routing, school bus routing, and waste collection. Pickup and Delivery (PDP) –This involves determining the most efficient routes for a fleet of vehicles to pick up items from specified pick-up points and deliver them to specified drop-off points, while satisfying various constraints and objectives. The objective could be to minimize total travel time, minimize fleet size, maximizing vehicle utilization, minimize transportation costs, reduce emissions while ensuring timely delivery and maintaining customer service level agreements (SLAs). Both pick-up and drop-off locations have capacity constraints, geographical proximity/constraints, service durations, time windows, precedence relationships and customer preferences. The fleet also has constraints such as limited capabilities, maximum load, maximum travel distance, or reduced emissions. Pickup and delivery use cases can be applied in various routing scenarios: Quick service food delivery, ride hailing, retail distribution, parcel delivery, intralogistics routing, school bus routing, and waste collection. Pickup and Delivery (PDP) – Terms of use ============ By using this software or microservice, you are agreeing to theterms and conditionsof the license and acceptable use policy. terms and conditions (https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/) References(s): -------------- • NVIDIA cuOpt docs NVIDIA cuOpt docs (https://docs.nvidia.com/cuopt/) • NVIDIA cuOpt Resources NVIDIA cuOpt Resources (https://github.com/NVIDIA/cuOpt-Resources/tree/branch-25.02) • DLI Course on NVIDIA cuOpt DLI Course on NVIDIA cuOpt (https://www.nvidia.com/en-us/training/) Model Architecture ================== Architecture Type:Not Applicable (N/A)Network Architecture:Not Applicable (N/A)Container Version:25.02.00 Architecture Type: Network Architecture: Container Version: Input ===== Input Format:Json data in Text or File format.Input Parameters:Please refer toopen-apispecification. Input Format: Input Parameters: open-api (https://docs.nvidia.com/cuopt/user-guide/open-api.html#/default/cuopt_cuopt_cuopt_post) Output ====== Output Format:Json data in Text or File format.Output Parameters:Please refer toopen-apispecification. Output Format: Output Parameters: open-api (https://docs.nvidia.com/cuopt/user-guide/open-api.html#/default/cuopt_cuopt_cuopt_post) Software Integration: --------------------- Runtime(s):N/ASupported Hardware Platform(s):Hopper, AmpereSupported Operating System(s):Linux and Windows(WSL) with CUDA >= 12.6. Runtime(s): Supported Hardware Platform(s): Supported Operating System(s):"
  },
  {
    "name": "mixtral-8x7b-instruct-v0.1",
    "nvidia_url": "https://build.nvidia.com/mistralai/mixtral-8x7b-instruct",
    "scraped_on": "2025-10-15T21:45:14.049475",
    "tags": [
      "advanced reasoning",
      "chat",
      "code generation",
      "large language models",
      "mistralai",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description: ============ Mixtral 8x7B Instruct is a language model that can follow instructions, complete requests, and generate creative text formats. Mixtral 8x7B a high-quality sparse mixture of experts model (SMoE) with open weights.This model has been optimized through supervised fine-tuning and direct preference optimization (DPO) for careful instruction following. On MT-Bench, it reaches a score of 8.30, making it the best open-source model, with a performance comparable to GPT3.5.Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.Mixtral has the following capabilities. • It gracefully handles a context of 32k tokens. • It handles English, French, Italian, German and Spanish. • It shows strong performance in code generation. • It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench. Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; seeMistral's 8x7B Instruct Hugging Face Model Card. Mistral's 8x7B Instruct Hugging Face Model Card (https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) Terms of use ============ By using this software or model, you are agreeing to theterms and conditionsof the license, acceptable use policy and Mistral's privacy policy. Mixtral-8x7B is released under the Apache 2.0 license terms and conditions (https://mistral.ai/terms-of-service/) References(s): ============== Mixtral 8x7B InstructModel Cardon Hugging FaceMixtral of experts | Mistral AI | Open source models Model Card (https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) Mixtral of experts | Mistral AI | Open source models (https://mistral.ai/news/mixtral-of-experts/) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Sparse Mixture of GPT-based expertsModel Version:0.1 Architecture Type: Network Architecture: Model Version: Input: ====== Input Format:TextInput Parameters:Temperature, Top P, Max Output Tokens Input Format: Input Parameters: Output: ======= Output Format:TextOutput Parameters:None Output Format: Output Parameters: Software Integration: ===================== Supported Hardware Platform(s):Hopper, Ampere, Turing, AdaSupported Operating System(s):Linux Supported Hardware Platform(s): Supported Operating System(s): Inference: ========== Engine:TritonTest Hardware:Other Engine: Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware:"
  },
  {
    "name": "diffdock",
    "nvidia_url": "https://build.nvidia.com/mit/diffdock",
    "scraped_on": "2025-10-15T21:45:13.701521",
    "tags": [
      "bionemo",
      "chemistry",
      "docking",
      "drug discovery",
      "mit",
      "nim"
    ],
    "model_card": "Model Overview ============== Description: ============ DiffDock is a generative diffusion model for drug discovery in molecular blind docking. DiffDock consists of two models: the Score and Confidence models. The Score model generates a series of potential poses for protein-ligand binding by running a reverse diffusion process. DiffDock does not require any information about a binding pocket. During its diffusion process, the molecule's position relative to the protein, its orientation, and the torsion angles are allowed to change. Running the learned reverse diffusion process transforms a distribution of noisy prior molecule poses to the one learned by the model. As a result, it outputs many sampled poses and ranks them via its confidence model. Leveraging the same neural-network architecture designed in the original DiffDock by MIT, the model v2.0 is trained by NVIDIA using PLINDER, a state-of-art dataset of well curated and labeled protein-ligand complexes, which therefore, delivers a much higher accuracy for molecular docking tasks. This model is ready for commercial and non-commercial use. License/Terms of Use: ===================== This model is released under theMIT License. MIT License (https://github.com/gcorso/DiffDock/blob/main/LICENSE) References: =========== Model Architecture: =================== Architecture Type:Score-Based Diffusion Model (SBDM)Network Architecture:Graph Convolution Neural Network Architecture Type: Network Architecture: The Score model is a 3-dimensional equivariant graph neural network that has three layers: embedding, interaction layer with 6 graph convolution layers, and output layer. In total, the Score model has 20M parameters. Input: ====== Input Type(s):Text (Ligand, Protein), Number (Poses to Generate, Batch Size, Diffusion Steps, Diffusion Time Divisions) Binary (No Final Step Noise, Save Diffusion Trajectory, and Skip Gen Conformer)Input Format(s):Text: String (SMILES, Structural Data Files (SDF) or Tripos molecule structure (Mol2) for Ligand), String (Protein Data Bank (PDB)), Number: Integer; Binary: BooleanInput Parameters:1DOther Properties Related to Input:No max sequence Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):Text (Ligand Molecule 3D Positions, 3D), Text (Ligand Molecule 3D Positions, 3D), Number (List of Confidence Scores, 1D)Output Format:Text: Structural Data Files (SDF), Text: Protein Data Bank (PDB), Number: Array of Floating Point 32Output Parameters:docked_ligand, visualizations_files, pose_confidence Output Type(s): Output Format: Output Parameters: Software Integration: ===================== Runtime Engine(s): • PyTorch Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Ada Lovelace • NVIDIA Hopper • NVIDIA Grace Hopper [Preferred/Supported] Operating System(s): • Linux Model Version(s): ================= DiffDock v2.1 Training & Evaluation Dataset: ============================== Training: ========= Link:PLINDERData Collection Method by dataset: Link: PLINDER (https://plinder-org.github.io/plinder/index.html) Data Collection Method by dataset: • Human Labeling Method by dataset: • Hybrid: Human & Automated Properties (Quantity, Dataset Descriptions, Sensor(s)):450,000 protein-ligand complexes automatically curated using the PDB database. See (https://www.biorxiv.org/content/10.1101/2024.07.17.603955v3). Properties (Quantity, Dataset Descriptions, Sensor(s)): https://www.biorxiv.org/content/10.1101/2024.07.17.603955v3 Evaluation: =========== Link:PoseBusters benchmark (PDB) setData Collection Method by dataset: Link: PoseBusters benchmark (PDB) set (https://zenodo.org/records/8278563) Data Collection Method by dataset: • Human Labeling Method by dataset: • Hybrid: Human & Automated Properties (Quantity, Dataset Descriptions, Sensor(s)):428 protein-ligand complexes manually curated using the PDB database. Seehttps://arxiv.org/abs/2308.05777v1. Properties (Quantity, Dataset Descriptions, Sensor(s)): https://arxiv.org/abs/2308.05777v1 Inference: ========== Engine:PyTorchTest Hardware: Engine: Test Hardware: • A100, A6000, H100, L40, L40S Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Users are responsible for ensuring the physical properties of model-generated molecular docking poses are appropriately evaluated and comply with applicable safety regulations and ethical standards. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. For more detailed information on ethical considerations for this model Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "esmfold",
    "nvidia_url": "https://build.nvidia.com/meta/esmfold",
    "scraped_on": "2025-10-15T21:45:13.209573",
    "tags": [
      "biology",
      "bionemo",
      "drug discovery",
      "meta",
      "nim",
      "protein folding"
    ],
    "model_card": "Model Overview ============== Description: ============ ESMFold is a protein structure prediction deep learning model developed by Facebook AI Research (FAIR)lin2023esmfold. The model was inspired by AlphaFold, but does not require multiple sequence alignment (MSA) as an input, leading to significantly faster inference times for protein structure prediction that is nearly as accurate as alignment-based methods. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIA Model Card. References: =========== Model Architecture: =================== Architecture Type:Pose EstimationNetwork Architecture:ESMFold Architecture Type: Network Architecture: Input: ====== Input Type(s):Protein SequenceInput Format(s):StringInput Parameters:1DOther Properties Related to Input:Protein Sequence matching the regular expression^[ARNDCQEGHILKMFPSTWYVXBOU]*$upto 1024 characters Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):Protrin Structure Pose(s)Output Format:PDB (text file)Output Parameters:1DOther Properties Related to Output:Pose Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s): • [Not Applicable (N/A)- Name Platform If Multiple] Supported Hardware Microarchitecture Compatibility: • [Ampere] • [L40] [Preferred/Supported] Operating System(s): • [Linux] Model Version(s): ESMFold V1 ============================ Training & Evaluation: ====================== Training Dataset: ================= Link:UniRef50 Link: UniRef50 (https://www.uniprot.org/help/uniref) ** Data Collection Method by dataset • [Not Applicable] ** Labeling Method by dataset • [Not Applicable] Properties (Quantity, Dataset Descriptions, Sensor(s)):UniRef50, September 2021 version, is used for the training of ESM models. The training dataset was partitioned by randomly selecting 0.5% (≈ 250,000) sequences to form the validation set. The training set has sequences removed via the procedure described Properties (Quantity, Dataset Descriptions, Sensor(s)): UniRef50 (https://www.uniprot.org/help/uniref) Dataset License(s):CC BY 4.0. Dataset License(s): CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/) Evaluation Dataset: =================== UniRef50** Data Collection Method by dataset UniRef50 (https://www.uniprot.org/help/uniref) • [Not Applicable] ** Labeling Method by dataset • [Not Applicable] Properties (Quantity, Dataset Descriptions, Sensor(s)):UniRef50, September 2021 version, is used for the training of ESM models. The training dataset was partitioned by randomly selecting 0.5% (≈ 250,000) sequences to form the validation set. The training set has sequences removed via the procedure described Properties (Quantity, Dataset Descriptions, Sensor(s)): UniRef50 (https://www.uniprot.org/help/uniref) Dataset License(s):CC BY 4.0. Dataset License(s): CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/) Inference: ========== Engine:TritonTest Hardware: Engine: Test Hardware: • [Other (Not Listed)] Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "molmim",
    "nvidia_url": "https://build.nvidia.com/nvidia/molmim-generate",
    "scraped_on": "2025-10-15T21:45:09.976563",
    "tags": [
      "bionemo",
      "chemistry",
      "drug discovery",
      "molecule generation",
      "nim",
      "nvidia"
    ],
    "model_card": "Model Overview ============== Description: ============ MolMIM generates a random sample of new molecules in SMILES format by sampling from the latent space around the point corresponding to the given seed molecule. MolMIM performs optimization with the CMA-ES algorithmin the model’s latent space and sample molecules with improved values of the desired scoring function. MolMIM is a latent variable model developed by NVIDIA that is trained in an unsupervised manner over a large-scale dataset of molecules in the form of SMILES strings. MolMIM utilizes transformer architecture to learn an informative fixed-size latent space using Mutual Information Machine (MIM) learning. MIM is a learning framework for a latent variable model which promotes informative and clustered latent codes. MolMIM can be used for sampling novel molecules from the model’s latent space. References(s): ============== Improving Small Molecule Generation using Mutual Information Machine Improving Small Molecule Generation using Mutual Information Machine (https://arxiv.org/abs/2208.09016) MIM: Mutual Information Machine MIM: Mutual Information Machine (https://arxiv.org/abs/1910.03175) The CMA Evolution Strategy: A Comparing Review The CMA Evolution Strategy: A Comparing Review (https://link.springer.com/chapter/10.1007/3-540-32494-1_4) Model Architecture: =================== Architecture Type:Encoder-DecoderNetwork Architecture:Perceiver Architecture Type: Network Architecture: MolMIM utilizes a Perceiver encoder architecture which outputs a fixed-size representation, where molecules of various lengths are mapped into a latent space. MolMIM’s decoder architecture is a Transformer. Both encoder and decoder container 6 layers with a hidden size of 512, 8 attention heads, and a feed-forward dimension of 2048. Total number of parameters in MolMIM is 65.2M. The model was trained with A-MIM learning. Input: ====== Input Type(s):Text (Molecular Sequence)Input Format(s):Comma Separated Values, Simplified Molecular-Input Line Entry System (SMILES)Input Parameters:1DOther Properties Related to Input:Maximum input length is 512 tokens. Pretraining dataset samples were randomly split into train, validation, and test sets ( 99% / 0.5% / 0.5% ). Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):Text, NumericalOutput Format:[SMILES]Output Parameters:[2D]Other Properties Related to Output:Maximum output length is 128 tokens Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s): • Triton Inference Server Supported Hardware Microarchitecture Compatibility: • Ampere • L40 Preferred/Supported Operating System(s): ======================================== • [Linux] • [Windows] Model Version(s): ================= MolMIM-24.03 Training and Evaluation Dataset: ================================ Link:ZINC-15 Link: ZINC-15 (https://zinc15.docking.org) ** Data Collection Method by dataset • Not Applicable ** Labeling Method by dataset • Not Applicable Properties (Quantity, Dataset Descriptions, Sensor(s)):1.54B molecules with molecular weight <= 500 Daltons, LogP <= 5, with reactivity levels rated as “reactive” and purchasability “annotated.” The compounds were filtered to ensure a maximum length of 512 characters. Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: =================== Link:MoleculeNet - Lipophilicity, FreeSolv, ESOL Link: MoleculeNet - Lipophilicity, FreeSolv, ESOL (https://moleculenet.org/datasets-1) ** Data Collection Method by dataset • Hybrid: Human & Automatic/Sensors ** Labeling Method by dataset • Hybrid: Human & Automated Properties (Quantity, Dataset Descriptions, Sensor(s)): MoleculeNet Physical Chemistryis an aggregation of public molecular datasets. The physical chemistry portion of MoleculeNet that we used for evaluation is made up of ESOL (1128 compunds), FreeSolv (642 compunds) and Lipohilicity (4200 compunds). MoleculeNet Physical Chemistry (https://moleculenet.org/datasets-1) Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, Vijay Pande,MoleculeNet: A Benchmark for Molecular Machine Learning, arXiv preprint, arXiv: 1703.00564, 2017. MoleculeNet: A Benchmark for Molecular Machine Learning (https://arxiv.org/abs/1703.00564) From the MoleculeNet documentation: • ESOL is made up of water solubility data(log solubility in mols per litre) for common organic small molecules. • FreeSolv is made up of experimental and calculated hydration free energy of small molecules in water. • Lipophilicity is composed of experimental results of octanol/water distribution coefficient(logD at pH 7.4). Inference: ========== Engine:Tensor(RT)Test Hardware: Engine: Test Hardware: • Ampere • L40 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Users are responsible for ensuring the physical properties of model-generated molecules are appropriately evaluated and comply with applicable safety regulations and ethical standards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "mistral-7b-instruct-v0.2",
    "nvidia_url": "https://build.nvidia.com/mistralai/mistral-7b-instruct-v2",
    "scraped_on": "2025-10-15T21:45:09.486154",
    "tags": [
      "chat",
      "language generation",
      "mistralai",
      "nvidia nim",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description: ============ Mistral-7B-Instruct is a language model that can follow instructions, complete requests, and generate creative text formats. It is an instruct version of the Mistral-7B-v0.2 generative text model fine-tuned using a variety of publicly available conversation datasets. Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; seeMistral's 7B Instruct Hugging Face Model Card. Mistral's 7B Instruct Hugging Face Model Card (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) Terms of use ============ By using this software or model, you are agreeing to theterms and conditionsof the license, acceptable use policy and Mistral's privacy policy. Mistral-7B is released under the Apache 2.0 license terms and conditions (https://mistral.ai/terms-of-service/) References(s): ============== Mistral 7B InstructModel Cardon Hugging FaceMistral 7BpaperMistral 7Bblogpost Model Card (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) paper (https://arxiv.org/abs/2310.06825) blogpost (https://mistral.ai/news/announcing-mistral-7b/) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Mistral-7BModel Version:0.1 Architecture Type: Network Architecture: Model Version: Input: ====== Input Format:TextInput Parameters:Max Tokens, Temperature, Top P Input Format: Input Parameters: Output: ======= Output Format:TextOutput Parameters:None Output Format: Output Parameters: Software Integration: ===================== Supported Hardware Platform(s):Hopper, Ampere, TuringSupported Operating System(s):Linux Supported Hardware Platform(s): Supported Operating System(s): Inference: ========== Engine:TritonTest Hardware:Other Engine: Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware:"
  },
  {
    "name": "gemma-7b",
    "nvidia_url": "https://build.nvidia.com/google/gemma-7b",
    "scraped_on": "2025-10-15T21:45:06.313085",
    "tags": [
      "chat",
      "code generation",
      "google",
      "language generation",
      "text-to-text"
    ],
    "model_card": "Gemma Model Card ================ Model Information ================= Description ----------- Gemma is a family of lightweight, state-of-the art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. References: =========== Author: Google Author Model Page:Gemma Model Page Gemma (https://ai.google.dev/gemma/docs) Model Card:https://ai.google.dev/gemma/docs/model_card Model Card https://ai.google.dev/gemma/docs/model_card Resources and Technical Documentation: Resources and Technical Documentation • Responsible Generative AI Toolkit Responsible Generative AI Toolkit (https://ai.google.dev/responsible) • Gemma on Kaggle Gemma on Kaggle (https://www.kaggle.com/models/google/gemma) • Gemma on Vertex Model Garden Gemma on Vertex Model Garden (https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335) Terms of Use:Terms Terms of Use Terms (https://ai.google.dev/gemma/terms) Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. Inputs and Outputs ------------------ Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:One-Dimensional (1D)Other Properties Related to Output:Text can be question, a prompt, or a document to be summarized. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Output: Output: ======= Input Type(s):TextInput Format(s):StringInput Parameters:One-Dimensional (1D)Other Properties Related to Output:Generated English-language text in response to the input (e.g., an answer to the question, a summary of the document). Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Output: Usage and Limitations ===================== These models have certain limitations that users should be aware of. Intended Usage -------------- Open Large Language Models (LLMs) have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development. • Content Creation and CommunicationText Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts.Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications.Text Summarization: Generate concise summaries of a text corpus, research papers, or reports. • Text Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts. • Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications. • Text Summarization: Generate concise summaries of a text corpus, research papers, or reports. • Research and EducationNatural Language Processing (NLP) Research: These models can serve as a foundation for researchers to experiment with NLP techniques, develop algorithms, and contribute to the advancement of the field.Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice.Knowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics. • Natural Language Processing (NLP) Research: These models can serve as a foundation for researchers to experiment with NLP techniques, develop algorithms, and contribute to the advancement of the field. • Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice. • Knowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics. Limitations ----------- • Training DataThe quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses.The scope of the training dataset determines the subject areas the model can handle effectively. • The quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses. • The scope of the training dataset determines the subject areas the model can handle effectively. • Context and Task ComplexityLLMs are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging.A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • LLMs are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging. • A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • Language Ambiguity and NuanceNatural language is inherently complex. LLMs might struggle to grasp subtle nuances, sarcasm, or figurative language. • Natural language is inherently complex. LLMs might struggle to grasp subtle nuances, sarcasm, or figurative language. • Factual AccuracyLLMs generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • LLMs generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • Common SenseLLMs rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. • LLMs rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. Model Data ========== Training Dataset ---------------- These models were trained on a text dataset that includes a wide variety of sources, totaling 6 trillion tokens. Here are the primary training data sources: • Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content. • Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions. • Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries. The combination of these diverse data sources is crucial for training a powerful language model that can handle a wide variety of different tasks and text formats. Data Preprocessing ------------------ Here are the key data cleaning and filtering methods applied to the training data: • CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content. • Sensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets. • Additional methods: Filtering based on content quality and safely in line withour policies. our policies (https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11) Implementation Information ========================== TensorRT-LLM ------------ The endpoint available on NGC catalog is accelerated by TensorRT-LLM, an open-source library for optimizing inference performance. Gemma is compatible across NVIDIA AI platforms—from the datacenter, cloud, to the local PC with RTX GPU systems. Gemma models use a vocabulary size of 256K and support a context length of up to 8K while using rotary positional embedding (RoPE). With support for Position Interpolation (PI) available in TensorRT-LLM, Gemma models using RoPE can support longer output sequence lengths at inference time while retaining original model architecture. Model Customization ------------------- The model is converted to .nemo for easy customization with NVIDIA NeMo framework – an end-to-end framework to curate data, tune models, and deploy anywhere. It supports various customization techniques including RLHF, SFT, LoRA, and Steer-LM. Evaluation ========== Model evaluation metrics and results. Benchmark Results ----------------- These models were evaluated against a large collection of different datasets and metrics to cover different aspects of text generation: MMLU (https://arxiv.org/abs/2009.03300) HellaSwag (https://arxiv.org/abs/1905.07830) PIQA (https://arxiv.org/abs/1911.11641) SocialIQA (https://arxiv.org/abs/1904.09728) BooIQ (https://arxiv.org/abs/1905.10044) WinoGrande (https://arxiv.org/abs/1907.10641) CommonsenseQA (https://arxiv.org/abs/1811.00937) OpenBookQA (https://arxiv.org/abs/1809.02789) ARC-e (https://arxiv.org/abs/1911.01547) ARC-c (https://arxiv.org/abs/1911.01547) TriviaQA (https://arxiv.org/abs/1705.03551) Natural Questions (https://github.com/google-research-datasets/natural-questions) HumanEval (https://arxiv.org/abs/2107.03374) MBPP (https://arxiv.org/abs/2108.07732) GSM8K (https://arxiv.org/abs/2110.14168) MATH (https://arxiv.org/abs/2103.03874) AGIEval (https://arxiv.org/abs/2304.06364) BIG-Bench (https://arxiv.org/abs/2206.04615) Average 54.0 56.4 Ethics and Safety ================= Evaluation Approach ------------------- Our evaluation methods include structured evaluations and internal red-teaming testing of relevant content policies. Red-teaming was conducted by a number of different teams, each with different goals and human evaluation metrics. These models were evaluated against a number of different categories relevant to ethics and safety, including: • Text-to-Text Content Safety: Human evaluation on prompts covering safety policies including child sexual abuse and exploitation, harassment, violence and gore, and hate speech. • Text-to-Text Representational Harms: Benchmark against relevant academic datasets such asWinoBiasandBBQ Dataset. WinoBias (https://arxiv.org/abs/1804.06876) BBQ Dataset (https://arxiv.org/abs/2110.08193v2) • Memorization: Automated evaluation of memorization of training data, including the risk of personally identifiable information exposure. • Large-scale harm: Tests for \"dangerous capabilities,\" such as chemical, biological, radiological, and nuclear (CBRN) risks. Evaluation Results ------------------ The results of ethics and safety evaluations are within acceptable thresholds for meetinginternal policiesfor categories such as child safety, content safety, representational harms, memorization, large-scale harms. On top of robust internal evaluations, the results of well known safety benchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA are shown here. internal policies (https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11) RealToxicity (https://arxiv.org/abs/2009.11462) BOLD (https://arxiv.org/abs/2101.11718) CrowS-Pairs (https://aclanthology.org/2020.emnlp-main.154/) BBQ Ambig (https://arxiv.org/abs/2110.08193v2) BBQ Disambig (https://arxiv.org/abs/2110.08193v2) Winogender (https://arxiv.org/abs/1804.09301) TruthfulQA (https://arxiv.org/abs/2109.07958) Winobias 1_2 (https://arxiv.org/abs/1804.06876) Winobias 2_2 (https://arxiv.org/abs/1804.06876) Toxigen (https://arxiv.org/abs/2203.09509) Ethical Considerations and Risks -------------------------------- The development of large language models (LLMs) raises several ethical concerns. In creating an open model, we have carefully considered the following: • Bias and FairnessLLMs trained on large-scale, real-world text data can reflect socio-cultural biases embedded in the training material. These models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card. • LLMs trained on large-scale, real-world text data can reflect socio-cultural biases embedded in the training material. These models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card. • Misinformation and MisuseLLMs can be misused to generate text that is false, misleading, or harmful.Guidelines are provided for responsible use with the model, see theResponsible Generative AI Toolkit. • LLMs can be misused to generate text that is false, misleading, or harmful. • Guidelines are provided for responsible use with the model, see theResponsible Generative AI Toolkit. Responsible Generative AI Toolkit (https://ai.google.dev/responsible) • Transparency and Accountability:This model card summarizes details on the models' architecture, capabilities, limitations, and evaluation processes.A responsibly developed open model offers the opportunity to share innovation by making LLM technology accessible to developers and researchers across the AI ecosystem. • This model card summarizes details on the models' architecture, capabilities, limitations, and evaluation processes. • A responsibly developed open model offers the opportunity to share innovation by making LLM technology accessible to developers and researchers across the AI ecosystem. Risks Identified and Mitigations: • Perpetuation of biases: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases. • Generation of harmful content: Mechanisms and guidelines for content safety are essential. Developers are encouraged to exercise caution and implement appropriate content safety safeguards based on their specific product policies and application use cases. • Misuse for malicious purposes: Technical limitations and developer and end-user education can help mitigate against malicious applications of LLMs. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in theGemma Prohibited Use Policy. Gemma Prohibited Use Policy (https://ai.google.dev/gemma/prohibited_use_policy) • Privacy violations: Models were trained on data filtered for removal of PII (Personally Identifiable Information). Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques. Google's commitments to operate sustainably. Google's commitments to operate sustainably (https://sustainability.google/operating-sustainably/) Benefits -------- At the time of release, this family of models provides high-performance open large language model implementations designed from the ground up for Responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown to provide superior performance to other, comparably-sized open model alternatives."
  },
  {
    "name": "vista-3d",
    "nvidia_url": "https://build.nvidia.com/nvidia/vista-3d",
    "scraped_on": "2025-10-15T21:45:03.035813",
    "tags": [
      "image segmentation",
      "interactive annotation",
      "medical imaging",
      "non-commercial use only",
      "nvidia"
    ],
    "model_card": "Model Overview ============== Description: ------------ VISTA-3D is a specialized interactive foundation model for 3D medical imaging. It excels in providing accurate and adaptable segmentation analysis across anatomies and modalities. Utilizing a multi-head architecture, VISTA-3D adapts to varying conditions and anatomical areas, helping guide users' annotation workflow. This model is for research purposes and not for clinical usage. • Segment everything:Enables whole body exploration, crucial for understanding complex diseases affecting multiple organs and for holistic treatment planning. Segment everything: • Segment using class:Provides detailed sectional views based on specific classes, essential for targeted disease analysis or organ mapping, such as tumor identification in critical organs. Segment using class: • Segment point prompts:Enhances segmentation precision through user-directed, click-based selection. This interactive approach accelerates the creation of accurate ground-truth data, essential in medical imaging analysis. Segment point prompts: Training Data Details --------------------- TheVISTA3D model was trained on a large and diverse dataset of 11454 3D CT volumes. This dataset was curated from in-house and publicly available sources. The training data encompassed awide range of acquisition protocols. VISTA3D model was trained on a large and diverse dataset of 11454 3D CT volumes wide range of acquisition protocols Thespatial resolutionsof the scans varied significantly, rangingfrom 0.45 × 0.45 × 0.45 mm³ to 1.50 × 1.50 × 7.50 mm³, with amedian resolution of 0.88 × 0.88 × 1.50 mm³. This indicates that the training data included scans with varying slice thicknesses and in-plane resolutions [our conversation history]. spatial resolutions from 0.45 × 0.45 × 0.45 mm³ to 1.50 × 1.50 × 7.50 mm³ median resolution of 0.88 × 0.88 × 1.50 mm³ Information regarding the gender breakdown of the participants within these datasets is not explicitly provided in the paper or its supplementary material[our conversation history, 47]. While Table 1 in the supplementary material lists the datasets used and the number of cases, itdoes not include demographic information like gender[our conversation history]. Similarly, Figure 1 in the supplementary material shows the distribution of annotated voxels per class but does not include gender information [our conversation history]. Information regarding the gender breakdown of the participants within these datasets is not explicitly provided in the paper or its supplementary material does not include demographic information like gender Other relevant details include: • The dataset includesvoxel-wise annotations of anatomical structures and lesions. voxel-wise annotations of anatomical structures and lesions • Pseudo-labels of117 classeswere generated using TotalSegmentator. 117 classes • Supervoxels were generated for every scan using SAM pre-trained weights. • Each data source was randomly split into64% training, 16% validation, and 20% test sets. 64% training, 16% validation, and 20% test sets Intended Use ------------ TheVISTA3D model is intended to facilitate clinicians and researchers using 3D Computed Tomography (CT) images. As a highly accurate and clinically applicable segmentation foundation model, it aims to streamline workflows in medical image analysis. Specifically,CT image segmentation can aid in diagnosis, treatment planning, and disease monitoringby providing detailed morphological information of body structures and abnormalities. VISTA3D aims to reduce the time-consuming and tedious nature of manual segmentation in clinical practice. VISTA3D model is intended to facilitate clinicians and researchers using 3D Computed Tomography (CT) images CT image segmentation can aid in diagnosis, treatment planning, and disease monitoring Capabilities ------------ VISTA3D possesses several essential capabilities for 3D CT image segmentation: • Accurate Automatic Segmentation:VISTA3D providesaccurate out-of-the-box segmentation for 127 common types of human anatomical structures and various lesions. For supported classes with sufficient labeled data, the model aims to achieve state-of-the-art or comparable performance to dataset-specific models. Accurate Automatic Segmentation: accurate out-of-the-box segmentation for 127 common types of human anatomical structures and various lesions • Interactive Refinement:The model supports3D interactive segmentation, allowing users to conveniently edit and refine automatic segmentation results through point clicks. This enables effective correction of inaccurate automatic segmentations. Interactive Refinement: 3D interactive segmentation • Zero-Shot Segmentation:VISTA3D exhibitsstate-of-the-art zero-shot performancefor unseen classes using its interactive branch. This allows users to interactively annotate novel structures with minimal annotation effort. This capability is enhanced by the distillation of image understanding from SAM through the generation of 3D supervoxels. Zero-Shot Segmentation: state-of-the-art zero-shot performance • 3D Operation:The model operates directly on3D volumetric images, leveraging 3D visual contexts rather than relying on time-consuming 2D slice-by-slice methods. 3D Operation: 3D volumetric images • Few-Shot/Transfer Learning:VISTA3D demonstratesstrong transfer learning ability, allowing users to quickly adapt the model to perform segmentation on new classes with only a few annotated examples. Few-Shot/Transfer Learning: strong transfer learning ability The VISTA3D model architecture includes two branches, anautomatic branch for direct segmentation of supported classesand aninteractive branch that accepts user clicksfor both supported and novel zero-shot classes. These branches share the same image encoder. automatic branch for direct segmentation of supported classes interactive branch that accepts user clicks Terms of use ------------ By using this model, you are agreeing to theterms and conditionsof the license. terms and conditions (https://docs.nvidia.com/ai-foundation-models-community-license.pdf) References(s): -------------- Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick, 2023. High-resolution 3D abdominal segmentation with random patch network fusion. Segment Anything. arXiv:2304.02643 Model Architecture: ------------------- Architecture Type:TransformerNetwork Architecture:SegResNet + Prompt Encoding Architecture Type: Network Architecture: Input: ------ Input Type(s):Computed Tomography (CT) ImageInput Format(s):(Neuroimaging Informatics Technology Initiative) NIfTIInput Parameters:Three-Dimensional (3D)Other Properties Related to Input:Array of Class/Point Information Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ------- Output Type(s):ImageOutput Format:NIfTIOutput Parameters:3D Output Type(s): Output Format: Output Parameters: Software Integration: ===================== Runtime Engine(s):MONAI Core v.1.4 Runtime Engine(s): Supported Hardware Microarchitecture Compatibility: • Ampere • Hopper [Preferred/Supported] Operating System(s): • Linux Inference: ========== Engine:TritonTest Hardware:A100, H100, L40 Engine: Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Terms of use ============ By using this model, you are agreeing to theterms and conditionsof the license. terms and conditions (https://docs.nvidia.com/ai-foundation-models-community-license.pdf) References(s) ============= He, Y., Guo, P., Tang, Y., Myronenko, A., Nath, V., Xu, Z., ... & Li, W. (2024). Vista3d: Versatile imaging segmentation and annotation model for 3d computed tomography. CVPR2025."
  },
  {
    "name": "rerank-qa-mistral-4b",
    "nvidia_url": "https://build.nvidia.com/nvidia/rerank-qa-mistral-4b",
    "scraped_on": "2025-10-15T21:45:02.625030",
    "tags": [
      "nvidia",
      "ranking",
      "retrieval augmented generation"
    ],
    "model_card": "Model Overview ============== Description ----------- The NVIDIA Retrieval QA Ranking Models is a model optimized for providing a probability score that a given passage contains the information to answer a question. The ranking model is a component in a text retrieval system to improve the overall accuracy. A text retrieval system often uses an embedding model (dense) or lexical search (sparse) index to return relevant text passages given the input. A ranking model can be used to rerank the potential candidate into a final order. Ranking model has the query-passage pairs as an input and therefore, can process cross attention between the words. It would not be feasible to apply a Ranking model on all documents in the knowledge base, therefore, ranking models are often deployed in combination with embedding models. NVIDIA Retrieval QA Ranking Model is a part of NVIDIA NeMo Retriever, which provides state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can also readily customize them for their domain-specific use cases, such as Information Technology, Human Resource help assistants, and Research & Development research assistants. Terms of use ------------ The use of this model is governed by theNVIDIA NeMo Foundational Models Evaluation License Agreement NVIDIA NeMo Foundational Models Evaluation License Agreement (https://registry.ngc.nvidia.com/orgs/ohlfw0olaadg/teams/ea-participants/resources/nemo_foundational_models_evaluation_license/files) Model Architecture: Mistral-4B Ranker ------------------------------------- Architecture Type:TransformerNetwork Architecture:Fine-tuned Mistral-7B-v0.1 LLM (only first 16 layers) Architecture Type: Network Architecture: The NVIDIA Retrieval QA Ranking Model is a transformer encoder - a LoRA finetuned version ofMistral-7B-v0.1 LLMthat uses only the first 16 layers for higher throughput. The last embedding output by the decoder model is used as a pooling strategy, and a binary classification head is fine-tuned for the ranking task. Mistral-7B-v0.1 LLM (https://huggingface.co/mistralai/Mistral-7B-v0.1) Ranking models for text ranking are typically trained using a cross-encoder architecture for sentence classification. This involves predicting a pair of sentences (for example, query and chunked passages). The Binary CrossEntropy loss is used to maximize the likelihood for passages containing information to answer the query and minimize the likelihood for passages which do not contain information to answer the query. We train the model on private and public datasets described in the Dataset and Training section. Intended use ------------ The NVIDIA Retrieval QA Ranking model is most suitable for users who want to improve their retrieval systems by reranking a set of candidates for a given question. Input ----- Input Type:Pair of textsInput Format:list of text pairs Input Type: Input Format: Output ------ Output Type:floatsOutput Format:list of floats, each the probability score (or raw logits). The user can decide if a Sigmoid activation function is applied to the logits. Output Type: Output Format: Model Version(s) ---------------- NVIDIA Retrieval QA Text Reranking Mistral 4B-1 Training Dataset & Evaluation ============================= Training Dataset ---------------- The development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset namedMSMARCOrestricts ‌commercial licensing, limiting the use of these models in commercial settings. To address this, we created our own internal open-domain QA dataset to train a commercially-viable text qa models. For NVIDIA proprietary data collection, we searched the passages from web logs and selected a collection of passages relevant to customer use cases for annotation by the NVIDIA internal data annotation team. MSMARCO (https://microsoft.github.io/msmarco/) The training dataset details are as follows: Use Case:Information retrieval for question and answering over text documents. Use Case: Data Sources: • Public datasets licensed for commercial use. • Text from public websites. • Annotations created by NVIDIA’s internal team. Language:English (US)Domains:Knowledge, Description, Numeric (unit, time), Entity, Location, PersonVolume:400k samples from public datasetHigh Level Schema: Language: Domains: Volume: High Level Schema: • query: question text • doc: full document that contains the answer • chunk: section of the document that contains the answer • relevancy label: rating of how relevant the passage is to the question • span: exact token range in the chunk that contains the answer Evaluation Results ------------------ We evaluated the NVIDIA Retrieval QA Ranking Models in comparison to literature open & commercial retriever models on academic benchmarks -NQ,HotpotQAandFiQA(Finance Q&A)from BeIR benchmark. In this benchmark, the metric used was Recall@5. As described, we need to apply the ranking model on the output of a embedding model. NQ (https://huggingface.co/datasets/BeIR/nq) HotpotQA (https://huggingface.co/datasets/hotpot_qa) FiQA(Finance Q&A) (https://huggingface.co/datasets/BeIR/fiqa) We also evaluated our embedding model with real internal customer datasets from telco, IT, consulting, and energy industries. The metric was Recall@5, to emulate a retrieval augmented generation (RAG) scenario where we would provide the top five most relevant passages as context in the prompt for the LLM model that is going to respond to the question. We compared our model’s information retrieval accuracy to a number of well-known embedding models made available by the AI community, including ones trained on non-commercial dataset (which are marked with \"*\"). Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcardshere. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nvolve-29k/bias) here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama3-8b-instruct",
    "nvidia_url": "https://build.nvidia.com/meta/llama3-8b",
    "scraped_on": "2025-10-15T21:44:59.396828",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "large language models",
      "meta",
      "text-to-text"
    ],
    "model_card": "Model Description ================= Llama 3 is a large language AI model comprising a collection of models capable of generating text and code in response to prompts. Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. Model DeveloperMeta Model Developer LicenseA custom commercial license is available at:https://llama.meta.com/llama3/license License https://llama.meta.com/llama3/license Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the modelREADME. For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please gohere. README (https://github.com/meta-llama/llama3) here (https://github.com/meta-llama/llama-recipes) Model Release DateApril 18, 2024. Model Release Date StatusThis is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback. Status Llama 3 Family of Models. Llama 3 Family of Models Intended Use ============ Intended Use CasesLlama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. Intended Use Cases Out-of-scopeUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**. Out-of-scope **Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy. Model Architecture • Architecture Type: Transformer • Network Architecture: Llama 3 Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Input • Input Format: Text • Input Parameters: Temperature, TopP Output • Output Format: Text and code • Output Parameters: Max output tokens Intended Use ============ Intended Use CasesLlama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. Intended Use Cases Out-of-scopeUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**. Out-of-scope **Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy. Model Architecture • Architecture Type: Transformer • Network Architecture: Llama 3 Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Input • Input Format: Text • Input Parameters: Temperature, TopP Output • Output Format: Text and code • Output Parameters: Max output tokens Training Data Params Context length GQA Token count Knowledge cutoff Hardware and Software ===================== Inference: ========== Engine:TritonTest Hardware: Engine: Test Hardware: • H100 Training FactorsWe used custom training libraries, Meta's Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute. Training Factors Carbon Footprint Pretraining utilized a cumulative7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Meta’s sustainability program. Carbon Footprint Pretraining utilized a cumulative Time (GPU hours) Power Consumption (W) Carbon Emitted(tCO2eq) CO2 emissions during pre-training. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others. CO2 emissions during pre-training Training Dataset ================ OverviewLlama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data. Overview Data FreshnessThe pretraining data has a cutoff of March 2023 for the 7B and December 2023 for the 70B models respectively. Data Freshness Benchmarks ========== In this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology seehere. here (https://github.com/meta-llama/llama3/blob/main/eval_methodology.md) Base pretrained models ---------------------- Category Benchmark Llama 3 8B Llama2 7B Llama2 13B Llama 3 70B Llama2 70B Instruction tuned models ------------------------ Benchmark Llama 3 8B Llama 2 7B Llama 2 13B Llama 3 70B Llama 2 70B Responsibility & Safety ----------------------- We believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community. Foundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications. Rather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience. As part of the Llama 3 release, we updated ourResponsible Use Guideto outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources includingMeta Llama Guard 2andCode Shieldsafeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide aReference Implementationto get you started. Responsible Use Guide (https://llama.meta.com/responsible-use-guide/) Meta Llama Guard 2 (https://llama.meta.com/purple-llama/) Code Shield (https://llama.meta.com/purple-llama/) Reference Implementation (https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai) Llama 3-Instruct ---------------- As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. Safety For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous, and interpretable. Refusals In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. Responsible Release ------------------- In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. Misuse If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found athttps://llama.meta.com/llama3/use-policy/. https://llama.meta.com/llama3/use-policy/ Critical Risks -------------- CBRNE(Chemical, Biological, Radiological, Nuclear, and high yield Explosives) We have conducted a two fold assessment of the safety of the model in this area: • Iterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks. • Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model). Cyber Security -------------- We have evaluated Llama 3 with CyberSecEval, Meta’s cybersecurity safety eval suite, measuring Llama 3’s propensity to suggest insecure code when used as a coding assistant, and Llama 3’s propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models ofequivalent coding capability. equivalent coding capability (https://huggingface.co/spaces/facebook/CyberSecEval) Child Safety ------------ Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. Community --------- Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository. Github repository (https://github.com/meta-llama/PurpleLlama) Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community. output reporting mechanism (https://developers.facebook.com/llama_output_feedback) bug bounty program (https://www.facebook.com/whitehat) Ethical Considerations and Limitations ====================================== The core values of Llama 3 are openness, inclusivity, and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. But Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporatingPurple Llamasolutions into your workflows and specificallyLlama Guardwhich provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety. Purple Llama (https://github.com/facebookresearch/PurpleLlama) Llama Guard (https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) Please see the Responsible Use Guide available athttp://llama.meta.com/responsible-use-guide http://llama.meta.com/responsible-use-guide Citation Instructions ===================== @article{llama3modelcard, title={Llama 3 Model Card}, author={AI@Meta}, year={2024}, url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md} https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md} (https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md%7D) } Contributors ============ Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos"
  },
  {
    "name": "llama3-70b-instruct",
    "nvidia_url": "https://build.nvidia.com/meta/llama3-70b",
    "scraped_on": "2025-10-15T21:44:55.954027",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "large language models",
      "meta",
      "text-to-text"
    ],
    "model_card": "Model Description ================= Llama 3 is a large language AI model comprising a collection of models capable of generating text and code in response to prompts. Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. Model DeveloperMeta Model Developer LicenseA custom commercial license is available at:https://llama.meta.com/llama3/license License https://llama.meta.com/llama3/license Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the modelREADME. For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please gohere. README (https://github.com/meta-llama/llama3) here (https://github.com/meta-llama/llama-recipes) Model Release DateApril 18, 2024. Model Release Date StatusThis is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback. Status Llama 3 Family of Models. Llama 3 Family of Models Intended Use ============ Intended Use CasesLlama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. Intended Use Cases Out-of-scopeUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**. Out-of-scope **Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy. Model Architecture • Architecture Type: Transformer • Network Architecture: Llama 3 Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Input • Input Format: Text • Input Parameters: Temperature, TopP Output • Output Format: Text and code • Output Parameters: Max output tokens Training Data Params Context length GQA Token count Knowledge cutoff Hardware and Software ===================== Inference: ========== Engine:TritonTest Hardware: Engine: Test Hardware: • H100 Training FactorsWe used custom training libraries, Meta's Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute. Training Factors Carbon Footprint Pretraining utilized a cumulative7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Meta’s sustainability program. Carbon Footprint Pretraining utilized a cumulative Time (GPU hours) Power Consumption (W) Carbon Emitted(tCO2eq) CO2 emissions during pre-training. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others. CO2 emissions during pre-training Training Dataset ================ OverviewLlama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data. Overview Data FreshnessThe pretraining data has a cutoff of March 2023 for the 7B and December 2023 for the 70B models respectively. Data Freshness Benchmarks ========== In this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology seehere. here (https://github.com/meta-llama/llama3/blob/main/eval_methodology.md) Base pretrained models ---------------------- Category Benchmark Llama 3 8B Llama2 7B Llama2 13B Llama 3 70B Llama2 70B Instruction tuned models ------------------------ Benchmark Llama 3 8B Llama 2 7B Llama 2 13B Llama 3 70B Llama 2 70B Responsibility & Safety ----------------------- We believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community. Foundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications. Rather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience. As part of the Llama 3 release, we updated ourResponsible Use Guideto outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources includingMeta Llama Guard 2andCode Shieldsafeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide aReference Implementationto get you started. Responsible Use Guide (https://llama.meta.com/responsible-use-guide/) Meta Llama Guard 2 (https://llama.meta.com/purple-llama/) Code Shield (https://llama.meta.com/purple-llama/) Reference Implementation (https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai) Llama 3-Instruct ---------------- As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. Safety For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous, and interpretable. Refusals In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. Responsible Release ------------------- In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. Misuse If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found athttps://llama.meta.com/llama3/use-policy/. https://llama.meta.com/llama3/use-policy/ Critical risks -------------- CBRNE(Chemical, Biological, Radiological, Nuclear, and high yield Explosives) We have conducted a two fold assessment of the safety of the model in this area: • Iterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks. • Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model). Cyber Security -------------- We have evaluated Llama 3 with CyberSecEval, Meta’s cybersecurity safety eval suite, measuring Llama 3’s propensity to suggest insecure code when used as a coding assistant, and Llama 3’s propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models ofequivalent coding capability. equivalent coding capability (https://huggingface.co/spaces/facebook/CyberSecEval) Child Safety ------------ Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. Community --------- Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository. Github repository (https://github.com/meta-llama/PurpleLlama) Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community. output reporting mechanism (https://developers.facebook.com/llama_output_feedback) bug bounty program (https://www.facebook.com/whitehat) Ethical Considerations and Limitations ====================================== The core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. But Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporatingPurple Llamasolutions into your workflows and specificallyLlama Guardwhich provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety. Purple Llama (https://github.com/facebookresearch/PurpleLlama) Llama Guard (https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) Please see the Responsible Use Guide available athttp://llama.meta.com/responsible-use-guide http://llama.meta.com/responsible-use-guide Citation Instructions ===================== @article{llama3modelcard, title={Llama 3 Model Card}, author={AI@Meta}, year={2024}, url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md} https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md} (https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md%7D) } Contributors ============ Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos"
  },
  {
    "name": "mixtral-8x22b-instruct-v0.1",
    "nvidia_url": "https://build.nvidia.com/mistralai/mixtral-8x22b-instruct",
    "scraped_on": "2025-10-15T21:44:52.849629",
    "tags": [
      "advanced reasoning",
      "chat",
      "code generation",
      "large language models",
      "mistralai",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description: ============ Mixtral 8x22B is MistralAI's latest open model. It sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size. Mixtral 8x22B comes with the following strengths: • It is fluent in English, French, Italian, German, and Spanish • It has strong mathematics and coding capabilities • It is natively capable of function calling; along with the constrained output mode implemented on la Plateforme, this enables application development and tech stack modernisation at scale • Its 64K tokens context window allows precise information recall from large documents Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see Mixtral 8x22b'sModel Card. Model Card (https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1) Terms of use ============ By using this software or model, you are agreeing to theterms and conditionsof the license, acceptable use policy and Mistral's privacy policy. Mixtral-8x22B is released under the Apache 2.0 license terms and conditions (https://mistral.ai/terms-of-service/) References(s): ============== Mixtral 8x22B InstructModel Cardon Hugging FaceCheaper, Better, Faster, Stronger | Mistral AI Model Card (https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1) Cheaper, Better, Faster, Stronger | Mistral AI (https://mistral.ai/news/mixtral-8x22b/) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Sparse Mixture of GPT-based expertsModel Version:0.1 Architecture Type: Network Architecture: Model Version: Input: ====== Input Format:TextInput Parameters:Temperature, Top P, Max Output Tokens Input Format: Input Parameters: Output: ======= Output Format:TextOutput Parameters:None Output Format: Output Parameters: Software Integration: ===================== Supported Hardware Platform(s):Hopper, Ampere, Turing, AdaSupported Operating System(s):Linux Supported Hardware Platform(s): Supported Operating System(s): Inference: ========== Engine:TritonTest Hardware:Other Engine: Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware:"
  },
  {
    "name": "phi-3-mini-128k-instruct",
    "nvidia_url": "https://build.nvidia.com/microsoft/phi-3-mini",
    "scraped_on": "2025-10-15T21:44:49.755861",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "large language models",
      "microsoft",
      "text-to-text"
    ],
    "model_card": "Model Summary ============= (Phi-3 Repo) Model Card (https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) MIT (https://opensource.org/license/mit) Intended Use ============ Our models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. Usage ===== Input formats ============= Given the nature of the training data, the Phi-3 Mini-128K-Instruct model is best suited for prompts using the chat format as follows: <|system|>You are a helpful assistant.<|end|> <|user|>How to explain Internet for a medieval knight?<|end|><|assistant|> Loading the model locally ========================= After obtaining the Phi-3 Mini-128K-Instruct model checkpoints, users can use this sample code for inference. import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline torch.random.manual_seed(0) model = AutoModelForCausalLM.from_pretrained( \"microsoft/Phi-3-Mini-128K-instruct\", device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-Mini-128K-instruct\") messages = [ {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, ] pipe = pipeline( \"text-generation\", model=model, tokenizer=tokenizer, ) generation_args = { \"max_new_tokens\": 500, \"return_full_text\": False, \"temperature\": 0.0, \"do_sample\": False, } output = pipe(messages, **generation_args) print(output[0]['generated_text']) Cross Platform Support ====================== ONNX runtimenow supports Phi-3 mini models across platforms and hardware. ONNX runtime (http://aka.ms/phi3-optimizations) Optimized phi-3 models are also published here in ONNX format, to run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the precision best suited to each of these targets. DirectML GPU acceleration is supported for Windows desktops GPUs (AMD, Intel, and NVIDIA). Along with DML, ONNX Runtime provides cross platform support for Phi3 mini across a range of devices CPU, GPU, and mobile. Here are some of the optimized configurations we have added: • ONNX models for int4 DML: Quantized to int4 via AWQ ONNX models for int4 DML: Quantized to int4 via AWQ • ONNX model for fp16 CUDA ONNX model for fp16 CUDA • ONNX model for int4 CUDA: Quantized to int4 via RTN ONNX model for int4 CUDA: Quantized to int4 via RTN • ONNX model for int4 CPU and Mobile: Quantized to int4 via R ONNX model for int4 CPU and Mobile: Quantized to int4 via R Data Overview ============= Training datasets ================= Our training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of 1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; 2) newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); 3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. We are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in thePhi-3 Technical Report. Phi-3 Technical Report. (https://arxiv.org/pdf/2404.14219) Benchmark datasets ================== We evaluated the model across a breadth of public and internal benchmarks to understand the model capabilities in the most comprehensive way under multiple tasks and conditions. More specifically, • Reasoning:Winogrande: commonsense reasoning around pronoun resolutionPIQA: physical commonsense reasoning around everyday situationsARC-challenge: grade-school multiple choice science questionsGPQA: very hard questions written and validated by experts in biology, physics, and chemistryMedQA: medical questions answeringSocial IQA: social commonsense intelligenceBoolQ: natural questions from contextTruthfulQA: grounded reasoning Reasoning: • Winogrande: commonsense reasoning around pronoun resolution Winogrande: commonsense reasoning around pronoun resolution • PIQA: physical commonsense reasoning around everyday situations PIQA: physical commonsense reasoning around everyday situations • ARC-challenge: grade-school multiple choice science questions ARC-challenge: grade-school multiple choice science questions • GPQA: very hard questions written and validated by experts in biology, physics, and chemistry GPQA: very hard questions written and validated by experts in biology, physics, and chemistry • MedQA: medical questions answering MedQA: medical questions answering • Social IQA: social commonsense intelligence Social IQA: social commonsense intelligence • BoolQ: natural questions from context BoolQ: natural questions from context • TruthfulQA: grounded reasoning TruthfulQA: grounded reasoning • Language understanding:HellaSwag: commonsense natural language inference around everyday eventsANLI: adversarial natural language inferenceLAMBADA: word prediction given a passage. Language understanding: • HellaSwag: commonsense natural language inference around everyday events HellaSwag: commonsense natural language inference around everyday events • ANLI: adversarial natural language inference ANLI: adversarial natural language inference • LAMBADA: word prediction given a passage. LAMBADA: word prediction given a passage. • World knowledge:TriviaQA: trivia question on general topics World knowledge: • TriviaQA: trivia question on general topics • Math:GSM8K: grade-school math word problemsGSM8K Hard: grade-school math word problems with large values and some absurdity.MATH: challenging competition math problems Math: • GSM8K: grade-school math word problems GSM8K: grade-school math word problems • GSM8K Hard: grade-school math word problems with large values and some absurdity. GSM8K Hard: grade-school math word problems with large values and some absurdity. • MATH: challenging competition math problems MATH: challenging competition math problems • Code:HumanEval, MBPP: python coding tasksSpider: SQL query tasks Code: • HumanEval, MBPP: python coding tasks HumanEval, MBPP: python coding tasks • Spider: SQL query tasks Spider: SQL query tasks • Multilingual:MGSM: multilingual grade-school mathMEGA: multilingual NLP tasks Multilingual: • MGSM: multilingual grade-school math MGSM: multilingual grade-school math • MEGA: multilingual NLP tasks MEGA: multilingual NLP tasks • Popular aggregated datasets: MMLU, BigBench-Hard, AGI Eval Popular aggregated datasets: MMLU, BigBench-Hard, AGI Eval • Multi-turn conversations:Data generated by In-house adversarial conversation simulation tool Multi-turn conversations: • Data generated by In-house adversarial conversation simulation tool • Single-turn trustworthiness evaluation:DecodingTrust: a collection of trustworthiness benchmark in eight different perspectivesXSTest: exaggerated safety evaluationToxigen: adversarial and hate speech detection Single-turn trustworthiness evaluation: • DecodingTrust: a collection of trustworthiness benchmark in eight different perspectives DecodingTrust: a collection of trustworthiness benchmark in eight different perspectives • XSTest: exaggerated safety evaluation XSTest: exaggerated safety evaluation • Toxigen: adversarial and hate speech detection Toxigen: adversarial and hate speech detection • Red Team:Responses to prompts provided by AI Red Team at Microsoft Red Team: • Responses to prompts provided by AI Red Team at Microsoft Safety ====== Approach ======== Phi-3 family of models has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated datasets. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and a modified version of RLHF (Reinforcement Learning from Human Feedback) by utilizing human-labeled and synthetic datasets, including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories. Safety Evaluation and Red-Teaming ================================= Prior to release, Phi-3 family of models followed a multi-faceted evaluation approach. Quantitative evaluation was conducted with multiple open-source safety benchmarks and in-house tools utilizing adversarial conversation simulation. For qualitative safety evaluation, we collaborated with the AI Red Team at Microsoft to assess safety risks posed by Phi-3-Medium in both average and adversarial user scenarios. The assessment was done in predetermined eight risk categories with automated scoring followed by thorough manual reviews of the model responses. Please refer to the technical report for more details of our safety alignment. ============================================================================== Model Quality ============= To understand the capabilities, we compare Phi-3 Mini with a set of models over a variety of benchmarks using our internal benchmark platform (SeeAppendix Afor benchmark methodology). At the high-level overview of the model quality on representative benchmarks: Appendix A 66.4 56.0 56.4 64.4 65.5 70.3 We take a closer look at different categories across 100 public benchmark datasets at the table below: Overall, the model with only 3.8B-param achieves a similar level of language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, we believe such weakness can be resolved by augmenting Phi-3-Mini with a search engine. Release Notes This is an update over the original instruction-tuned Phi-3-mini release based on valuable customer feedback. The model used additional post-training data leading to substantial gains on long-context understanding, instruction following, and structure output. We also improve multi-turn conversation quality, explicitly support <|system|> tag, and significantly improve reasoning capability. We believe most use cases will benefit from this release, but we encourage users to test in their particular AI applications. We appreciate the enthusiastic adoption of the Phi-3 model family, and continue to welcome all feedback from the community. Instruction Following, Structure Output, and Reasoning ------------------------------------------------------ The table below highlights improvements on instruction following and structure output of the new release on our internal benchmark datasets. 25.7 37.3 Long Context ------------ Phi-3 Mini-128K-Instruct supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA. We see that Phi-3 Mini-128K-Instruct is better than Llama-3-8B-Instruct, Mistral-7B, and Mixtral 8x7B models. 25.9 13.6 19.0 10.3 Ruler: a retrieval-based benchmark for long context understanding Ruler (https://github.com/hsiehjackson/RULER) Average 68.8 84.6 RepoQA: a benchmark for long context code understanding RepoQA (https://evalplus.github.io/repoqa.html) Average 32.4 77 Responsible AI Considerations ============================= Like other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: • Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. • Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. • Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. • Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. • Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses. Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses. Developers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: • Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. • High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. • Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). • Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. • Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Appendix A: Benchmark Methodology ================================= We include a brief word on methodology here - and in particular, how we think about optimizing prompts. In an ideal world, we wouldnever change any promptsin our benchmarks to ensure it is always an apples-to-apples comparison when comparing different models. Indeed, this is our default approach, and is the case in the vast majority of models we have run to date. never change any prompts There are, however, some exceptions to this. In some cases, we see a model that performs worse than expected on a given evaldue to a failure to respect the output format. For example: due to a failure to respect the output format • A Claude model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with “Sure, I can help with that. …” which may break the parser. In such cases, we have opted to try differentsystem messages(e.g. “You must always respond to a question” or “Get to the point!”). A Claude model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with “Sure, I can help with that. …” which may break the parser. In such cases, we have opted to try differentsystem messages(e.g. “You must always respond to a question” or “Get to the point!”). system messages • With LLaMA-1 models, we observed that few shots actually hurt model performance. In this case we did allow running the benchmarks with 0-shots for all cases. With LLaMA-1 models, we observed that few shots actually hurt model performance. In this case we did allow running the benchmarks with 0-shots for all cases. • We have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, we do allow for model-specific mappings for chat to completion prompts. We have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, we do allow for model-specific mappings for chat to completion prompts. However,we do not: we do not • Pick different few-shot examples. Few shots will always be the same when comparing different models. Pick different few-shot examples. Few shots will always be the same when comparing different models. • Change prompt format: e.g. if it is an A/B/C/D multiple choice, we do not tweak this to 1/2/3/4 multiple choice. Change prompt format: e.g. if it is an A/B/C/D multiple choice, we do not tweak this to 1/2/3/4 multiple choice."
  },
  {
    "name": "arctic-embed-l",
    "nvidia_url": "https://build.nvidia.com/snowflake/arctic-embed-l",
    "scraped_on": "2025-10-15T21:44:46.696368",
    "tags": [
      "embedding",
      "nemo retriever",
      "retrieval augmented generation",
      "snowflake",
      "text-to-embedding"
    ],
    "model_card": "Model Overview ============== Description ----------- snowflake-arctic-embed is a suite of text embedding models that creates high-quality retrieval models optimized for performance. These models are ready for commercial use free-of-charge. The snowflake-arctic-embedding models achieve state-of-the-art performance on the MTEB/BEIR leaderboard for each of their size variants. As shown below, each class of model size achieves SOTA retrieval accuracy compared to other top models. The models are trained by leveraging existing open-source text representation models, such as bert-base-uncased, and are trained in a multi-stage pipeline to optimize their retrieval performance. Following pretraining, models are further optimized with long training on a smaller dataset (about 1m samples) of triplets of query, positive document, and negative document derived from hard harmful mining. Mining of the negatives and data curation is crucial to retrieval accuracy. snowflake-arctic-embed-xs (https://huggingface.co/Snowflake/snowflake-arctic-embed-xs/) snowflake-arctic-embed-s (https://huggingface.co/Snowflake/snowflake-arctic-embed-s/) snowflake-arctic-embed-m (https://huggingface.co/Snowflake/snowflake-arctic-embed-m/) snowflake-arctic-embed-m-long (https://huggingface.co/Snowflake/snowflake-arctic-embed-m-long/) snowflake-arctic-embed-l (https://huggingface.co/Snowflake/snowflake-arctic-embed-l/) Based on theintfloat/e5-large-unsupervisedmodel, the large model is a direct drop-in for closed APIs and delivers the most accurate retrieval experience. intfloat/e5-large-unsupervised (https://huggingface.co/intfloat/e5-large-unsupervised) snowflake-arctic-embed-l (https://huggingface.co/Snowflake/snowflake-arctic-embed-l/) Terms of use ------------ Arctic is licensed under theApache-2. Apache-2 (https://www.apache.org/licenses/LICENSE-2.0) References ---------- HuggingFace HuggingFace (https://huggingface.co/Snowflake/snowflake-arctic-embed-l/) Github Github (https://github.com/Snowflake-Labs/arctic-embed) Blog post Blog post (https://www.snowflake.com/blog/introducing-snowflake-arctic-embed-snowflakes-state-of-the-art-text-embedding-family-of-models/) Model Architecture ================== Architecture Type:TransformerNetwork Architecture:Fine-tuned E5-Large-Unsupervised RetrieverEmbedding Dimension:1024Parameter Count:335 million Architecture Type: Network Architecture: Embedding Dimension: Parameter Count: Input ===== Input Type:TextInput Format:List of strings Input Type: Input Format: Output ====== Output Type:Floating PointsOutput Format:list of float arraysOther Properties Related to Output:Each array contains the embeddings for the corresponding input string. Output Type: Output Format: Other Properties Related to Output: Model Version ============= snowflake-arctic-embed-l Supported Operating System(s): ============================== • Linux Training Dataset: ================= Properties (Quantity, Dataset Descriptions, Sensor(s)):Pretrained on large batches of query-document pairs where negatives are derived in-batch—pretraining leverages about 400m samples of a mix of public datasets and proprietary web search data. Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference: ========== Engine:TensorRT-LLMwithTritonTest Hardware:L40 Engine: TensorRT-LLM (https://github.com/NVIDIA/TensorRT-LLM) Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware:"
  },
  {
    "name": "phi-3-mini-4k-instruct",
    "nvidia_url": "https://build.nvidia.com/microsoft/phi-3-mini-4k",
    "scraped_on": "2025-10-15T21:44:43.269466",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "large language models",
      "microsoft",
      "text-to-text"
    ],
    "model_card": "Model Summary ============= (Phi-3 Repo) Model Card (https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) MIT (https://opensource.org/license/mit) Intended Use ============ Our models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. Usage ===== Input formats ============= Given the nature of the training data, the Phi-3 Mini-4K-Instruct model is best suited for prompts using the chat format as follows: <|system|>You are a helpful assistant.<|end|> <|user|>How to explain Internet for a medieval knight?<|end|><|assistant|> Loading the model locally ========================= After obtaining the Phi-3 Mini-4K-Instruct model checkpoints, users can use this sample code for inference. import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline torch.random.manual_seed(0) model = AutoModelForCausalLM.from_pretrained( \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True, ) tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") messages = [ {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, ] pipe = pipeline( \"text-generation\", model=model, tokenizer=tokenizer, ) generation_args = { \"max_new_tokens\": 500, \"return_full_text\": False, \"temperature\": 0.0, \"do_sample\": False, } output = pipe(messages, **generation_args) print(output[0]['generated_text']) Cross Platform Support ====================== ONNX runtimenow supports Phi-3 mini models across platforms and hardware. ONNX runtime (http://aka.ms/phi3-optimizations) Optimized phi-3 models are also published here in ONNX format, to run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the precision best suited to each of these targets. DirectML GPU acceleration is supported for Windows desktops GPUs (AMD, Intel, and NVIDIA). Along with DML, ONNX Runtime provides cross platform support for Phi3 mini across a range of devices CPU, GPU, and mobile. Here are some of the optimized configurations we have added: • ONNX models for int4 DML: Quantized to int4 via AWQ ONNX models for int4 DML: Quantized to int4 via AWQ • ONNX model for fp16 CUDA ONNX model for fp16 CUDA • ONNX model for int4 CUDA: Quantized to int4 via RTN ONNX model for int4 CUDA: Quantized to int4 via RTN • ONNX model for int4 CPU and Mobile: Quantized to int4 via R ONNX model for int4 CPU and Mobile: Quantized to int4 via R Data Overview ============= Training datasets ================= Our training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of 1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; 2) newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); 3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. We are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in thePhi-3 Technical Report. Phi-3 Technical Report. (https://arxiv.org/pdf/2404.14219) Benchmark datasets ================== We evaluated the model across a breadth of public and internal benchmarks to understand the model capabilities in the most comprehensive way under multiple tasks and conditions. More specifically, • Reasoning:Winogrande: commonsense reasoning around pronoun resolutionPIQA: physical commonsense reasoning around everyday situationsARC-challenge: grade-school multiple choice science questionsGPQA: very hard questions written and validated by experts in biology, physics, and chemistryMedQA: medical questions answeringSocial IQA: social commonsense intelligenceBoolQ: natural questions from contextTruthfulQA: grounded reasoning Reasoning: • Winogrande: commonsense reasoning around pronoun resolution Winogrande: commonsense reasoning around pronoun resolution • PIQA: physical commonsense reasoning around everyday situations PIQA: physical commonsense reasoning around everyday situations • ARC-challenge: grade-school multiple choice science questions ARC-challenge: grade-school multiple choice science questions • GPQA: very hard questions written and validated by experts in biology, physics, and chemistry GPQA: very hard questions written and validated by experts in biology, physics, and chemistry • MedQA: medical questions answering MedQA: medical questions answering • Social IQA: social commonsense intelligence Social IQA: social commonsense intelligence • BoolQ: natural questions from context BoolQ: natural questions from context • TruthfulQA: grounded reasoning TruthfulQA: grounded reasoning • Language understanding:HellaSwag: commonsense natural language inference around everyday eventsANLI: adversarial natural language inferenceLAMBADA: word prediction given a passage. Language understanding: • HellaSwag: commonsense natural language inference around everyday events HellaSwag: commonsense natural language inference around everyday events • ANLI: adversarial natural language inference ANLI: adversarial natural language inference • LAMBADA: word prediction given a passage. LAMBADA: word prediction given a passage. • World knowledge:TriviaQA: trivia question on general topics World knowledge: • TriviaQA: trivia question on general topics • Math:GSM8K: grade-school math word problemsGSM8K Hard: grade-school math word problems with large values and some absurdity.MATH: challenging competition math problems Math: • GSM8K: grade-school math word problems GSM8K: grade-school math word problems • GSM8K Hard: grade-school math word problems with large values and some absurdity. GSM8K Hard: grade-school math word problems with large values and some absurdity. • MATH: challenging competition math problems MATH: challenging competition math problems • Code:HumanEval, MBPP: python coding tasksSpider: SQL query tasks Code: • HumanEval, MBPP: python coding tasks HumanEval, MBPP: python coding tasks • Spider: SQL query tasks Spider: SQL query tasks • Multilingual:MGSM: multilingual grade-school mathMEGA: multilingual NLP tasks Multilingual: • MGSM: multilingual grade-school math MGSM: multilingual grade-school math • MEGA: multilingual NLP tasks MEGA: multilingual NLP tasks • Popular aggregated datasets: MMLU, BigBench-Hard, AGI Eval Popular aggregated datasets: MMLU, BigBench-Hard, AGI Eval • Multi-turn conversations:Data generated by In-house adversarial conversation simulation tool Multi-turn conversations: • Data generated by In-house adversarial conversation simulation tool • Single-turn trustworthiness evaluation:DecodingTrust: a collection of trustworthiness benchmark in eight different perspectivesXSTest: exaggerated safety evaluationToxigen: adversarial and hate speech detection Single-turn trustworthiness evaluation: • DecodingTrust: a collection of trustworthiness benchmark in eight different perspectives DecodingTrust: a collection of trustworthiness benchmark in eight different perspectives • XSTest: exaggerated safety evaluation XSTest: exaggerated safety evaluation • Toxigen: adversarial and hate speech detection Toxigen: adversarial and hate speech detection • Red Team:Responses to prompts provided by AI Red Team at Microsoft Red Team: • Responses to prompts provided by AI Red Team at Microsoft Safety ====== Approach ======== Phi-3 family of models has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated datasets. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and a modified version of RLHF (Reinforcement Learning from Human Feedback) by utilizing human-labeled and synthetic datasets, including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories. Safety Evaluation and Red-Teaming ================================= Prior to release, Phi-3 family of models followed a multi-faceted evaluation approach. Quantitative evaluation was conducted with multiple open-source safety benchmarks and in-house tools utilizing adversarial conversation simulation. For qualitative safety evaluation, we collaborated with the AI Red Team at Microsoft to assess safety risks posed by Phi-3-Medium in both average and adversarial user scenarios. The assessment was done in predetermined eight risk categories with automated scoring followed by thorough manual reviews of the model responses. Please refer to the technical report for more details of our safety alignment. ============================================================================== Model Quality ============= To understand the capabilities, we compare Phi-3 Mini with a set of models over a variety of benchmarks using our internal benchmark platform (SeeAppendix Afor benchmark methodology). At the high-level overview of the model quality on representative benchmarks: Appendix A 67.6 56.0 56.4 64.4 65.5 70.4 We take a closer look at different categories across 80 public benchmark datasets at the table below: Overall, the model with only 3.8B-param achieves a similar level of language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, we believe such weakness can be resolved by augmenting Phi-3-Mini with a search engine. Release Notes This is an update over the original instruction-tuned Phi-3-mini release based on valuable customer feedback. The model used additional post-training data leading to substantial gains on instruction following and structure output. We also improve multi-turn conversation quality, explicitly support <|system|> tag, and significantly improve reasoning capability. We believe most use cases will benefit from this release, but we encourage users to test in their particular AI applications. We appreciate the enthusiastic adoption of the Phi-3 model family, and continue to welcome all feedback from the community. The table below highlights improvements on instruction following, structure output, and reasoning of the new release on our internal benchmark datasets. 21.9 36.7 Responsible AI Considerations ============================= Like other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: • Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. • Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. • Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. • Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. • Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses. Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses. Developers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: • Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. • High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. • Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). • Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. • Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Appendix A: Benchmark Methodology ================================= We include a brief word on methodology here - and in particular, how we think about optimizing prompts. In an ideal world, we wouldnever change any promptsin our benchmarks to ensure it is always an apples-to-apples comparison when comparing different models. Indeed, this is our default approach, and is the case in the vast majority of models we have run to date. never change any prompts There are, however, some exceptions to this. In some cases, we see a model that performs worse than expected on a given evaldue to a failure to respect the output format. For example: due to a failure to respect the output format • A Claude model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with “Sure, I can help with that. …” which may break the parser. In such cases, we have opted to try differentsystem messages(e.g. “You must always respond to a question” or “Get to the point!”). A Claude model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with “Sure, I can help with that. …” which may break the parser. In such cases, we have opted to try differentsystem messages(e.g. “You must always respond to a question” or “Get to the point!”). system messages • With LLaMA-1 models, we observed that few shots actually hurt model performance. In this case we did allow running the benchmarks with 0-shots for all cases. With LLaMA-1 models, we observed that few shots actually hurt model performance. In this case we did allow running the benchmarks with 0-shots for all cases. • We have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, we do allow for model-specific mappings for chat to completion prompts. We have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, we do allow for model-specific mappings for chat to completion prompts. However,we do not: we do not • Pick different few-shot examples. Few shots will always be the same when comparing different models. Pick different few-shot examples. Few shots will always be the same when comparing different models. • Change prompt format: e.g. if it is an A/B/C/D multiple choice, we do not tweak this to 1/2/3/4 multiple choice. Change prompt format: e.g. if it is an A/B/C/D multiple choice, we do not tweak this to 1/2/3/4 multiple choice."
  },
  {
    "name": "sea-lion-7b-instruct",
    "nvidia_url": "https://build.nvidia.com/aisingapore/sea-lion-7b-instruct",
    "scraped_on": "2025-10-15T21:44:40.193802",
    "tags": [
      "aisingapore",
      "chat",
      "large language models",
      "regional language generation",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description =========== SEA-LION-7B-Instruct is a multilingual model for natural language understanding (NLU), natural language generation (NLG), and natural language reasoning (NLR) tasks that has been fine-tuned with thousands of English and Indonesian instruction-completion pairs alongside a smaller pool of instruction-completion pairs from other Association of Southeast Asian Nations (ASEAN) languages. These instructions have been carefully curated and rewritten to ensure the model was trained on truly open, commercially permissive and high quality datasets. This model is for demonstration purposes and not-for-production usage. SEA-LION stands for Southeast Asian Languages In One Network. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to theSEA-LION Model Card. SEA-LION Model Card (https://huggingface.co/aisingapore/sea-lion-7b-instruct) License and Terms of use ======================== GOVERNING TERMS: Your use of this API is governed by theNVIDIA API Trial Service Terms of Use; and the use of this model is governed by theNVIDIA AI Foundation Models Community LicenseandMIT License. NVIDIA API Trial Service Terms of Use (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA AI Foundation Models Community License (https://docs.nvidia.com/ai-foundation-models-community-license.pdf) MIT License (https://opensource.org/license/MIT) Model Developer:Products Pillar, AI SingaporeModel Release Date:Jan 31, 2024. Model Developer: Model Release Date: Model Architecture • Architecture Type: Transformer • Network Architecture: MosaicML Pretrained Transformer (MPT) Input • Input Type: Text • Input Format: String • Input Parameters: max_tokens, temperature, top_p, stop, frequency_penalty, presence_penalty, seed Output • Output Type: Text • Output Format: String Training dataset ================ SEA-LION-7B-Instruct was trained on a wide range of instructions that were manually and stringently verified by our team. A large portion of the effort was dedicated to ensuring that each instruction-completion pair that the model sees is of a high quality and any errors were corrected and rewritten by native speakers or else dropped from our mix. In addition, special care was taken to ensure that the datasets used had commercially permissive licenses through verification with the original data source. Benchmark Performance ===================== SEA-LION-7B-Instruct was evaluated on the BHASA benchmark (arXivandGitHub) across a variety of tasks. arXiv (https://arxiv.org/abs/2309.06085v2) GitHub (https://github.com/aisingapore/bhasa) BHASA stands out amongst other evaluations for SEA languages for its holistic approach to evaluation, including not just traditional Natural Language Processing (NLP) benchmarking tasks (such as sentiment analysis and question answering), but also linguistic and cultural diagnostic tests which are meticulously handcrafted. The evaluation was done zero-shot with Indonesian prompts and only a sample of 100-1000 instances for each dataset was used as per the setting described in the BHASA paper. The scores shown in the table below have been adjusted to only consider answers provided in the appropriate language. 68.41 91.45 17.54 55.24 63.28 64.27 68.30 85.60 • For Natural Language Understanding (NLU) tasks, the model was tested on Sentiment Analysis (Sentiment) using the NusaX dataset, Question Answering (QA) using the TyDiQA dataset, and Toxicity Detection (Toxicity) using the Indonesian Multi-Label Hate Speech Detection dataset. The metrics used are F1 scores for all three tasks. • For Natural Language Generation (NLG) tasks, the model was tested on Machine Translation from English to Indonesian (Eng>Indo) and from Indonesian to English (Indo>Eng) using the FLORES-200 dataset, and Abstractive Summarization (Summary) using the XLSum dataset. The metrics used for Machine Translation and Abstractive Summarization are ChrF++ and ROUGE-L respectively. • For Natural Language Reasoning (NLR) tasks, the model was tested on Natural Language Inference (NLI) using the IndoNLI lay dataset and on Causal Reasoning (Causal) using the XCOPA dataset. The metrics are based on accuracy for both tasks. Software Integration: ===================== • Supported Hardware Platform(s): Lovelace [Preferred/Supported] Operating System(s): • Linux Model Version ============= SEA-LION-7B-Instruct Inference ========= Engine:Triton + TensorRT-LLMTest Hardware:L40S Engine: Test Hardware:"
  },
  {
    "name": "paligemma",
    "nvidia_url": "https://build.nvidia.com/google/google-paligemma",
    "scraped_on": "2025-10-15T21:44:36.938239",
    "tags": [
      "computer vision",
      "cv",
      "google",
      "image",
      "image-to-text",
      "language generation",
      "video",
      "vision assistant",
      "visual question answering",
      "vlm"
    ],
    "model_card": "Model Overview ============== Description: ============ The Google PaLIGemma-3B-mix model is a one-shot visual language understanding solution for image-to-text generation. This model is ready for commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Google's(PaliGemma Model Card. (PaliGemma Model Card (https://ai.google.dev/gemma/docs/paligemma/model-card) License, Acceptable Use, and Research Privacy Policy ==================================================== By using this model, you are agreeing to the terms and conditions of theLicense,Acceptable Use PolicyandGoogle Research Privacy Policy. License (https://github.com/google-research/google-research/blob/master/LICENSE) Acceptable Use Policy (https://policies.google.com/terms) Google Research Privacy Policy (https://policies.google.com/privacy) References(s): ============== • SigLIP paper SigLIP paper (https://arxiv.org/pdf/2303.15343) • Gemma paper Gemma paper (https://arxiv.org/pdf/2403.08295) • PaLIGemma on HuggingFace PaLIGemma on HuggingFace (https://huggingface.co/google/paligemma-3b-mix-224-jax) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:SigLIP + Gemma Architecture Type: Network Architecture: Input: ====== Input Format:Image + TextInput Parameters:Image: Red, Green, and Blue (RGB); Text: StringOther Properties Related to Input:Prompt to caption the image or a question. Input Format: Input Parameters: Other Properties Related to Input: Output: ======= Output Format:TextOutput Parameters:temperature, top_p, max_tokensOther Properties Related to Output:Stream Output Format: Output Parameters: Other Properties Related to Output: Supported Operating System(s): ============================== • Linux Inference: ========== Engine:TritonTest Hardware:Other Engine: Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware:"
  },
  {
    "name": "phi-3-medium-4k-instruct",
    "nvidia_url": "https://build.nvidia.com/microsoft/phi-3-medium-4k-instruct",
    "scraped_on": "2025-10-15T21:44:33.833329",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "large language models",
      "microsoft",
      "text-to-text"
    ],
    "model_card": "Phi-3 Medium-4K-Instruct Model Card Model Summary ============= MIT (https://opensource.org/license/mit) Intended Use ============ Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. Data Overview ============= Training datasets ================= The training data includes a wide variety of sources, totaling 4.8 trillion tokens (including 10% multilingual), and is a combination of 1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; 2) newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); 3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. Benchmark datasets ================== The authors evaluated the model across a breadth of public and internal benchmarks to understand the model capabilities in the most comprehensive way under multiple tasks and conditions. More specifically, • Reasoning:Winogrande: commonsense reasoning around pronoun resolutionPIQA: physical commonsense reasoning around everyday situationsARC-easy, ARC-challenge: grade-school multiple choice science questions at easy and challenge levelCommonsensQA: generic commonsense questionsMedQA: medical questions answeringSocial IQA: social commonsense intelligenceBoolQ: natural questions from contextTruthfulQA: grounded reasoning Reasoning: • Winogrande: commonsense reasoning around pronoun resolution Winogrande: commonsense reasoning around pronoun resolution • PIQA: physical commonsense reasoning around everyday situations PIQA: physical commonsense reasoning around everyday situations • ARC-easy, ARC-challenge: grade-school multiple choice science questions at easy and challenge level ARC-easy, ARC-challenge: grade-school multiple choice science questions at easy and challenge level • CommonsensQA: generic commonsense questions CommonsensQA: generic commonsense questions • MedQA: medical questions answering MedQA: medical questions answering • Social IQA: social commonsense intelligence Social IQA: social commonsense intelligence • BoolQ: natural questions from context BoolQ: natural questions from context • TruthfulQA: grounded reasoning TruthfulQA: grounded reasoning • Language understanding:HellaSwag: commonsense natural language inference around everyday eventsANLI: adversarial natural language inferenceLAMBADA: word prediction given a passage. Language understanding: • HellaSwag: commonsense natural language inference around everyday events HellaSwag: commonsense natural language inference around everyday events • ANLI: adversarial natural language inference ANLI: adversarial natural language inference • LAMBADA: word prediction given a passage. LAMBADA: word prediction given a passage. • World knowledge:Natural Questions: question about Wikipedia knowledgeTriviaQA: trivia question on general topics World knowledge: • Natural Questions: question about Wikipedia knowledge Natural Questions: question about Wikipedia knowledge • TriviaQA: trivia question on general topics TriviaQA: trivia question on general topics • Math:GSM8K: grade-school math word problemsGSM8K Hard: grade-school math word problems with large values and some absurdity.MATH: challenging competition math problems Math: • GSM8K: grade-school math word problems GSM8K: grade-school math word problems • GSM8K Hard: grade-school math word problems with large values and some absurdity. GSM8K Hard: grade-school math word problems with large values and some absurdity. • MATH: challenging competition math problems MATH: challenging competition math problems • Code:HumanEval, MBPP: python coding tasksSpider: SQL query tasks Code: • HumanEval, MBPP: python coding tasks HumanEval, MBPP: python coding tasks • Spider: SQL query tasks Spider: SQL query tasks • Multilingual:MGSM: multilingual grade-school mathMEGA: multilingual NLP tasks Multilingual: • MGSM: multilingual grade-school math MGSM: multilingual grade-school math • MEGA: multilingual NLP tasks MEGA: multilingual NLP tasks • Popular aggregated datasets: MMLU, BigBench-Hard, AGI Eval Popular aggregated datasets: MMLU, BigBench-Hard, AGI Eval • Multi-turn conversations:Data generated by In-house adversarial conversation simulation tool Multi-turn conversations: • Data generated by In-house adversarial conversation simulation tool • Single-turn trustworthiness evaluation:DecodingTrust: a collection of trustworthiness benchmark in eight different perspectivesXSTest: exaggerated safety evaluationToxigen: adversarial and hate speech detection Single-turn trustworthiness evaluation: • DecodingTrust: a collection of trustworthiness benchmark in eight different perspectives DecodingTrust: a collection of trustworthiness benchmark in eight different perspectives • XSTest: exaggerated safety evaluation XSTest: exaggerated safety evaluation • Toxigen: adversarial and hate speech detection Toxigen: adversarial and hate speech detection • Red Team:Responses to prompts provided by AI Red Team at Microsoft Red Team: • Responses to prompts provided by AI Red Team at Microsoft Safety ====== Approach ======== Phi-3 family of models has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated datasets. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and a modified version of RLHF (Reinforcement Learning from Human Feedback) by utilizing human-labeled and synthetic datasets, including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories. Safety Evaluation and Red-Teaming ================================= Prior to release, Phi-3 family of models followed a multi-faceted evaluation approach. Quantitative evaluation was conducted with multiple open-source safety benchmarks and in-house tools utilizing adversarial conversation simulation. For qualitative safety evaluation, the authors collaborated with the AI Red Team at Microsoft to assess safety risks posed by Phi-3-Medium in both average and adversarial user scenarios. The assessment was done in predetermined eight risk categories with automated scoring followed by thorough manual reviews of the model responses. Please refer to the technical report for more details of the safety alignment. ============================================================================== Model Quality ============= To understand the capabilities, the authors compare Phi-3 Medium with a set of models over a variety of benchmarks using the internal benchmark platform BabelBench (SeeAppendix Afor benchmark methodology). Appendix A At the high-level overview of the model quality on representative benchmarks: 75.4 72.3 74.1 80.7 72.7 73.2 83.8 The authors take a closer look at different categories across 80 public benchmark datasets at the table below: Overall, the Phi-3 Medium-4K-Instruct with only 7B-param achieves a similar level of language understanding and math as much larger models. Moreover, the model outperforms bigger models in reasoning capability and only behind GPT-4-Turbo. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, the authors believe such weakness can be resolved by augmenting Phi-3-Medium with a search engine. Usage ===== Input formats ------------- Given the nature of the training data, the Phi-3 Medium-4K-Instruct model is best suited for prompts using the chat format as follows: <|user|>How to explain Internet for a medieval knight?<|end|><|assistant|> Loading the model locally ------------------------- After obtaining the Phi-3 Medium-4K-Instruct model checkpoints, users can use this sample code for inference. Cross Platform Support ====================== ONNX runtimenow supports Phi3 small models across platforms and hardware. ONNX runtime (http://aka.ms/phi3-optimizations) Optimized phi-3 models are also published here in ONNX format, to run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the precision best suited to each of these targets. DirectML GPU acceleration is supported for Windows desktops GPUs (AMD, Intel, and NVIDIA). Along with DML, ONNX Runtime provides cross platform support for Phi3 mini across a range of devices CPU, GPU, and mobile. Here are some of the optimized configurations the authors have added: • ONNX models for int4 DML: Quantized to int4 via AWQ ONNX models for int4 DML: Quantized to int4 via AWQ • ONNX model for fp16 CUDA ONNX model for fp16 CUDA • ONNX model for int4 CUDA: Quantized to int4 via RTN ONNX model for int4 CUDA: Quantized to int4 via RTN • ONNX model for int4 CPU and Mobile: Quantized to int4 via RTN ONNX model for int4 CPU and Mobile: Quantized to int4 via RTN Responsible AI Considerations ============================= Like other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: • Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. • Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. • Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. • Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. • Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, the authors strongly recommend users manually verify all API uses. Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, the authors strongly recommend users manually verify all API uses. Developers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: • Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. • High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. • Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). • Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. • Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Appendix A: Benchmark Methodology ================================= The authors include a brief word on methodology here - and in particular, how the authors think about optimizing prompts. In an ideal world, the authors wouldnever change any promptsin the benchmarks to ensure it is always an apples-to-apples comparison when comparing different models. Indeed, this is the default approach, and is the case in the vast majority of models the authors have run to date. never change any prompts There are, however, some exceptions to this. In some cases, the authors see a model that performs worse than expected on a given evaluationdue to a failure to respect the output format. For example: due to a failure to respect the output format • A Claude model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with “Sure, I can help with that. …” which may break the parser. In such cases, the authors have opted to try differentsystem messages(e.g. “You must always respond to a question” or “Get to the point!”). A Claude model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with “Sure, I can help with that. …” which may break the parser. In such cases, the authors have opted to try differentsystem messages(e.g. “You must always respond to a question” or “Get to the point!”). system messages • With LLaMA-1 models, the authors observed that few shots actually hurt model performance. In this case the authors did allow running the benchmarks with 0-shots for all cases. With LLaMA-1 models, the authors observed that few shots actually hurt model performance. In this case the authors did allow running the benchmarks with 0-shots for all cases. • The authors have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, the authors do allow for model-specific mappings for chat to completion prompts. The authors have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, the authors do allow for model-specific mappings for chat to completion prompts. However,the authors do not: the authors do not • Pick different few-shot examples. Few shots will always be the same when comparing different models. Pick different few-shot examples. Few shots will always be the same when comparing different models. • Change prompt format: e.g. if it is an A/B/C/D multiple choice, the authors do not tweak this to 1/2/3/4 multiple choice. Change prompt format: e.g. if it is an A/B/C/D multiple choice, the authors do not tweak this to 1/2/3/4 multiple choice."
  },
  {
    "name": "phi-3-small-128k-instruct",
    "nvidia_url": "https://build.nvidia.com/microsoft/phi-3-small-128k-instruct",
    "scraped_on": "2025-10-15T21:44:30.603952",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "large language models",
      "microsoft",
      "text-to-text"
    ],
    "model_card": "Phi-3 Small-128K-Instruct Model Card Model Summary ============= MIT (https://opensource.org/license/mit) Intended Use ============ Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. Data Overview ============= Training datasets ================= The training data includes a wide variety of sources, totaling 4.8 trillion tokens (including 10% multilingual), and is a combination of 1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; 2) newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); 3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. Benchmark datasets ================== The authors evaluated the model across a breadth of public and internal benchmarks to understand the model capabilities in the most comprehensive way under multiple tasks and conditions. More specifically, • Reasoning:Winogrande: commonsense reasoning around pronoun resolutionPIQA: physical commonsense reasoning around everyday situationsARC-easy, ARC-challenge: grade-school multiple choice science questions at easy and challenge levelCommonsensQA: generic commonsense questionsMedQA: medical questions answeringSocial IQA: social commonsense intelligenceBoolQ: natural questions from contextTruthfulQA: grounded reasoning Reasoning: • Winogrande: commonsense reasoning around pronoun resolution Winogrande: commonsense reasoning around pronoun resolution • PIQA: physical commonsense reasoning around everyday situations PIQA: physical commonsense reasoning around everyday situations • ARC-easy, ARC-challenge: grade-school multiple choice science questions at easy and challenge level ARC-easy, ARC-challenge: grade-school multiple choice science questions at easy and challenge level • CommonsensQA: generic commonsense questions CommonsensQA: generic commonsense questions • MedQA: medical questions answering MedQA: medical questions answering • Social IQA: social commonsense intelligence Social IQA: social commonsense intelligence • BoolQ: natural questions from context BoolQ: natural questions from context • TruthfulQA: grounded reasoning TruthfulQA: grounded reasoning • Language understanding:HellaSwag: commonsense natural language inference around everyday eventsANLI: adversarial natural language inferenceLAMBADA: word prediction given a passage. Language understanding: • HellaSwag: commonsense natural language inference around everyday events HellaSwag: commonsense natural language inference around everyday events • ANLI: adversarial natural language inference ANLI: adversarial natural language inference • LAMBADA: word prediction given a passage. LAMBADA: word prediction given a passage. • World knowledge:Natural Questions: question about Wikipedia knowledgeTriviaQA: trivia question on general topics World knowledge: • Natural Questions: question about Wikipedia knowledge Natural Questions: question about Wikipedia knowledge • TriviaQA: trivia question on general topics TriviaQA: trivia question on general topics • Math:GSM8K: grade-school math word problemsGSM8K Hard: grade-school math word problems with large values and some absurdity.MATH: challenging competition math problems Math: • GSM8K: grade-school math word problems GSM8K: grade-school math word problems • GSM8K Hard: grade-school math word problems with large values and some absurdity. GSM8K Hard: grade-school math word problems with large values and some absurdity. • MATH: challenging competition math problems MATH: challenging competition math problems • Code:HumanEval, MBPP: python coding tasksSpider: SQL query tasks Code: • HumanEval, MBPP: python coding tasks HumanEval, MBPP: python coding tasks • Spider: SQL query tasks Spider: SQL query tasks • Multilingual:MGSM: multilingual grade-school mathMEGA: multilingual NLP tasks Multilingual: • MGSM: multilingual grade-school math MGSM: multilingual grade-school math • MEGA: multilingual NLP tasks MEGA: multilingual NLP tasks • Popular aggregated datasets: MMLU, BigBench-Hard, AGI Eval Popular aggregated datasets: MMLU, BigBench-Hard, AGI Eval • Long context:GovReport, QMSum, SQuALITY, SummScreenFD: long context summarizationQasper: long context question answer Long context: • GovReport, QMSum, SQuALITY, SummScreenFD: long context summarization GovReport, QMSum, SQuALITY, SummScreenFD: long context summarization • Qasper: long context question answer Qasper: long context question answer • Multi-turn conversations:Data generated by In-house adversarial conversation simulation tool Multi-turn conversations: • Data generated by In-house adversarial conversation simulation tool • Single-turn trustworthiness evaluation:DecodingTrust: a collection of trustworthiness benchmark in eight different perspectivesXSTest: exaggerated safety evaluationToxigen: adversarial and hate speech detection Single-turn trustworthiness evaluation: • DecodingTrust: a collection of trustworthiness benchmark in eight different perspectives DecodingTrust: a collection of trustworthiness benchmark in eight different perspectives • XSTest: exaggerated safety evaluation XSTest: exaggerated safety evaluation • Toxigen: adversarial and hate speech detection Toxigen: adversarial and hate speech detection • Red Team:Responses to prompts provided by AI Red Team at Microsoft Red Team: • Responses to prompts provided by AI Red Team at Microsoft Safety ====== Approach ======== Phi-3 family of models has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated datasets. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and a modified version of RLHF (Reinforcement Learning from Human Feedback) by utilizing human-labeled and synthetic datasets, including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories. Safety Evaluation and Red-Teaming ================================= Prior to release, Phi-3 family of models followed a multi-faceted evaluation approach. Quantitative evaluation was conducted with multiple open-source safety benchmarks and in-house tools utilizing adversarial conversation simulation. For qualitative safety evaluation, the authors collaborated with the AI Red Team at Microsoft to assess safety risks posed by Phi-3-small in both average and adversarial user scenarios. The assessment was done in predetermined eight risk categories with automated scoring followed by thorough manual reviews of the model responses. Please refer to the technical report for more details of the safety alignment. ============================================================================== Model Quality ============= To understand the capabilities, the authors compare Phi-3 Small with a set of models over a variety of benchmarks using the internal benchmark platform BabelBench (SeeAppendix Afor benchmark methodology). Appendix A At the high-level overview of the model quality on representative benchmarks: 72.4 59.9 67.7 67.1 72.7 73.2 83.8 The authors take a closer look at different categories across 80 public benchmark datasets at the table below: Overall, the Phi-3 Small-128K-Instruct with only 7B-param achieves a similar level of language understanding and math as much larger models. Moreover, the model outperforms bigger models in reasoning capability and only behind GPT-4-Turbo. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, the authors believe such weakness can be resolved by augmenting Phi-3-Small with a search engine. Long Context ============ Phi-3 Small-128K-Instruct supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document question answer (QA). The authors see with just 7B params the Phi-3 Small outperforms models with the same parameters size, and competitive with model on a much bigger size such as Mixtral 8x7B. 18.9 13.6 19.0 10.3 25.7 27.4 Usage ===== Input formats ------------- Given the nature of the training data, the Phi-3 Small-128K-Instruct model is best suited for prompts using the chat format as follows: <|user|>How to explain Internet for a medieval knight?<|end|><|assistant|> Loading the model locally ------------------------- After obtaining the Phi-3 Small-128K-Instruct model checkpoints, users can use this sample code for inference. Cross Platform Support ====================== ONNX runtimenow supports Phi3 small models across platforms and hardware. ONNX runtime (http://aka.ms/phi3-optimizations) Optimized phi-3 models are also published here in ONNX format. The ONNX models provided run with ONNX Runtime on GPU across server platforms. Support for DML (for Windows GPU), CPU, and mobile variants will be added later. Here are some of the optimized configurations the authors have added: • ONNX model for fp16 CUDA ONNX model for fp16 CUDA • ONNX model for int4 CUDA: Quantized to int4 via RTN ONNX model for int4 CUDA: Quantized to int4 via RTN Responsible AI Considerations ============================= Like other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: • Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. • Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. • Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. • Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. • Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, the authors strongly recommend users manually verify all API uses. Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, the authors strongly recommend users manually verify all API uses. Developers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: • Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. • High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. • Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). • Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. • Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Appendix A: Benchmark Methodology ================================= The authors include a brief word on methodology here - and in particular, how the authors think about optimizing prompts. In an ideal world, the authors wouldnever change any promptsin the benchmarks to ensure it is always an apples-to-apples comparison when comparing different models. Indeed, this is the default approach, and is the case in the vast majority of models the authors have run to date. never change any prompts There are, however, some exceptions to this. In some cases, the authors see a model that performs worse than expected on a given evaluationdue to a failure to respect the output format. For example: due to a failure to respect the output format • A Claude model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with “Sure, I can help with that. …” which may break the parser. In such cases, the authors have opted to try differentsystem messages(e.g. “You must always respond to a question” or “Get to the point!”). A Claude model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with “Sure, I can help with that. …” which may break the parser. In such cases, the authors have opted to try differentsystem messages(e.g. “You must always respond to a question” or “Get to the point!”). system messages • With LLaMA-1 models, the authors observed that few shots actually hurt model performance. In this case the authors did allow running the benchmarks with 0-shots for all cases. With LLaMA-1 models, the authors observed that few shots actually hurt model performance. In this case the authors did allow running the benchmarks with 0-shots for all cases. • The authors have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, the authors do allow for model-specific mappings for chat to completion prompts. The authors have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, the authors do allow for model-specific mappings for chat to completion prompts. However,the authors do not: the authors do not • Pick different few-shot examples. Few shots will always be the same when comparing different models. Pick different few-shot examples. Few shots will always be the same when comparing different models. • Change prompt format: e.g. if it is an A/B/C/D multiple choice, the authors do not tweak this to 1/2/3/4 multiple choice. Change prompt format: e.g. if it is an A/B/C/D multiple choice, the authors do not tweak this to 1/2/3/4 multiple choice."
  },
  {
    "name": "phi-3-small-8k-instruct",
    "nvidia_url": "https://build.nvidia.com/microsoft/phi-3-small-8k-instruct",
    "scraped_on": "2025-10-15T21:44:27.214007",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "large language models",
      "microsoft",
      "text-to-text"
    ],
    "model_card": "Phi-3 Small-8K-Instruct Model Card Model Summary ============= MIT (https://opensource.org/license/mit) Intended Use ============ Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. Data Overview ============= Training datasets ================= The training data includes a wide variety of sources, totaling 4.8 trillion tokens (including 10% multilingual), and is a combination of 1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; 2) newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); 3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. Benchmark datasets ================== The authors evaluated the model across a breadth of public and internal benchmarks to understand the model capabilities in the most comprehensive way under multiple tasks and conditions. More specifically, • Reasoning:Winogrande: commonsense reasoning around pronoun resolutionPIQA: physical commonsense reasoning around everyday situationsARC-easy, ARC-challenge: grade-school multiple choice science questions at easy and challenge levelCommonsensQA: generic commonsense questionsMedQA: medical questions answeringSocial IQA: social commonsense intelligenceBoolQ: natural questions from contextTruthfulQA: grounded reasoning Reasoning: • Winogrande: commonsense reasoning around pronoun resolution Winogrande: commonsense reasoning around pronoun resolution • PIQA: physical commonsense reasoning around everyday situations PIQA: physical commonsense reasoning around everyday situations • ARC-easy, ARC-challenge: grade-school multiple choice science questions at easy and challenge level ARC-easy, ARC-challenge: grade-school multiple choice science questions at easy and challenge level • CommonsensQA: generic commonsense questions CommonsensQA: generic commonsense questions • MedQA: medical questions answering MedQA: medical questions answering • Social IQA: social commonsense intelligence Social IQA: social commonsense intelligence • BoolQ: natural questions from context BoolQ: natural questions from context • TruthfulQA: grounded reasoning TruthfulQA: grounded reasoning • Language understanding:HellaSwag: commonsense natural language inference around everyday eventsANLI: adversarial natural language inferenceLAMBADA: word prediction given a passage. Language understanding: • HellaSwag: commonsense natural language inference around everyday events HellaSwag: commonsense natural language inference around everyday events • ANLI: adversarial natural language inference ANLI: adversarial natural language inference • LAMBADA: word prediction given a passage. LAMBADA: word prediction given a passage. • World knowledge:Natural Questions: question about Wikipedia knowledgeTriviaQA: trivia question on general topics World knowledge: • Natural Questions: question about Wikipedia knowledge Natural Questions: question about Wikipedia knowledge • TriviaQA: trivia question on general topics TriviaQA: trivia question on general topics • Math:GSM8K: grade-school math word problemsGSM8K Hard: grade-school math word problems with large values and some absurdity.MATH: challenging competition math problems Math: • GSM8K: grade-school math word problems GSM8K: grade-school math word problems • GSM8K Hard: grade-school math word problems with large values and some absurdity. GSM8K Hard: grade-school math word problems with large values and some absurdity. • MATH: challenging competition math problems MATH: challenging competition math problems • Code:HumanEval, MBPP: python coding tasksSpider: SQL query tasks Code: • HumanEval, MBPP: python coding tasks HumanEval, MBPP: python coding tasks • Spider: SQL query tasks Spider: SQL query tasks • Multilingual:MGSM: multilingual grade-school mathMEGA: multilingual NLP tasks Multilingual: • MGSM: multilingual grade-school math MGSM: multilingual grade-school math • MEGA: multilingual NLP tasks MEGA: multilingual NLP tasks • Popular aggregated datasets: MMLU, BigBench-Hard, AGI Eval Popular aggregated datasets: MMLU, BigBench-Hard, AGI Eval • Multi-turn conversations:Data generated by In-house adversarial conversation simulation tool Multi-turn conversations: • Data generated by In-house adversarial conversation simulation tool • Single-turn trustworthiness evaluation:DecodingTrust: a collection of trustworthiness benchmark in eight different perspectivesXSTest: exaggerated safety evaluationToxigen: adversarial and hate speech detection Single-turn trustworthiness evaluation: • DecodingTrust: a collection of trustworthiness benchmark in eight different perspectives DecodingTrust: a collection of trustworthiness benchmark in eight different perspectives • XSTest: exaggerated safety evaluation XSTest: exaggerated safety evaluation • Toxigen: adversarial and hate speech detection Toxigen: adversarial and hate speech detection • Red Team:Responses to prompts provided by AI Red Team at Microsoft Red Team: • Responses to prompts provided by AI Red Team at Microsoft Safety ====== Approach ======== Phi-3 family of models has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated datasets. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and a modified version of RLHF (Reinforcement Learning from Human Feedback) by utilizing human-labeled and synthetic datasets, including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories. Safety Evaluation and Red-Teaming ================================= Prior to release, Phi-3 family of models followed a multi-faceted evaluation approach. Quantitative evaluation was conducted with multiple open-source safety benchmarks and in-house tools utilizing adversarial conversation simulation. For qualitative safety evaluation, the authors collaborated with the AI Red Team at Microsoft to assess safety risks posed by Phi-3-small in both average and adversarial user scenarios. The assessment was done in predetermined eight risk categories with automated scoring followed by thorough manual reviews of the model responses. Please refer to the technical report for more details of the safety alignment. ============================================================================== Model Quality ============= To understand the capabilities, the authors compare Phi-3 Small with a set of models over a variety of benchmarks using the internal benchmark platform BabelBench (SeeAppendix Afor benchmark methodology). Appendix A At the high-level overview of the model quality on representative benchmarks: 73.7 59.9 67.7 67.1 72.7 73.2 83.8 The authors take a closer look at different categories across 80 public benchmark datasets at the table below: Overall, the Phi-3 Small-8K-Instruct with only 7B-param achieves a similar level of language understanding and math as much larger models. Moreover, the model outperforms bigger models in reasoning capability and only behind GPT-4-Turbo. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, the authors believe such weakness can be resolved by augmenting Phi-3-Small with retrieval augmented generation. Usage ===== Input formats ------------- Given the nature of the training data, the Phi-3-Small-8K-Instruct model is best suited for prompts using the chat format as follows: <|user|>How to explain Internet for a medieval knight?<|end|><|assistant|> Loading the model locally ------------------------- The model requires tiktoken andTritonpackages. After obtaining the Phi-3-Small-8K-Instruct model checkpoints, users can use this sample code for inference. Triton (https://developer.nvidia.com/triton-inference-server) Cross Platform Support ====================== ONNX runtimenow supports Phi3 small models across platforms and hardware. ONNX runtime (http://aka.ms/phi3-optimizations) Optimized phi-3 models are also published here in ONNX format. The ONNX models provided run with ONNX Runtime on GPU across server platforms. Support for DML (for Windows GPU), CPU, and mobile variants will be added later. Here are some of the optimized configurations the authors have added: • ONNX model for fp16 CUDA ONNX model for fp16 CUDA • ONNX model for int4 CUDA: Quantized to int4 via RTN ONNX model for int4 CUDA: Quantized to int4 via RTN Responsible AI Considerations ============================= Like other language models, the Phi-3 family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: • Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. • Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. • Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. • Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. • Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, the authors strongly recommend users manually verify all API uses. Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, the authors strongly recommend users manually verify all API uses. Developers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: • Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. • High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. • Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). • Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. • Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Appendix A: Benchmark Methodology ================================= The authors include a brief word on methodology here - and in particular, how the authors think about optimizing prompts. In an ideal world, the authors wouldnever change any promptsin the benchmarks to ensure it is always an apples-to-apples comparison when comparing different models. Indeed, this is the default approach, and is the case in the vast majority of models the authors have run to date. never change any prompts There are, however, some exceptions to this. In some cases, the authors see a model that performs worse than expected on a given evaluationdue to a failure to respect the output format. For example: due to a failure to respect the output format • A Claude model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with “Sure, I can help with that. …” which may break the parser. In such cases, the authors have opted to try differentsystem messages(e.g. “You must always respond to a question” or “Get to the point!”). A Claude model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with “Sure, I can help with that. …” which may break the parser. In such cases, the authors have opted to try differentsystem messages(e.g. “You must always respond to a question” or “Get to the point!”). system messages • With LLaMA-1 models, the authors observed that few shots actually hurt model performance. In this case the authors did allow running the benchmarks with 0-shots for all cases. With LLaMA-1 models, the authors observed that few shots actually hurt model performance. In this case the authors did allow running the benchmarks with 0-shots for all cases. • The authors have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, the authors do allow for model-specific mappings for chat to completion prompts. The authors have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, the authors do allow for model-specific mappings for chat to completion prompts. However,the authors do not: the authors do not • Pick different few-shot examples. Few shots will always be the same when comparing different models. Pick different few-shot examples. Few shots will always be the same when comparing different models. • Change prompt format: e.g. if it is an A/B/C/D multiple choice, the authors do not tweak this to 1/2/3/4 multiple choice. Change prompt format: e.g. if it is an A/B/C/D multiple choice, the authors do not tweak this to 1/2/3/4 multiple choice."
  },
  {
    "name": "rfdiffusion",
    "nvidia_url": "https://build.nvidia.com/ipd/rfdiffusion",
    "scraped_on": "2025-10-15T21:44:24.087590",
    "tags": [
      "biology",
      "bionemo",
      "drug discovery",
      "ipd",
      "nim",
      "protein generation"
    ],
    "model_card": "Model Overview ============== Description: ============ RFdiffusion (RoseTTAFold Diffusion) is a generative model that creates novel protein structures for protein scaffolding and protein binder design tasks. This model generates entirely new protein backbones and designs proteins that can be specifically tailored to bind to target molecules. This model is available for commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAGitHub Model Card. GitHub Model Card (https://github.com/RosettaCommons/RFdiffusion) License/Terms of Use: --------------------- This model is released under theBSD License. BSD License (https://github.com/RosettaCommons/RFdiffusion/blob/820bfdfaded8c260b962dc40a3171eae316b6ce0/LICENSE) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. References: =========== Model Architecture: =================== Architecture Type:Diffusion-based Generative Neural NetworkNetwork Architecture:RFdiffusion Architecture Type: Network Architecture: Input: ====== Input Type(s):Text (Protein)Input Format(s):Protein Data Bank (PDB)Input Parameters:String, One-Dimensional (1D) Input Type(s): Input Format(s): Input Parameters: Output: ======= Output Type(s):Text (Protein)Output Format:Protein Data Bank (PDB)Output Parameters:String, 1D Output Type(s): Output Format: Output Parameters: Software Integration: ===================== Runtime Engine(s): • PyTorch Supported Hardware Microarchitecture Compatibility: • NVIDIA Hopper • NVIDIA Ampere • NVIDIA Lovelace • NVIDIA Turing [Preferred/Supported] Operating System(s): • Linux Model Version(s): ================= RFdiffusion 2.0.0 Training & Evaluation: ====================== Training Dataset: ================= Link:The Protein Data Bank Link: The Protein Data Bank (https://www.rcsb.org/) ** Data Collection Method by dataset • Hybrid: Automatic, Human For PDB dataset, scientists worldwide submit structural data determined by X-ray crystallography or cryo-electron microscopy (cryo-EM). This includes atomic coordinates, experimental data, and metadata about the biological macromolecules. ** Labeling Method by dataset • Hybrid: Automatic, Human For PDB dataset, expert biocurators review the submitted data to ensure accuracy and completeness. This involves checking the plausibility of the data and annotating it with relevant biological and chemical information. Properties (Quantity, Dataset Descriptions, Sensor(s)):The training dataset used for RFdiffusion, as detailed in referenced paper, consists of protein structures sampled from the Protein Data Bank (PDB). To prepare these structures for training, a noising process is applied. This process involves simulating up to 200 steps of random modifications on the protein structures. Specifically, the modifications include perturbing the Cα coordinates with 3D Gaussian noise and applying Brownian motion to the residue orientations on the manifold of rotation matrices. Properties (Quantity, Dataset Descriptions, Sensor(s)): Dataset License(s):CC0 1.0. Dataset License(s): CC0 1.0 (https://www.rcsb.org/news/feature/611e8d97ef055f03d1f222c6) Evaluation Dataset: =================== The evaluation strategy involved training the model on PDB structures (as described in Training Dataset) with added noise and then assessing its ability to denoise these structures, as well as evaluating its performance on design tasks with auxiliary conditioning information. ** Data Collection Method by dataset • Automatic: random splits from PDB dataset. ** Labeling Method by dataset • Automatic: random splits from PDB dataset. The training, validation, and test splits were derived from protein assemblies in the PDB, which includes structures determined by X-ray crystallography or cryo-electron microscopy (cryoEM). Inference: ========== Engine:PyTorchTest Hardware: Engine: Test Hardware: • A100 • L40 • H100 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "retail-object-detection",
    "nvidia_url": "https://build.nvidia.com/nvidia/retail-object-detection",
    "scraped_on": "2025-10-15T21:44:20.827679",
    "tags": [
      "computer vision",
      "cv",
      "image",
      "nvidia",
      "nvidia nim",
      "object detection",
      "tao toolkit",
      "video",
      "vlm"
    ],
    "model_card": "Retail Object Detection ======================= Description: ============ RetailObjectDetection detects retail items within an image; it classifies objects as retail or not. This model is ready for commercial use. References: =========== Citations --------- • Tobin, Josh, et al. \"Domain randomization for transferring deep neural networks from simulation to the real world.\" 2017IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2017. IEEE/RSJ international conference on intelligent robots and systems (IROS) • Morrical, Nathan, et al. \"NViSII: A scriptable tool for photorealistic image generation.\"arXiv preprint arXiv:2105.13962 (2021). arXiv preprint arXiv:2105.13962 (2021) Using TAO Pre-trained Models ---------------------------- • GetTAO Container TAO Container (https://ngc.nvidia.com/catalog/containers/nvidia:tao:tao-toolkit) • Get other purpose-built models from the NGC model registry:TrafficCamNetPeopleNetPeopleNet-TransformerDashCamNetFaceDetectIRVehicleMakeNetVehicleTypeNetPeopleSegNetPeopleSemSegNetLicense Plate DetectionLicense Plate RecognitionPoseClassificationNetFacial LandmarkFaceDetect2D Body Pose EstimationActionRecognitionNetPeople ReIdentificationPointPillarNetCitySegFormerRetail Object DetectionRetail Object EmbeddingOptical InspectionOptical Character DetectionOptical Character RecognitionPCB ClassificationPeopleSemSegFormer • TrafficCamNet TrafficCamNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:trafficcamnet) • PeopleNet PeopleNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplenet) • PeopleNet-Transformer PeopleNet-Transformer (https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplenet_transformer) • DashCamNet DashCamNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:dashcamnet) • FaceDetectIR FaceDetectIR (https://ngc.nvidia.com/catalog/models/nvidia:tao:facedetectir) • VehicleMakeNet VehicleMakeNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:vehiclemakenet) • VehicleTypeNet VehicleTypeNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:vehicletypenet) • PeopleSegNet PeopleSegNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplesegnet) • PeopleSemSegNet PeopleSemSegNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplesemsegnet) • License Plate Detection License Plate Detection (https://ngc.nvidia.com/catalog/models/nvidia:tao:lpdnet) • License Plate Recognition License Plate Recognition (https://ngc.nvidia.com/catalog/models/nvidia:tao:lprnet) • PoseClassificationNet PoseClassificationNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:poseclassificationnet) • Facial Landmark Facial Landmark (https://ngc.nvidia.com/catalog/models/nvidia:tao:fpenet) • FaceDetect FaceDetect (https://ngc.nvidia.com/catalog/models/nvidia:tao:facenet) • 2D Body Pose Estimation 2D Body Pose Estimation (https://ngc.nvidia.com/catalog/models/nvidia:tao:bodyposenet) • ActionRecognitionNet ActionRecognitionNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:actionrecognitionnet) • People ReIdentification People ReIdentification (https://ngc.nvidia.com/catalog/models/nvidia:tao:reidentificationnet) • PointPillarNet PointPillarNet (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pointpillarnet) • CitySegFormer CitySegFormer (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/citysemsegformer) • Retail Object Detection Retail Object Detection (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/retail_object_detection) • Retail Object Embedding Retail Object Embedding (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/retail_object_recognition) • Optical Inspection Optical Inspection (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/optical_inspection) • Optical Character Detection Optical Character Detection (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/ocdnet) • Optical Character Recognition Optical Character Recognition (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/ocrnet) • PCB Classification PCB Classification (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pcb_classification) • PeopleSemSegFormer PeopleSemSegFormer (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplesemsegformer) Model Architecture: =================== Architecture Type:Convolution Neural Network (CNN)Network Architecture:EfficientDet, DINO-FAN_base Architecture Type: Network Architecture: Input: ====== Input Type(s):ImageInput Format(s):Red, Green, Blue (RGB)Input Parameters:2DOther Properties Related to Input:RGB Fixed Resolution: 416x416 and 960x544 (W x H); No minimum bit depth, alpha, or gamma. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: The logos on retail items were smudged. Output: ======= Output Type(s):Label(s)Output Format:Label: Text StringOther Properties Related to Output:Category Label(s): returns a single category. Output Type(s): Output Format: Other Properties Related to Output: The logos on retail items were smudged. Software Integration: ===================== Runtime Engine(s): • TAO - 5.2 • DeepStream 6.1 or later Supported Hardware Architecture(s): • Ampere • Jetson • Hopper • Lovelace • Pascal • Turing • Volta Supported Operating System(s): • Linux • Linux 4 Tegra Model Version(s): ================= • trainable_binary_v1.0 • deployable_binary_v1.0 Training & Evaluation: ====================== Training Dataset: ================= Data Collection Method by dataset: • Automatic/Sensors Labeling Method by dataset: • Human Properties:320,000 proprietary synthetic images of objects found in retail settings randomizing several simulation domains including: Properties: • light types, light intensities • object sizes, orientations, and locations • camera locations • background textures • flying distractors The background textures are real images sampled from: • Proprietary real images • images taken from a retail checkout counter • HDRI texture maps create by NVIDIA Omniverse Each synthetic image contains 1 target retail item. This dataset is set up to simulate the diverse environments in the real world and to have the detector learn to extract retail items from noisy backgrounds. The logos on retail items were smudged. Fine-tuning Data ---------------- This model is fine-tuned on about 600 real proprietary images from 6 different real environments. In each environment, only 1 image per item is collected. The fine tuning data are captured under random camera heights and field of views. All fine tuning data were collected indoor, having retail items placed at the checkout counter, shelf, baskets, and conveyor belt. The camera is typically set up at approximately 10 feet height, 45-degree angle off the vertical axis and has close field-of-view. This content was chosen to decrease the simulation-to-reality gap of the model trained on synthetic data, and to improve the accuracy and the robustness of the model. The logos on retail items were smudged. Fine-tuning Data Ground-truth Labeling Guidelines ------------------------------------------------- The fine tuning data are created by labeling ground-truth bounding-boxes and categories by human-labelers. The following guidelines were used while labeling the training data for NVIDIA Retail Detection models. If you are looking to transfer-learn or to fine-tune the models to adapt to your target environment and classes, please follow the guidelines below for better model accuracy. • All objects that fall under the definition of retail items and are larger than the smallest bounding-box limit for the corresponding class (height >= 10px OR width >= 10px) are labeled with the appropriate class label. • Occlusion: For partially occluded objects that are visible approximately 60% or are marked as visible objects with a bounding box around the visible part of the object. These objects are marked as partially occluded. Objects under 60% visibility are not annotated. • Truncation: An object, at the edge of the frame, which is 60% or more visible is marked with the truncation flag. • Each frame is not required to have an object. Evaluation Dataset: =================== Data Collection Method by dataset: • Automatic/Sensors Labeling Method by dataset: • Human Properties:15,000 proprietary real-world images of objects found in retail settings. Properties: Methodology and KPI ------------------- AP50 is calculated using intersection-over-union (IOU) criterion greater than 0.5. The KPI for the evaluation data are reported in the table below. Model is evaluated based on AP50 and AR0.5:0.95. Both AR and AP numbers are based on 100 maximum detections each image. Please note that “unseen items” measurements are irrelevant to the 100-class detection model. Binary-class Retail Item Detection Model ---------------------------------------- overall (mean of all scenes) 0.959 0.859 0.938 0.818 Inference: ========== Engine:Tensor(RT)Test Hardware: Engine: Test Hardware: • Jetson AGX Xavier • Xavier NX • Orin • Orin NX • NVIDIA T4 • Ampere GPU • A2 • A30 • L4 • T4 • DGX H100 • DGX A100 • DGX H100 • L40 • JAO 64GB • Orin NX16GB • Orin Nano 8GB The inference is run on the provided unpruned model at FP16 precision. The model input resolution is 416x416. The inference performance is run usingtrtexecon Jetson AGX Orin 64GB and A10. The performance shown here is the inference only performance. The end-to-end performance with streaming video data might slightly vary depending on other bottlenecks in the hardware and software. trtexec (https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#trtexec-ovr) How to use this model ===================== Instructions to use unpruned model with TAO ------------------------------------------- In order to use these models as pretrained weights for transfer learning, please use the snippet below as template for the model component of the experiment spec file to train a Efficientdet-TF2 model. For more information on the experiment spec file, please refer to the [RetailDetector notebook and the EfficientDdet-TFtf2 TAO doc]. Instructions to deploy these models with DeepStream --------------------------------------------------- Here is an example of using the Retail Item Embedder together with the Retail Item Detector [TODO: add Retail Item Embedder url here] for an end-to-end video analytic application. To do so, deploy these models withDeepStream SDK. DeepStream SDK is a streaming analytic toolkit to accelerate building AI-based video analytic applications. DeepStream supports direct integration of these models into the deepstream sample app. DeepStream SDK (https://developer.nvidia.com/deepstream-sdk) To deploy these models withDeepStream 6.2, please follow the instructions below: DeepStream 6.2 (https://deepstreamsdk.gitlab-master-pages.nvidia.com/docs-template/text/DS_ref_app_deepstream.html?highlight=attach) Downloadand install DeepStream SDK. The installation instructions for DeepStream are provided inDeepStream development guide. The config files for the purpose-built models are located in: Download (https://developer.nvidia.com/deepstream-sdk) DeepStream development guide (https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html) /opt/nvidia/deepstream is the default DeepStream installation directory. This path will be different if you are installing in a different directory. The sample config files are provided in NVIDIA-AI-IOT(TODO: Update the URL when deepstream_tao_apps are merged with???). Assume the repo is cloned under$DS_TAO_APPS_HOME, in$DS_TAO_APPS_HOME/configs/retailDetector_tao, Key Parameters inpgie_retailDetector_100_tao_config.yml In order to decode the bounding box information from the EfficientDet output tensor, the custom parser function and library have to be specified. To inference the model, please run: The \"Deploying to DeepStream\" chapter ofTAO User Guideprovides more details. TAO User Guide (https://docs.nvidia.com/tao/tao-toolkit/index.html) Technical blogs --------------- • Access the latest in Vision AI development workflows with NVIDIA TAO Toolkit 5.0 Access the latest in Vision AI development workflows with NVIDIA TAO Toolkit 5.0 (https://developer.nvidia.com/blog/access-the-latest-in-vision-ai-model-development-workflows-with-nvidia-tao-toolkit-5-0/) • Improve accuracy and robustness of vision ai models with vision transformers and NVIDIA TAO Improve accuracy and robustness of vision ai models with vision transformers and NVIDIA TAO (https://developer.nvidia.com/blog/improve-accuracy-and-robustness-of-vision-ai-apps-with-vision-transformers-and-nvidia-tao/) • Train like a ‘pro’ without being an AI expert using TAO AutoML Train like a ‘pro’ without being an AI expert using TAO AutoML (https://developer.nvidia.com/blog/training-like-an-ai-pro-using-tao-automl/) • Create Custom AI models using NVIDIA TAO Toolkit with Azure Machine Learning Create Custom AI models using NVIDIA TAO Toolkit with Azure Machine Learning (https://developer.nvidia.com/blog/creating-custom-ai-models-using-nvidia-tao-toolkit-with-azure-machine-learning/) • Developing and Deploying AI-powered Robots with NVIDIA Isaac Sim and NVIDIA TAO Developing and Deploying AI-powered Robots with NVIDIA Isaac Sim and NVIDIA TAO (https://developer.nvidia.com/blog/developing-and-deploying-ai-powered-robots-with-nvidia-isaac-sim-and-nvidia-tao/) • Learn endless ways to adapt and supercharge your AI workflows with TAO -Whitepaper Whitepaper (https://developer.nvidia.com/tao-toolkit-usecases-whitepaper/1-introduction) • Customize Action Recognition with TAO and deploy with DeepStream Customize Action Recognition with TAO and deploy with DeepStream (https://developer.nvidia.com/blog/developing-and-deploying-your-custom-action-recognition-application-without-any-ai-expertise-using-tao-and-deepstream/) • Read the 2 part blog on training and optimizing 2D body pose estimation model with TAO -Part 1|Part 2 Part 1 (https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-1) Part 2 (https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-2) • Learn how to trainreal-time License plate detection and recognition appwith TAO and DeepStream. real-time License plate detection and recognition app (https://developer.nvidia.com/blog/creating-a-real-time-license-plate-detection-and-recognition-app) • Model accuracy is extremely important, learn how you can achievestate of the art accuracy for classification and object detection modelsusing TAO state of the art accuracy for classification and object detection models (https://developer.nvidia.com/blog/preparing-state-of-the-art-models-for-classification-and-object-detection-with-tao-toolkit/) Suggested reading ----------------- • More information on about TAO Toolkit and pre-trained models can be found at theNVIDIA Developer Zone NVIDIA Developer Zone (https://developer.nvidia.com/tao-toolkit) • TAO documentation TAO documentation (https://docs.nvidia.com/tao/tao-toolkit/index.html) • Read theTAO getting Startedguide andrelease notes. TAO getting Started (https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_quick_start_guide.html) release notes (https://docs.nvidia.com/tao/tao-toolkit/text/release_notes.html) • If you have any questions or feedback, please refer to the discussions onTAO Toolkit Developer Forums TAO Toolkit Developer Forums (https://forums.developer.nvidia.com/c/accelerated-computing/intelligent-video-analytics/tao-toolkit/17) • Deploy your models for video analytics application using DeepStream. Learn more aboutDeepStream SDK DeepStream SDK (https://developer.nvidia.com/deepstream-sdk) • Deploy your models inRivafor ConvAI use case. Riva (https://developer.nvidia.com/riva) Ethical Considerations: ======================= NVIDIA Retail Object Detection model detects retail items. However, no additional information such as people and other distractors in the background are inferred. Training and evaluation dataset mostly consists of North American content. An ideal training and evaluation dataset would additionally include content from other geographies. NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Promise and the Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere here (https://www.nvidia.com/en-us/support/submit-security-vulnerability)"
  },
  {
    "name": "visual-changenet",
    "nvidia_url": "https://build.nvidia.com/nvidia/visual-changenet",
    "scraped_on": "2025-10-15T21:44:17.514534",
    "tags": [
      "computer vision",
      "cv",
      "image",
      "image generation",
      "image segmentation",
      "nvidia",
      "nvidia nim",
      "tao toolkit",
      "video",
      "vlm"
    ],
    "model_card": "Visual ChangeNet Segmentation Model Card (Commercial) ===================================================== Model Overview ============== The Visual ChangeNet-Segmentation Model detects changes in land cover using remote sensing imagery (RSI). This model is ready for commercial use. References: =========== Using TAO Pre-trained Models ---------------------------- • GetTAO Container TAO Container (https://ngc.nvidia.com/catalog/containers/nvidia:tao:tao-toolkit) • Get other purpose-built models from the NGC model registry:TrafficCamNetPeopleNetPeopleNet-TransformerDashCamNetFaceDetectIRVehicleMakeNetVehicleTypeNetPeopleSegNetPeopleSemSegNetLicense Plate DetectionLicense Plate RecognitionPoseClassificationNetFacial LandmarkFaceDetect2D Body Pose EstimationActionRecognitionNetPeople ReIdentificationPointPillarNetCitySegFormerRetail Object DetectionRetail Object EmbeddingOptical InspectionOptical Character DetectionOptical Character RecognitionPCB ClassificationPeopleSemSegFormer • TrafficCamNet TrafficCamNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:trafficcamnet) • PeopleNet PeopleNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplenet) • PeopleNet-Transformer PeopleNet-Transformer (https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplenet_transformer) • DashCamNet DashCamNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:dashcamnet) • FaceDetectIR FaceDetectIR (https://ngc.nvidia.com/catalog/models/nvidia:tao:facedetectir) • VehicleMakeNet VehicleMakeNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:vehiclemakenet) • VehicleTypeNet VehicleTypeNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:vehicletypenet) • PeopleSegNet PeopleSegNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplesegnet) • PeopleSemSegNet PeopleSemSegNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplesemsegnet) • License Plate Detection License Plate Detection (https://ngc.nvidia.com/catalog/models/nvidia:tao:lpdnet) • License Plate Recognition License Plate Recognition (https://ngc.nvidia.com/catalog/models/nvidia:tao:lprnet) • PoseClassificationNet PoseClassificationNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:poseclassificationnet) • Facial Landmark Facial Landmark (https://ngc.nvidia.com/catalog/models/nvidia:tao:fpenet) • FaceDetect FaceDetect (https://ngc.nvidia.com/catalog/models/nvidia:tao:facenet) • 2D Body Pose Estimation 2D Body Pose Estimation (https://ngc.nvidia.com/catalog/models/nvidia:tao:bodyposenet) • ActionRecognitionNet ActionRecognitionNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:actionrecognitionnet) • People ReIdentification People ReIdentification (https://ngc.nvidia.com/catalog/models/nvidia:tao:reidentificationnet) • PointPillarNet PointPillarNet (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pointpillarnet) • CitySegFormer CitySegFormer (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/citysemsegformer) • Retail Object Detection Retail Object Detection (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/retail_object_detection) • Retail Object Embedding Retail Object Embedding (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/retail_object_recognition) • Optical Inspection Optical Inspection (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/optical_inspection) • Optical Character Detection Optical Character Detection (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/ocdnet) • Optical Character Recognition Optical Character Recognition (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/ocrnet) • PCB Classification PCB Classification (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pcb_classification) • PeopleSemSegFormer PeopleSemSegFormer (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplesemsegformer) Model Architecture: =================== Architecture Type:Transformer-BasedNetwork Architecture:Siamese Network Architecture Type: Network Architecture: Visual ChangeNet is a state of the art transformer-based Change Detection model. Visual ChangeNet is based on Siamese Network, which is a class of neural network architectures containing two or more identical subnetworks. The training algorithm works by updating the parameters across all the sub-networks in tandem. In TAO, Visual ChangeNet supports two images as input where the end goal is to either classify or segment the change between the \"golden or reference\" image and the \"test\" image. More specifically, this model was trained with theNVDINOv2 backbone, which was trained using a self-supervised manner on NVIDIA proprietary data and achieved SOTA accuracy on zero-shot ImageNet classification. To enable the ViT backbone into Visual ChangeNet, theViT-Adapterwas used as the neck architecture. The ViT-Adapter improves the accuracy on dense predictions, such as object detection and segmentation. In TAO, two different types of Change Detection networks are supported: NVDINOv2 backbone (https://arxiv.org/abs/2304.07193) ViT-Adapter (https://arxiv.org/abs/2205.08534) • Visual ChangeNet-Segmentation - for segmentation of change between two input images. • Visual ChangeNet-Classification - for classification of change between two input images. Visual ChangeNet-Segmentation is specifically intended for change segmentation. In this model card, the Visual ChangeNet-Segmentation model is leveraged to demonstrate land cover semantic change detection using the LandSat-SCD dataset. The model uses a pretrained NVDINOv2 backbone, trained on NVIDIA-commercial dataset, and then fine-tuned on the LandSat-SCD dataset. Input: ====== Input Type(s):ImagesInput Format(s):Red, Green, Blue (RGB)Input Parameters:Three-Dimensional (3D)Other Properties Related to Input:Two input images: Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: • Golden: RGB Image of dimensions: 416 X 416 X 3 (H x W x C) • Sample: RGB Image of dimensions: 416 X 416 X 3 (H x W x C) Channel Ordering of the Input: NCHW, where N = Batch Size, C = number of channels (3), H = Height of images (416), W = Width of the images (416) Here is a sample image for a pre and post change images along with ground-truth segmentation change map side-by-side. Output: ======= Output Type(s):Segmentation Change MapOutput Format:3D VectorOther Properties Related to Output:Segmentation change map with the same resolution as the input images: 416 X 416 X 10 (H x W x C), where C = number of output change classes. Output Type(s): Output Format: Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s): • TAO - 5.2 Supported Hardware Architecture(s): • Ampere • Jetson • Hopper • Lovelace • Pascal • Turing • Volta Supported Operating System(s): • Linux • Linux 4 Tegra Model Version(s): ================= • trainable_v1.0- NVDINOv2 Visual ChangeNet-Segmentation model LandSat-SCD is trainable. trainable_v1.0 • deployable_v1.0- NVDINOv2 Visual ChangeNet-Segmentation model LandSat-SCD is deployable to DeepStream. deployable_v1.0 • deployable_v1.1- NVDINOv2 Visual ChangeNet-Segmentation model LandSat-SCD is deployable to DeepStream. deployable_v1.1 • deployable_v1.2- NVDINOv2 Visual ChangeNet-Segmentation model LandSat-SCD is deployable to DeepStream. deployable_v1.2 Training & Evaluation: ====================== Training Dataset: ================= Data Collection Method by dataset: • Automatic/Sensors Labeling Method by dataset: • Human Properties:Trained on the open-source remote sensing semantic land change detection dataset consisting of 8468 images containing remote sensing (RS) image pairs of resolution 416 × 416. They are randomly split into three parts to make train, val, and test sets of samples 6053, 1729, and 686 respectively. Properties: Evaluation Dataset: =================== Data Collection Method by dataset: • Automatic/Sensors Labeling Method by dataset: • Human Properties:Evaluated on the open-source remote sensing semantic land change detection dataset of 686 images. Properties: Methodology and KPI ------------------- The performance of the Visual ChangeNet-Segmentation model for multi-class semantic change detection is measured using overall accuracy, average precision, average recall and avergae IoU score for all the classes. Inference: ========== Engine:Tensor(RT)Test Hardware: Engine: Test Hardware: • Jetson AGX Xavier • Xavier NX • Orin • Orin NX • NVIDIA T4 • Ampere GPU • A2 • A30 • L4 • T4 • DGX H100 • DGX A100 • DGX H100 • L40 • JAO 64GB • Orin NX16GB • Orin Nano 8GB The inference is run on the provided unpruned model at FP16 precision. The inference performance is run usingtrtexecon Jetson AGX Xavier, Xavier NX, Orin, Orin NX and NVIDIA T4, and Ampere GPUs. The Jetson devices are running at Max-N configuration for maximum GPU frequency. The performance shown here is the inference only performance. The end-to-end performance with streaming video data might vary depending on other bottlenecks in the hardware and software. trtexec (https://github.com/NVIDIA/TensorRT/tree/main/samples/trtexec) NVDINOv2 + ViT-Adapter + Visual ChangeNet Using this Model ================ These models need to be used with NVIDIA hardware and software. For hardware, the models can run on any NVIDIA GPU including NVIDIA Jetson devices. These models can only be used with theTrain Adapt Optimize (TAO) Toolkit, orTensorRT. Train Adapt Optimize (TAO) Toolkit (https://developer.nvidia.com/tao-toolkit) TensorRT (https://developer.nvidia.com/tensorrt) The primary use case for these models is for Visual ChangeNet-Segmentation using RGB images. The model is a Siamese Network that outputs semantic change maps denoting pixel-level change between the two images. These models are intended for training and fine-tuning using the TAO Toolkit and your datasets for image comparison. High-fidelity models can be trained on new use cases. A Jupyter Notebook is available as a part of theTAO containerand can be used to re-training. TAO container (https://ngc.nvidia.com/catalog/containers/nvidia:tao:tao-toolkit) The models are also intended for edge deployment using TensorRT. Using the Model with TAO ------------------------ To use these models as pretrained weights for transfer learning, use the following as a template for themodelandtraincomponent of the experiment spec file to train a Siamese Network model. For more information on the experiment spec file, see theTAO Toolkit User Guide - Visual ChangeNet-Segmentation. TAO Toolkit User Guide - Visual ChangeNet-Segmentation (https://docs.nvidia.com/tao/tao-toolkit/text/visual_changenet/visual_changenet_segment.html) Technical Blogs =============== • Learn how to transformIndustrial Defect Detection with NVIDIA TAO and Vision AI Models Industrial Defect Detection with NVIDIA TAO and Vision AI Models (https://developer.nvidia.com/blog/transforming-industrial-defect-detection-with-nvidia-tao-and-vision-ai-models/) • Read the 2 part blog on training and optimizing 2D body pose estimation model with TAO -Part 1|Part 2 Part 1 (https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-1) Part 2 (https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-2) • Learn how to trainreal-time license plate detection and recognition appwith TAO and DeepStream. real-time license plate detection and recognition app (https://developer.nvidia.com/blog/creating-a-real-time-license-plate-detection-and-recognition-app) • Model accuracy is extremely important, learn how you can achievestate of the art accuracy for classification and object detection modelsusing TAO state of the art accuracy for classification and object detection models (https://developer.nvidia.com/blog/preparing-state-of-the-art-models-for-classification-and-object-detection-with-tao-toolkit/) • Learn how to train theInstance segmentation model using MaskRCNN with TAO Instance segmentation model using MaskRCNN with TAO (https://developer.nvidia.com/blog/training-instance-segmentation-models-using-maskrcnn-on-tao-toolkit/) • Read the technical tutorial on howPeopleNet model can be trained with custom data using Transfer Learning Toolkit PeopleNet model can be trained with custom data using Transfer Learning Toolkit (https://devblogs.nvidia.com/training-custom-pretrained-models-using-tlt/) • Learn how totrain and deploy real-time intelligent video analytics apps and services using DeepStream SDK train and deploy real-time intelligent video analytics apps and services using DeepStream SDK (https://devblogs.nvidia.com/building-iva-apps-using-deepstream-5.0/) Suggested Reading ================= • More information on about TAO Toolkit and pre-trained models can be found at theNVIDIA Developer Zone NVIDIA Developer Zone (https://developer.nvidia.com/tao-toolkit) • Read theTAO Quick Startguide andrelease notes. TAO Quick Start (https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_quick_start_guide.html) release notes (https://docs.nvidia.com/tao/tao-toolkit/text/release_notes.html) • If you have any questions or feedback, see the discussions onTAO Toolkit Developer Forums TAO Toolkit Developer Forums (https://forums.developer.nvidia.com/c/accelerated-computing/intelligent-video-analytics/tao-toolkit/17) • Deploy your model on the edge using DeepStream. Learn more aboutDeepStream SDK DeepStream SDK (https://developer.nvidia.com/deepstream-sdk) Ethical Considerations: ======================= NVIDIA Visual ChangeNet-Segmentation model detects changes between pair-wise images. NVIDIA’s platforms and application frameworks enable developers to build a wide array of AI applications. Consider potential algorithmic bias when choosing or creating the models being deployed. NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Promise and the Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere here (https://www.nvidia.com/en-us/support/submit-security-vulnerability)"
  },
  {
    "name": "breeze-7b-instruct",
    "nvidia_url": "https://build.nvidia.com/mediatek/breeze-7b-instruct",
    "scraped_on": "2025-10-15T21:44:17.009491",
    "tags": [
      "chat",
      "mediatek",
      "regional language generation",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description =========== Breeze-7B-Instruct derives from the base model Breeze-7B-Base, making the resulting model amenable to be used as-is for commonly seen tasks. The current release version of Breeze-7B is v1.0, which has undergone a more refined training process compared to Breeze-7B-v0_1, resulting in significantly improved performance in both English and Traditional Chinese. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to theBreeze Model card. Breeze Model card (https://huggingface.co/MediaTek-Research/Breeze-7B-Instruct-v1_0) License and Terms of use ======================== GOVERNING TERMS: Your use of this API is governed by theNVIDIA API Trial Service Terms of Use; and the use of this model is governed by theNVIDIA AI Foundation Models Community License. NVIDIA API Trial Service Terms of Use (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA AI Foundation Models Community License (https://docs.nvidia.com/ai-foundation-models-community-license.pdf) Model Developer:MediaTek ResearchModel Release Date:March 5, 2024. Model Developer: Model Release Date: Features ======== • Expanding the vocabulary dictionary size from 32k to 62k to better support Traditional Chinese • 8k-token context length • Multi-turn dialogue (without special handling for harmfulness) Benchmark Performance ===================== The comparison of Breeze-7B-Instruct-v1_0 with other open-source instruction-tuned language models of similar parameter size, known for their good performance in Chinese, is presented here. GPT-3.5-Turbo (https://openai.com) Qwen1.5-7B-Chat (https://huggingface.co/Qwen/Qwen1.5-7B-Chat) Breeze-7B-Instruct-v1_0 (https://huggingface.co/MediaTek-Research/Breeze-7B-Instruct-v1_0) Breeze-7B-Instruct-v1_0 Mistral-7B-v0.2-Instruct (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) Yi-6B-Chat (https://huggingface.co/01-ai/Yi-6B-Chat) Taiwan-LLM-13B-v2.0-chat (https://huggingface.co/yentinglin/Taiwan-LLM-13B-v2.0-chat) Taiwan-LLM-7B-v2.1-chat (https://huggingface.co/yentinglin/Taiwan-LLM-7B-v2.1-chat) * Taiwan-LLM models respond to multi-turn questions (English) in Traditional Chinese. Breeze-7B-Instruct-v1_0 Model Architecture • Architecture Type: Causal decoder-only transformer language model • Network Architecture: Mistral7b Input • Input Type: Text • Input Format: String • Input Parameters: max_tokens, temperature, top_p, stop, frequency_penalty, presence_penalty, seed Output • Output Type: Text • Output Format: String Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Software Integration: ===================== • Supported Hardware Platform(s): Lovelace [Preferred/Supported] Operating System(s): • Linux Model Version ============= Breeze-7B-Instruct-v1_0 Inference ========= Engine:Triton + TensorRT-LLMTest Hardware:L40 Engine: Test Hardware:"
  },
  {
    "name": "bge-m3",
    "nvidia_url": "https://build.nvidia.com/baai/bge-m3",
    "scraped_on": "2025-10-15T21:44:13.801227",
    "tags": [
      "baai",
      "embeddings",
      "retrieval augmented generation",
      "text-to-embedding"
    ],
    "model_card": "Model Overview ============== Description ----------- BGE-M3 is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity. • Multi-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval. • Multi-Linguality: It can support more than 100 working languages. • Multi-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. Some suggestions for retrieval pipeline in RAG Authors recommend to use the following pipeline: hybrid retrieval + re-ranking. • Hybrid retrieval leverages the strengths of various methods, offering higher accuracy and stronger generalization capabilities. A classic example: using both embedding retrieval and the BM25 algorithm. Now, you can try to use BGE-M3, which supports both embedding and sparse retrieval. This allows you to obtain token weights (similar to the BM25) without any additional cost when generate dense embeddings. To use hybrid retrieval, please refer toVespaand Milvus. Hybrid retrieval leverages the strengths of various methods, offering higher accuracy and stronger generalization capabilities. A classic example: using both embedding retrieval and the BM25 algorithm. Now, you can try to use BGE-M3, which supports both embedding and sparse retrieval. This allows you to obtain token weights (similar to the BM25) without any additional cost when generate dense embeddings. To use hybrid retrieval, please refer toVespaand Milvus. Vespa (https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/mother-of-all-embedding-models-cloud.ipynb) • As cross-encoder models, re-ranker demonstrates higher accuracy than bi-encoder embedding model. Utilizing the re-ranking model (e.g.,bge-reranker,bge-reranker-v2) after retrieval can further filter the selected text. As cross-encoder models, re-ranker demonstrates higher accuracy than bi-encoder embedding model. Utilizing the re-ranking model (e.g.,bge-reranker,bge-reranker-v2) after retrieval can further filter the selected text. bge-reranker (https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker) bge-reranker-v2 (https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker) Specs ===== • Model BAAI/bge-m3 (https://huggingface.co/BAAI/bge-m3) BAAI/bge-m3-unsupervised (https://huggingface.co/BAAI/bge-m3-unsupervised) BAAI/bge-m3-retromae (https://huggingface.co/BAAI/bge-m3-retromae) xlm-roberta (https://huggingface.co/FacebookAI/xlm-roberta-large) retromae (https://github.com/staoxiao/RetroMAE) BAAI/bge-large-en-v1.5 (https://huggingface.co/BAAI/bge-large-en-v1.5) BAAI/bge-base-en-v1.5 (https://huggingface.co/BAAI/bge-base-en-v1.5) BAAI/bge-small-en-v1.5 (https://huggingface.co/BAAI/bge-small-en-v1.5) Terms of use ------------ bge-m3 is licensed under theMIT Licence. MIT Licence (https://opensource.org/license/MIT) Ethical Considerations: ----------------------- NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concerns here. References ---------- HuggingFace HuggingFace (https://huggingface.co/BAAI/bge-m3) Github Github (https://github.com/FlagOpen/FlagEmbedding) Model Architecture ================== Architecture Type:TransformerNetwork Architecture:Fine-tuned XLMRobertaModelEmbedding Dimensiion:1024Parameter Count:568 million Architecture Type: Network Architecture: Embedding Dimensiion: Parameter Count: Input ===== Input Type:TextInput Format:List of strings Input Type: Input Format: Output ====== Output Type:Floating PointsOutput Format:list of float arraysOther Properties Related to Output:Each array contains the embeddings for the corresponding input string. Output Type: Output Format: Other Properties Related to Output: Model Version ============= BAAI/bge-m3 Supported Operating System(s): ============================== • Linux Training Dataset: ================= MLDR (https://huggingface.co/datasets/Shitao/MLDR) bge-m3-data (https://huggingface.co/datasets/Shitao/bge-m3-data) Inference: ========== Engine:TensorRTwithTritonTest Hardware:L40 Engine: TensorRT (https://github.com/NVIDIA/TensorRT) Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware:"
  },
  {
    "name": "solar-10.7b-instruct",
    "nvidia_url": "https://build.nvidia.com/upstage/solar-10_7b-instruct",
    "scraped_on": "2025-10-15T21:44:10.618516",
    "tags": [
      "chat",
      "language generation",
      "large language models",
      "non-commercial use only",
      "text-to-text",
      "upstage"
    ],
    "model_card": "Model Overview ============== Description =========== SOLAR-10.7B, is an advanced large language model (LLM) with 10.7 billion parameters, that demonstrates superior performance in various natural language processing (NLP) tasks. It's compact, yet remarkably powerful, and demonstrates unparalleled state-of-the-art performance in models with parameters under 30B. It uses a methodology for scaling LLMs called depth up-scaling (DUS), which encompasses architectural modifications and continued pretraining. In other words, it integrates Mistral 7B weights into the upscaled layers, and finally, continues pre-training for the entire model. It outperforms models with up to 30B parameters, even surpassing the Mixtral 8X7B model. We at NVIDIA have optimized SOLAR-10.7B using TensorRT-LLM to run optimally on latest NVIDIA GPUs. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to theSOLAR-10.7B-Instruct-v1.0 Model Card. SOLAR-10.7B-Instruct-v1.0 Model Card (https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0) License and Terms of use ======================== GOVERNING TERMS: Your use of this API is governed by theNVIDIA API Trial Service Terms of Use; and the use of this model is governed by theNVIDIA AI Foundation Models Community LicenseandCC BY-NC 4.0 License. NVIDIA API Trial Service Terms of Use (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA AI Foundation Models Community License (https://docs.nvidia.com/ai-foundation-models-community-license.pdf) CC BY-NC 4.0 License (https://creativecommons.org/licenses/by-nc/4.0/legalcode.en) Model Developer:UpstageModel Release Date:December 13, 2023 Model Developer: Model Release Date: Model Architecture • Architecture Type: Transformer • Network Architecture: Llama Input • Input Type: Text • Input Format: String • Input Parameters: max_tokens, temperature, top_p, stop, frequency_penalty, presence_penalty, seed Output • Output Type: Text • Output Format: String Software Integration: ===================== • Supported Hardware Platform(s): NVIDIA Lovelace [Preferred/Supported] Operating System(s): • Linux Inference ========= Engine:TensorRT-LLMTest Hardware:L40S Engine: Test Hardware: Usage Instructions ================== This model has been fine-tuned primarily for single-turn conversation, making it less suitable for multi-turn conversations such as chat. Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nv-embed-v1",
    "nvidia_url": "https://build.nvidia.com/nvidia/nv-embed-v1",
    "scraped_on": "2025-10-15T21:44:07.497646",
    "tags": [
      "non-commercial use only",
      "nvidia",
      "retrieval augmented generation",
      "text-to-embedding"
    ],
    "model_card": "Model Overview ============== Description ----------- The NV-Embed Model is a generalist embedding model that excels across 56 tasks, including retrieval, reranking, classification, clustering, and semantic textual similarity tasks. NV-Embed achieves the highest score of 59.36 on 15 retrieval tasks within this benchmark. NV-Embed features several innovative designs, such as latent vectors for improved pooled embedding output and a two-stage instruction tuning method, enhancing the accuracy of both retrieval and non-retrieval tasks. For more technical details, refer to the paper:NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models. NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models (https://arxiv.org/pdf/2405.17428) Terms of use ------------ The use of this model is governed by thelicense. license (https://spdx.org/licenses/CC-BY-NC-4.0) References(s) ------------- For more details, refer to theNV-Embed paper. NV-Embed paper (https://arxiv.org/pdf/2405.17428) Intended use ------------ The NV-Embed Model is designed for users who need a high-performance generalist embedding model for tasks such as text retrieval, reranking, classification, clustering, and semantic textual similarity. Model Architecture ------------------ Architecture Type:Decoder-only LLMNetwork Architecture:Mistral-7B-v0.1 with Latent-Attention poolingEmbedding Dimension:4096Max Input Tokens:32kParameter Count:7.1 billion Architecture Type: Network Architecture: Embedding Dimension: Max Input Tokens: Parameter Count: The NV-Embed Model is based on the Mistral-7B-v0.1 architecture with a unique Latent-Attention pooling mechanism. This allows the model to generate more expressive pooled embeddings by having the LLM attend to latent vectors. It employs a two-stage instruction tuning method to improve performance across various tasks. Input ----- Input Type:textInput Format:list of strings with task-specific instructions Input Type: Input Format: Output ------ Output Type:floatsOutput Format:list of float arrays, each array containing the embeddings for the corresponding input string Output Type: Output Format: Model Version(s) ---------------- NV-Embed-v1 Training Dataset & Evaluation ============================= Training Dataset ---------------- The NV-Embed model was trained on a diverse mixture of publicly available datasets, including various retrieval and non-retrieval tasks. The training data did not include any synthetic data from proprietary models like GPT-4, ensuring the model's accessibility and reproducibility. Evaluation Results ------------------ NV-Embed was evaluated using the Massive Text Embedding Benchmark (MTEB), achieving a record-high score of 69.32 across 56 tasks. It significantly outperforms previous leading embedding models, particularly excelling in retrieval tasks. Performance on MTEB benchmark: • Overall Score:69.32 Overall Score: • Score on Retrieval Tasks:59.36 Score on Retrieval Tasks: Ethical Considerations ====================== Bias, Safety & Security, and Privacy ------------------------------------ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcardshere. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nv-embed-v1/bias) here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Special Training Data Considerations ------------------------------------ The model was trained on publicly available data, which may contain toxic language and societal biases. Therefore, the model may amplify those biases, such as associating certain genders with specific social stereotypes."
  },
  {
    "name": "palmyra-med-70b",
    "nvidia_url": "https://build.nvidia.com/writer/palmyra-med-70b",
    "scraped_on": "2025-10-15T21:44:07.085317",
    "tags": [
      "chat",
      "healthcare",
      "text-to-text",
      "writer"
    ],
    "model_card": "Model Overview ============== Model Overview Model Developer:WriterModel Release Date:June 10th, 2024 Model Developer: Model Release Date: Description: ============ Description: Palmyra-Med is a model built by Writer specifically to meet the needs of the healthcare industry. The leading LLM on biomedical benchmarks, with an average score of 85.87%, outperforming base models in the industry and a medically trained human test-taker. NVIDIA has optimized this model using TRT-LLM with 2 H100s. This model is ready for non-commercial use. Third-Party Community Consideration =================================== Third-Party Community Consideration This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Writer'sModel Card. Model Card (https://huggingface.co/Writer/Palmyra-Med-70B) License, Acceptable Use, and Research Privacy Policy ==================================================== License, Acceptable Use, and Research Privacy Policy The trial service is governed by the NVIDIA API Trial Terms of Service (found athttps://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf)); and the use of this model is governed by theWriter Open Model License. https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf ) (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) Writer Open Model License (https://writer.com/legal/open-model-license/) Specialized for Biomedical Applications --------------------------------------- Specialized for Biomedical Applications Palmyra-Med-70B is meticulously designed to meet the unique linguistic and knowledge demands of the medical and life sciences sectors. It has been fine-tuned on an extensive collection of high-quality biomedical data, ensuring it can comprehend and generate text with precise domain-specific accuracy and fluency. Intended Use ============ Intended Use Intended Use CasesPalmyra-Med-70b is intended for non-commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. Intended Use Cases Out-of-scopeUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Writer Open source License. Use in languages other than English**. Out-of-scope **Developers may fine-tune Palmyra-Med-70b models for languages beyond English provided they comply with the Writer Open source License and the Acceptable Use Policy. Medical Use Cases ================= Medical Use Cases Palmyra-Med-70b excels in analyzing and summarizing complex clinical notes, EHR data, and discharge summaries, extracting key information to generate concise, structured summaries. It can answer a wide range of medical questions and perform advanced clinical entity recognition, identifying key medical concepts such as diseases, symptoms, medications, procedures, and anatomical structures from unstructured text. By leveraging its deep understanding of medical terminology, the model enhances information retrieval, data analysis, and knowledge discovery from EHRs, research articles, and other biomedical sources. These capabilities support applications like clinical decision support, pharmacovigilance, and medical research. Bias, Risks, and Limitations ============================ Bias, Risks, and Limitations Palmyra-Med-70b, despite leveraging high-quality data, may contain inaccuracies, biases, or misalignments and has not been rigorously evaluated in clinical trials or real-world healthcare settings. It is advised not to use the model for direct patient care, clinical decision support, or professional medical purposes. Palmyra-Med-70b should not replace professional medical judgment, and adapting it for medical use would require extensive additional work, including thorough testing, guideline alignment, bias mitigation, human oversight, and regulatory compliance. Always consult a qualified healthcare provider for personal medical needs. Model Architecture • Architecture Type: Transformer • Network Architecture: Llama • Finetuned from model: Palmyra-X-004 • Model Version: 0.1 Input • Input Type: Text • Input Format: String • Input Parameters: max_tokens, temperature, top_p, stop, frequency_penalty, presence_penalty, seed Output • Output Type: Text • Output Format: String Software Integration: ===================== Software Integration: • Supported Hardware Platform(s): NVIDIA Hopper • [Preferred/Supported] Operating System(s): Linux Inference ========= Inference Engine:TensorRT-LLMTest Hardware:H100 Engine: Test Hardware: Evaluation Results ================== Evaluation Results Palmyra-Med-70b achieves state-of-the-art results with an average score of 85.9% despite having fewer parameters than typical language models. Its strong performance in tasks like Clinical Knowledge Graph (KG), Medical Genetics, and PubMedQA underscores its effective grasp of biomedical knowledge. More information can be found here:https://huggingface.co/Writer/Palmyra-Med-70B https://huggingface.co/Writer/Palmyra-Med-70B Ethical Considerations: ======================= Ethical Considerations: NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "ocdrnet",
    "nvidia_url": "https://build.nvidia.com/nvidia/ocdrnet",
    "scraped_on": "2025-10-15T21:44:03.641752",
    "tags": [
      "computer vision",
      "cv",
      "image",
      "nvidia",
      "optical character detection",
      "optical character recognition",
      "tao toolkit",
      "video",
      "vlm"
    ],
    "model_card": "This model card combines the relevant information of OCR and OCD models OCRNet Model Overview ===================== Description =========== Optical character recognition network recognizes characters from the gray images. Terms of use ============ License to use these models is covered by the Model EULA. By downloading the unpruned or pruned version of the model, you accept the terms and conditions of theselicenses. licenses (https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/) References(s): ============== Citations --------- • Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., ... & Lee, H. (2019). What is wrong with scene text recognition model comparisons? dataset and model analysis. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 4715-4723). • Zhang, Y., Gueguen, L., Zharkov, I., Zhang, P., Seifert, K., & Kadlec, B. (2017, July). Uber-text: A large-scale dataset for optical character recognition from street-level imagery. In SUNw: Scene Understanding Workshop-CVPR (Vol. 2017, p. 5). • Singh, A., Pang, G., Toh, M., Huang, J., Galuba, W., & Hassner, T. (2021). Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 8802-8812) • Graves, Alex, et al. \"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.\" In: Proceedings of the 23rd international conference on Machine learning (2006) • He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for Image Recognition. In: CVPR (2015) • Zhou, D., Yu, Z., Xie, E., Xiao, C., Anandkumar, A., Feng, J., & Alvarez, J. M. (2022, June). Understanding the robustness in vision transformers. In International Conference on Machine Learning (pp. 27378-27394). PMLR. • Kuo, C. W., Ashmore, J. D., Huggins, D., & Kira, Z. (2019, January). Data-efficient graph embedding learning for PCB component detection. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV) (pp. 551-560). IEEE. Model Architecture: =================== Architecture Type:Convolution Neural Network (CNN)Network Architecture:ResNet50Model Version: Architecture Type: Network Architecture: Model Version: • trainable_v1.0- Pre-trained model with ResNet backbone on scene text. trainable_v1.0 • deployable_v1.0- Models deployable with ResNet backbone. deployable_v1.0 • trainable_v2.0- Pre-trained model with FAN backbone on scene text. trainable_v2.0 • deployable_v2.0- Model deployable with FAN backbone on scene text. deployable_v2.0 • trainable_v2.1- Pre-trained model with FAN backbone on PCB text. trainable_v2.1 • deployable_v2.1- Model deployable with FAN backbone on PCB text. deployable_v2.1 Input: ====== Input Type(s):ImageInput Format:Gray ImageInput Parameters:3DOther Properties Related to Input: Input Type(s): Input Format: Input Parameters: Other Properties Related to Input: • Gray Images of 1 X 32 X 100 (C H W) for trainable_v1.0/deployable_v1.0 • Gray Images of 1 X 64 X 200 (C H W) for trainable_v2.0/trainable_v2.1/deployable_v2.0/deployable_v2.1 Output: ======= Output Type(s):Sequence of charactersOutput Format:Character Id sequence: Text String(s)Other Properties Related to Output:None Output Type(s): Output Format: Other Properties Related to Output: Software Integration: ===================== Runtime(s):NVIDIA AI EnterpriseToolkit:TAO FrameworkSupported Hardware Platform(s):Ampere, Jetson, Hopper, Lovelace, Pascal, TuringSupported Operating System(s):Linux Runtime(s): Toolkit: Supported Hardware Platform(s): Supported Operating System(s): Training & Finetuning: ====================== Dataset: ======== OCRNet pretrained model was trained on Uber-Text and TextOCR dataset. The Uber-Text contains street-level images collected from car mounted sensors and truths annotated by a team of image analysts. The TextOCR is the images with annotated texts from OpenImages dataset. After collecting the original data from Uber-text and TextOCR, we remove all the text images with*label in Uber-text and only keep alphanumeric text images with the maximum length is 25 in both datasets. We finally construct the dataset with 805007 text images for training and 24388 images for validation. Inference: ========== Engine:TensorRTTest Hardware: Engine: Test Hardware: • Orin Nano • Orin NX • AGX Orin • L4 • L40 • T4 • A2 • A30 • A100 • H100 OCDNet Model Overview ===================== Model Overview ============== The model described in this card is an optical characters detection network, which aims to detect text in images. Trainable and deployable OCDNet models are provided. These are trained on Uber-Text dataset and ICDAR2015 dataset respectively. Terms of use ============ License to use these models is covered by the Model EULA. By downloading the unpruned or pruned version of the model, you accept the terms and conditions of theselicenses. licenses (https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/) Model Architecture ================== This model is based on a relatively sophisticated text detection network called DBNet. DBNet is a network architecture for real-time scene text detection with differentiable binarization. It aims to solve the problem of text localization and segmentation in natural images with complex backgrounds and various text shapes. Training ======== The training algorithm inserts the binarization operation into the segmentation network and jointly optimizes it so that the network can learn to separate foreground and background pixels more effectively. The binarization threshold is learned by minimizing the IoU loss between the predicted binary map and the ground truth binary map. Training Data ------------- The trainable models were trained on theUber-Textdataset andICDAR2015dataset respectively. The Uber-Text dataset contains street-level images collected from car mounted sensors and truths annotated by a team of image analysts--including train_4Kx4K, train_1Kx1K, val_4Kx4K, val_1Kx1K, test_4Kx4K as the training datasets and test_1Kx1K as the validation dataset. The dataset was constructed with 107812 images for training and 10157 images for validation. The ICDAR2015 dataset contains 1000 training images and 500 test images. The deployable models were ONNX models that were exported using the trainable models. Uber-Text (https://s3-us-west-2.amazonaws.com/uber-common-public/ubertext/index.html) ICDAR2015 (https://rrc.cvc.uab.es/?ch=4) Performance =========== Evaluation Data --------------- The OCDNet model was evaluated using the Uber-Text test dataset and ICDAR2015 test dataset. Methodology and KPI ------------------- The key performance indicator is the hmean of detection. The KPI for the evaluation data are reported below. Real-time Inference Performance ------------------------------- The inference uses FP16 precision. The input shape is<batch>x3x640x640. The inference performance runs against an OCDNet-deployable model withtrtexecon AGX Orin, Orin NX, Orin Nano, NVIDIA L4, NVIDIA L4, and NVIDIA A100 GPUs. The Jetson devices run at Max-N configuration for maximum system performance. The data is for inference-only performance. The end-to-end performance with streaming video data might vary slightly depending on the application's use case. trtexec (https://github.com/NVIDIA/TensorRT/tree/master/samples/trtexec) How to Use This Model ===================== This model needs to be used with NVIDIA Hardware and Software: The model can run on any NVIDIA GPU, including NVIDIA Jetson devices, withTAO Toolkit,DeepStream SDKorTensorRT. TAO Toolkit (https://developer.nvidia.com/tao-toolkit) DeepStream SDK (https://developer.nvidia.com/deepstream-sdk) TensorRT (https://developer.nvidia.com/tensorrt) The primary use case for this model is to detect text on images. There are two types of models provided (both unpruned). • trainable • deployable Thetrainablemodels are intended for training with the user's own dataset using TAO Toolkit. This can provide high-fidelity models that are adapted to the use case. A Jupyter notebook is available as a part of theTAO containerand can be used to re-train. TAO container (https://ngc.nvidia.com/catalog/containers/nvidia:tao:tao-toolkit) Thedeployablemodels share the same structure as thetrainablemodel, but inonnxformat. Thedeployablemodels can be deployed using TensorRT, nvOCDR, and DeepStream. Input ----- Images of C x H x W (H and W should be multiples of 32.) Output ------ BBox or polygon coordinates for each detected text in the input image Instructions to Use the Model with TAO -------------------------------------- To use these models as pretrained weights for transfer learning, use the snippet below as a template for themodelcomponent of the experiment spec file to train an OCDNet model. For more information on the experiment spec file, refer to theTAO Toolkit User Guide. TAO Toolkit User Guide (https://docs.nvidia.com/tao/tao-toolkit/index.html) To use trainable_resnet18_v1.0 model: To use trainable_ocdnet_vit_v1.0 model: Instructions to deploy the model with DeepStream ------------------------------------------------ To create the entire end-to-end video analytic application, deploy this model withDeepStream SDK. DeepStream SDK is a streaming analytic toolkit to accelerate building AI-based video analytic applications. DeepStream supports direct integration of this model into the Deepstream sample app. DeepStream SDK (https://developer.nvidia.com/deepstream-sdk) To deploy this model withDeepStream, followthese instructions. DeepStream (https://developer.nvidia.com/deepstream-sdk) these instructions (https://docs.nvidia.com/tao/tao-toolkit/text/ds_tao/nvocdr_ds.html) Limitations =========== Restricted Usage in Different Fields ------------------------------------ The NVIDIA OCDNet trainable model is trained on Uber Text, ICDAR2015 and PCB text dataset, which contains street-view images only. To get better accuracy in a specific field, more data is usually required to fine tune the pre-trained model with TAO Toolkit. Model versions: =============== • trainable_resnet18_v1.0- Pre-trained models with deformable-resnet18 backbone, trained on Uber-Text dataset. trainable_resnet18_v1.0 • trainable_resnet50_v1.0- Pre-trained models with deformable-resnet50 backbone, trained on Uber-Text dataset. trainable_resnet50_v1.0 • trainable_ocdnet_vit_v1.0- Pre-trained models with fan-tiny backbone, trained on ICDAR2015 dataset. trainable_ocdnet_vit_v1.0 • trainable_ocdnet_vit_v1.1- Pre-trained models with fan-tiny backbone, trained on Uber-Text dataset. trainable_ocdnet_vit_v1.1 • trainable_ocdnet_vit_v1.2- Pre-trained models with fan-tiny backbone, trained on PCB dataset. trainable_ocdnet_vit_v1.2 • trainable_ocdnet_vit_v1.3- Pre-trained models with fan-tiny backbone, trained on ImageNet2012 dataset. trainable_ocdnet_vit_v1.3 • trainable_ocdnet_vit_v1.4- Pre-trained models with fan-tiny backbone, trained on ICDAR2015 dataset and model are pruned. trainable_ocdnet_vit_v1.4 • deployable_v1.0- Model deployable with deformable-resnet backbone. deployable_v1.0 • deployable_v2.0- Model deployable with fan-tiny backbone, trained on ICDAR2015. deployable_v2.0 • deployable_v2.1- Model deployable with fan-tiny backbone, trained on Uber-Text. deployable_v2.1 • deployable_v2.2- Model deployable with fan-tiny backbone, trained on PCB dataset. deployable_v2.2 • deployable_v2.3- Model deployable with fan-tiny backbone, trained on ICDAR2015 and model are pruned. deployable_v2.3 Reference ========= Citations --------- • Liao M., Wan Z., Yao C., Chen K., Bai X.: Real-time Scene Text Detection with Differentiable Binarization (2020). • Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., and Wei, Y: Deformable convolutional networks. (2017). • He, W., Zhang, X., Yin, F., and Liu, C.: Deep direct regression for multi-oriented scene text detection. (2017). • Zhang, Y., Gueguen, L., Zharkov, I., Zhang, P., Seifert, K., & Kadlec, B. (2017, July). Uber-text: A large-scale dataset for optical character recognition from street-level imagery. In SUNw: Scene Understanding Workshop-CVPR (Vol. 2017, p. 5). • Zhou, D., Yu, Z., Xie, E., Xiao, C., Anandkumar, A., Feng, J., & Alvarez, J. M. (2022, June). Understanding the robustness in vision transformers. In International Conference on Machine Learning (pp. 27378-27394). PMLR. • Kuo, C. W., Ashmore, J. D., Huggins, D., & Kira, Z. (2019, January). Data-efficient graph embedding learning for PCB component detection. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV) (pp. 551-560). IEEE. Using TAO Pre-trained Models ============================ • GetTAO Container TAO Container (https://ngc.nvidia.com/catalog/containers/nvidia:tao:tao-toolkit) • Get other purpose-built models from the NGC model registry:TrafficCamNetPeopleNetPeopleNetPeopleNet-TransformerDashCamNetFaceDetectIRVehicleMakeNetVehicleTypeNetPeopleSegNetPeopleSemSegNetLicense Plate DetectionLicense Plate RecognitionGaze EstimationFacial LandmarkHeart Rate EstimationGesture RecognitionEmotion RecognitionFaceDetect2D Body Pose EstimationActionRecognitionNetActionRecognitionNetPoseClassificationNetPeople ReIdentificationPointPillarNetCitySegFormerRetail Object DetectionRetail Object EmbeddingOptical InspectionOptical Character DetectionOptical Character RecognitionPCB ClassificationPeopleSemSegFormerLPDNetLicense Plate RecognitionGaze EstimationFacial LandmarkHeart Rate EstimationGesture RecognitionEmotion RecognitionFaceDetect2D Body Pose EstimationActionRecognitionNetActionRecognitionNetPoseClassificationNetPeople ReIdentificationPointPillarNetCitySegFormerRetail Object DetectionRetail Object EmbeddingOptical InspectionOptical Character DetectionOptical Character RecognitionPCB ClassificationPeopleSemSegFormer • TrafficCamNet TrafficCamNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:trafficcamnet) • PeopleNet PeopleNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplenet) • PeopleNet PeopleNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplenet) • PeopleNet-Transformer PeopleNet-Transformer (https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplenet_transformer) • DashCamNet DashCamNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:dashcamnet) • FaceDetectIR FaceDetectIR (https://ngc.nvidia.com/catalog/models/nvidia:tao:facedetectir) • VehicleMakeNet VehicleMakeNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:vehiclemakenet) • VehicleTypeNet VehicleTypeNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:vehicletypenet) • PeopleSegNet PeopleSegNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplesegnet) • PeopleSemSegNet PeopleSemSegNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:peoplesemsegnet) • License Plate Detection License Plate Detection (https://ngc.nvidia.com/catalog/models/nvidia:tao:lpdnet) • License Plate Recognition License Plate Recognition (https://ngc.nvidia.com/catalog/models/nvidia:tao:lprnet) • Gaze Estimation Gaze Estimation (https://ngc.nvidia.com/catalog/models/nvidia:tao:gazenet) • Facial Landmark Facial Landmark (https://ngc.nvidia.com/catalog/models/nvidia:tao:fpenet) • Heart Rate Estimation Heart Rate Estimation (https://ngc.nvidia.com/catalog/models/nvidia:tao:heartratenet) • Gesture Recognition Gesture Recognition (https://ngc.nvidia.com/catalog/models/nvidia:tao:gesturenet) • Emotion Recognition Emotion Recognition (https://ngc.nvidia.com/catalog/models/nvidia:tao:emotionnet) • FaceDetect FaceDetect (https://ngc.nvidia.com/catalog/models/nvidia:tao:facenet) • 2D Body Pose Estimation 2D Body Pose Estimation (https://ngc.nvidia.com/catalog/models/nvidia:tao:bodyposenet) • ActionRecognitionNet ActionRecognitionNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:actionrecognitionnet) • ActionRecognitionNet ActionRecognitionNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:actionrecognitionnet) • PoseClassificationNet PoseClassificationNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:poseclassificationnet) • People ReIdentification People ReIdentification (https://ngc.nvidia.com/catalog/models/nvidia:tao:reidentificationnet) • PointPillarNet PointPillarNet (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pointpillarnet) • CitySegFormer CitySegFormer (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/citysemsegformer) • Retail Object Detection Retail Object Detection (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/retail_object_detection) • Retail Object Embedding Retail Object Embedding (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/retail_object_recognition) • Optical Inspection Optical Inspection (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/optical_inspection) • Optical Character Detection Optical Character Detection (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/ocdnet) • Optical Character Recognition Optical Character Recognition (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/ocrnet) • PCB Classification PCB Classification (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pcb_classification) • PeopleSemSegFormer PeopleSemSegFormer (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplesemsegformer) • LPDNet LPDNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:lpdnet) • License Plate Recognition License Plate Recognition (https://ngc.nvidia.com/catalog/models/nvidia:tao:lprnet) • Gaze Estimation Gaze Estimation (https://ngc.nvidia.com/catalog/models/nvidia:tao:gazenet) • Facial Landmark Facial Landmark (https://ngc.nvidia.com/catalog/models/nvidia:tao:fpenet) • Heart Rate Estimation Heart Rate Estimation (https://ngc.nvidia.com/catalog/models/nvidia:tao:heartratenet) • Gesture Recognition Gesture Recognition (https://ngc.nvidia.com/catalog/models/nvidia:tao:gesturenet) • Emotion Recognition Emotion Recognition (https://ngc.nvidia.com/catalog/models/nvidia:tao:emotionnet) • FaceDetect FaceDetect (https://ngc.nvidia.com/catalog/models/nvidia:tao:facenet) • 2D Body Pose Estimation 2D Body Pose Estimation (https://ngc.nvidia.com/catalog/models/nvidia:tao:bodyposenet) • ActionRecognitionNet ActionRecognitionNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:actionrecognitionnet) • ActionRecognitionNet ActionRecognitionNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:actionrecognitionnet) • PoseClassificationNet PoseClassificationNet (https://ngc.nvidia.com/catalog/models/nvidia:tao:poseclassificationnet) • People ReIdentification People ReIdentification (https://ngc.nvidia.com/catalog/models/nvidia:tao:reidentificationnet) • PointPillarNet PointPillarNet (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pointpillarnet) • CitySegFormer CitySegFormer (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/citysemsegformer) • Retail Object Detection Retail Object Detection (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/retail_object_detection) • Retail Object Embedding Retail Object Embedding (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/retail_object_recognition) • Optical Inspection Optical Inspection (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/optical_inspection) • Optical Character Detection Optical Character Detection (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/ocdnet) • Optical Character Recognition Optical Character Recognition (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/ocrnet) • PCB Classification PCB Classification (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pcb_classification) • PeopleSemSegFormer PeopleSemSegFormer (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplesemsegformer) License ======= The license to use these models is covered by the Model EULA. By downloading the unpruned or pruned version of the model, you accept the terms and conditions of theselicenses. licenses (https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/) Technical Blogs =============== • Train like a ‘pro’ without being an AI expert using TAO AutoML Train like a ‘pro’ without being an AI expert using TAO AutoML (https://developer.nvidia.com/blog/training-like-an-ai-pro-using-tao-automl/) • Create Custom AI models using NVIDIA TAO Toolkit with Azure Machine Learning Create Custom AI models using NVIDIA TAO Toolkit with Azure Machine Learning (https://developer.nvidia.com/blog/creating-custom-ai-models-using-nvidia-tao-toolkit-with-azure-machine-learning/) • Developing and Deploying AI-powered Robots with NVIDIA Isaac Sim and NVIDIA TAO Developing and Deploying AI-powered Robots with NVIDIA Isaac Sim and NVIDIA TAO (https://developer.nvidia.com/blog/developing-and-deploying-ai-powered-robots-with-nvidia-isaac-sim-and-nvidia-tao/) • Learn endless ways to adapt and supercharge your AI workflows with TAO -Whitepaper Whitepaper (https://developer.nvidia.com/tao-toolkit-usecases-whitepaper/1-introduction) • Customize Action Recognition with TAO and deploy with DeepStream Customize Action Recognition with TAO and deploy with DeepStream (https://developer.nvidia.com/blog/developing-and-deploying-your-custom-action-recognition-application-without-any-ai-expertise-using-tao-and-deepstream/) • Read the 2 part blog on training and optimizing 2D body pose estimation model with TAO -Part 1|Part 2 Part 1 (https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-1) Part 2 (https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-2) • Learn how to trainareal-time License plate detection and recognition appwith TAO and DeepStream. real-time License plate detection and recognition app (https://developer.nvidia.com/blog/creating-a-real-time-license-plate-detection-and-recognition-app) • Model accuracy is extremely important. Learn how to achievestate-of-the-art accuracy for classification and object detection modelsusing TAO. state-of-the-art accuracy for classification and object detection models (https://developer.nvidia.com/blog/preparing-state-of-the-art-models-for-classification-and-object-detection-with-tao-toolkit/) Suggested Reading ================= • More information about TAO Toolkit and pre-trained models can be found at theNVIDIA Developer Zone. NVIDIA Developer Zone (https://developer.nvidia.com/tao-toolkit) • Read theTAO Toolkit Quick Start Guideandrelease notes. TAO Toolkit Quick Start Guide (https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_quick_start_guide.html) release notes (https://docs.nvidia.com/tao/tao-toolkit/text/release_notes.html) • If you have any questions or feedback, please refer to the discussions on theTAO Toolkit Developer Forums. TAO Toolkit Developer Forums (https://forums.developer.nvidia.com/c/accelerated-computing/intelligent-video-analytics/tao-toolkit/17) • Deploy your model on the edge using theDeepStream SDK. DeepStream SDK (https://developer.nvidia.com/deepstream-sdk) Ethical AI ========== The NVIDIA OCDNet model detects optical characters. NVIDIA’s platforms and application frameworks enable developers to build a wide array of AI applications. Consider potential algorithmic bias when choosing or creating the models being deployed. Work with the model’s developers to ensure that it meets the requirements for the relevant industry and use case, that the necessary instructions and documentation are provided to understand error rates, confidence intervals, and results, and that the model is being used under the conditions and in the manner intended. Please report security vulnerabilities or NVIDIA AI Concernshere here (https://www.nvidia.com/en-us/support/submit-security-vulnerability)"
  },
  {
    "name": "stable-diffusion-3-medium",
    "nvidia_url": "https://build.nvidia.com/stabilityai/stable-diffusion-3-medium",
    "scraped_on": "2025-10-15T21:44:03.147024",
    "tags": [
      "image generation",
      "stabilityai",
      "text-to-image"
    ],
    "model_card": "Model Overview ============== Note: You need to request the model checkpoint and license from Stability AI ============================================================================ Request the model checkpoint fromStability AI Stability AI (https://stability.ai/membership) Description: ============ Stable Diffusion 3 Medium is Stability AI's text-to-image model that features greatly improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. Compared to previous versions of Stable Diffusion versions, Stable Diffusion 3 leverages a novel Multimodal Diffusion Transformer (MMDiT) architecture that combines adiffusion transformer architectureandflow matching. We have optimized the model's speed using NVIDIA's TensorRT (TRT), resulting in significant acceleration with no quality loss. diffusion transformer architecture (https://arxiv.org/abs/2212.09748) flow matching (https://arxiv.org/abs/2210.02747) Developed by: Stability AIModel type: Generative text-to-image model Model Card ========== Stable Diffusion 3 Model Card Stable Diffusion 3 Model Card (https://huggingface.co/stabilityai/stable-diffusion-3-medium) Terms of use ============ By accessing this model, you are agreeing to the Stable Diffusion 3 terms and conditions of thelicense,acceptable use policyandstability.ai privacy policy license (https://huggingface.co/stabilityai/stable-diffusion-3-medium/blob/main/LICENSE.md) acceptable use policy (https://github.com/Stability-AI/stablediffusion/blob/main/LICENSE) stability.ai privacy policy (https://platform.stability.ai/legal/privacy-policy) Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; seeStability-AI's Stable Diffusion 3 Medium Model Card. Stability-AI's Stable Diffusion 3 Medium Model Card (https://huggingface.co/stabilityai/stable-diffusion-3-medium) References(s): ============== • Scaling Rectified Flow Transformers for High-Resolution Image Synthesispaper Scaling Rectified Flow Transformers for High-Resolution Image Synthesis (https://arxiv.org/abs/2307.01952) • Stability-AI's Stable Diffusion 3 Medium Model Cardwebpage Stability-AI's Stable Diffusion 3 Medium Model Card (https://huggingface.co/stabilityai/stable-diffusion-3-medium) Model Architecture: =================== Architecture Type:Transformer and Convolutional Neural Network (CNN)Network Architecture:Multimodal Diffusion TransformerModel Version:Stable Diffusion 3 Medium Architecture Type: Network Architecture: Model Version: Input: ====== Input Format:TextInput Parameters:steps, classifier-free guidance scale, output image aspect ratio, negative prompt Input Format: Input Parameters: Output: ======= Output Format:Red, Green, Blue (RGB) JPEG ImageOutput Size:1024x1024, 768x1344, 1344x768, 1344x768, 1344x768, 1344x768, 1216x832Output Parameters:2D Output Format: Output Size: Output Parameters: Software Integration: ===================== Supported Hardware Platform(s):Hopper, Ampere/TuringSupported Operating System(s):Linux Supported Hardware Platform(s): Supported Operating System(s): Inference: ========== Engine:TritonTest Hardware:Other Engine: Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware: Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nvclip",
    "nvidia_url": "https://build.nvidia.com/nvidia/nvclip",
    "scraped_on": "2025-10-15T21:44:00.028499",
    "tags": [
      "computer vision",
      "multimodal embeddings",
      "nvidia",
      "nvidia nim",
      "run-on-rtx",
      "text and image"
    ],
    "model_card": "NV-CLIP (Commercial Foundation Model) ===================================== Model Overview ============== NV-CLIP is a multimodal embeddings model for image and text. Trained on 700M proprietary images, NV-CLIP is the NVIDIA commercial version of OpenAI’s CLIP (Contrastive Language-Image Pre-Training) model. NV-CLIP can be applied to various areas such as multimodal search, zero-shot image classification, and downstream computer vision tasks such as object detection and more. Getting Started with NV-CLIP NIM microservice ============================================= Deploying and integrating NV-CLIP NIM microservice is straightforward and based on industry standard APIs. See the NV-CLIP NIM microservice documentation to get started. Applications ============ • Multimodal search: Enable accurate image and text search to quickly search database of images and videos. Multimodal search: Enable accurate image and text search to quickly search database of images and videos. • Zero-shot and few-shot inference: Classify images without re-training or fine-tuning. Zero-shot and few-shot inference: Classify images without re-training or fine-tuning. • Downstream vision tasks: Use the embeddings to enable downstream complex vision AI tasks such as segmentation, detection, VLMs and more. Downstream vision tasks: Use the embeddings to enable downstream complex vision AI tasks such as segmentation, detection, VLMs and more. References: =========== • Radford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021. Model Architecture: =================== Architecture Type:Transformer-Based Architecture Type: NV-CLIP as a backbone can be used towards various downstream tasks such as classification, detection, segmentation and text based image retrieval. Input: ====== Input Type(s):Images, TextsInput Format(s):List of Red, Green, Blue (RGB) Images or StringsOther Properties Related to Input: Input Type(s): Input Format(s): Other Properties Related to Input: Channel Ordering of the Input: NCHW, where N = Batch Size, C = number of channels (3), H = Height of images (224), W = Width of the images (224) Output: ======= Output Type(s):Float tensorOutput Format:3D TensorOther Properties Related to Output:The output of this model is an embedding of an input image or text of size 1024 for ViT-H variant. Output Type(s): Output Format: Other Properties Related to Output: Supported Operating System(s): • Linux Model Version(s): ================= • nv_clip_224_vit_h_trainable_v1.0- NV-CLIP ViT-H with 224 resolution is foundation model and is trainable. nv_clip_224_vit_h_trainable_v1.0 Using this Model ================ These models need to be used with NVIDIA hardware and software. These models can only be used with NV-CLIP NIM microservice. The primary use case for these models is getting feature embeddings from images and text. These embeddings can then be used for curation, clustering, zero-shot or few-shot downstream tasks such as classification. These embeddings can also be used towards text and image-based image Training Dataset: ================= Data Collection Method by dataset: • Automated Labeling Method by dataset: • Automated Properties: Evaluation Dataset: =================== Link:https://www.image-net.org/ Link: https://www.image-net.org/ Data Collection Method by dataset: • Unknown Labeling Method by dataset: • Unknown Properties:50,000 validation images fromImageNet dataset Properties: ImageNet dataset (https://www.image-net.org/download.php) Methodology and KPI ------------------- The performance of zero shot accuracy of NV-CLIP on ImageNet validation dataset. Ethical Considerations: ======================= Bias, Safety & Security, and Privacy ------------------------------------ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcardshere. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/nvclip_vit/bias) here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Special Training Data Considerations ------------------------------------ The model was trained on publicly available data, which may contain toxic language and societal biases. Therefore, the model may amplify those biases, such as associating certain genders with specific social stereotypes. Governing Terms --------------- The NIM container is governed by the NVIDIA AI Enterprise Software License Agreement | NVIDIA; and the use of this model is governed by the ai-foundation-models-community-license.pdf (nvidia.com). You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws."
  },
  {
    "name": "mistral-7b-instruct-v0.3",
    "nvidia_url": "https://build.nvidia.com/mistralai/mistral-7b-instruct-v03",
    "scraped_on": "2025-10-15T21:43:59.533702",
    "tags": [
      "chat",
      "language generation",
      "mistralai",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description: ============ Mistral-7B-Instruct-v0.3 is a language model that can follow instructions, complete requests, and generate creative text formats. It is an instruct version of the Mistral-7B-v0.3 generative text model fine-tuned using a variety of publicly available conversation datasets. This model is ready for non-commercial use. Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; seeMistral's 7B Instruct Hugging Face Model Card. Mistral's 7B Instruct Hugging Face Model Card (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) Terms of use ============ By using this software or model, you are agreeing to theterms and conditionsof the license, acceptable use policy and Mistral's privacy policy. Mistral-7B is released under the Apache 2.0 license terms and conditions (https://mistral.ai/terms-of-service/) References(s): ============== Mistral 7B Instruct v0.3Model Cardon Hugging FaceMistral 7BPaperMistral 7BBlogpost Model Card (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) Paper (https://arxiv.org/abs/2310.06825) Blogpost (https://mistral.ai/news/announcing-mistral-7b/) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Mistral-7BModel Version:0.3 Architecture Type: Network Architecture: Model Version: Input • Input Type: Text • Input Format: String • Input Parameters: max_tokens, temperature, top_p, stop, frequency_penalty, presence_penalty, seed Output • Output Type: Text • Output Format: String Software Integration: ===================== • Supported Hardware Platform(s): NVIDIA Hopper • Preferred Operating System(s): Linux Inference ========= Engine:TensorRT-LLMTest Hardware:H100 Engine: Test Hardware:"
  },
  {
    "name": "llama3-chatqa-1.5-8b",
    "nvidia_url": "https://build.nvidia.com/nvidia/chatqa-1-5-8b",
    "scraped_on": "2025-10-15T21:43:59.090763",
    "tags": [
      "chat",
      "non-commercial use only",
      "nvidia",
      "text-to-text"
    ],
    "model_card": "Llama3-ChatQA-1.5-8B Model card =============================== Model Information ================= Model Summary ------------- Author:NVIDIA Author: Description ----------- Llama3-ChatQA-1.5 excels at conversational question answering (QA) and retrieval-augmented generation (RAG). Llama3-ChatQA-1.5 is developed using an improved training recipe from ChatQA paper, and it is built on top of Llama-3 base model. Specifically, we incorporate more conversational QA data to enhance its tabular and arithmetic calculation capability. Llama3-ChatQA-1.5 has two variants: Llama3-ChatQA-1.5-8B and Llama3-ChatQA-1.5-70B. Terms of Use ------------ By accessing this model, you are agreeing to theNVIDIA AI Foundation Models Community LicenseAdditional Information:META LLAMA 3 COMMUNITY LICENSE AGREEMENT. NVIDIA AI Foundation Models Community License (https://developer.download.nvidia.com/ai-foundation-models/nvidia-ai-foundation-models-license-10Nov2023.pdf) META LLAMA 3 COMMUNITY LICENSE AGREEMENT (https://llama.meta.com/llama3/license/) Reference: ---------- Resources and Technical Documentation ------------------------------------- • ChatQA Project Page ChatQA Project Page (https://chatqa-project.github.io/) • Technical Report Technical Report (https://arxiv.org/pdf/2401.10225) Model Architecture: ------------------- Architecture Type:Transformer Decoder NetworkNetwork Architecture:Llama-3 Architecture Type: Network Architecture: Inputs and outputs ------------------ Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:One-Dimensional (1D) Input Type(s): Input Format(s): Input Parameters: Output: ======= Output Type(s):TextOutput Format(s):StringOutput Parameters:One-Dimensional (1D) Output Type(s): Output Format(s): Output Parameters: Ethical Considerations (For NVIDIA Models Only): ================================================ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards [Insert Link to Model Card++ here]. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "gemma-2-9b-it",
    "nvidia_url": "https://build.nvidia.com/google/gemma-2-9b-it",
    "scraped_on": "2025-10-15T21:43:55.875278",
    "tags": [
      "chat",
      "code generation",
      "google",
      "language generation",
      "text-to-text"
    ],
    "model_card": "Gemma 2 Model Card ================== Description ----------- Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. References: =========== Author: GoogleModel Page:Gemma Author Model Page Gemma (https://ai.google.dev/gemma/docs) Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. Terms of Use ============ Terms Terms (https://ai.google.dev/gemma/terms) Prohibited uses of Gemma models are outlined in theGemma Prohibited Use Policy. Gemma Prohibited Use Policy (https://ai.google.dev/gemma/prohibited_use_policy) Model Information ================= Summary description and brief definition of inputs and outputs. Inputs and outputs ------------------ Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:One-Dimensional (1D)Other Properties Related to Output:Text can be question, a prompt, or a document to be summarized. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Output: Output: ======= Output Type(s):TextOutput Format(s):StringOutput Parameters:One-Dimensional (1D)Other Properties Related to Output:Generated English-language text in response to the input (e.g., an answer to the question, a summary of the document). Output Type(s): Output Format(s): Output Parameters: Other Properties Related to Output: Citation -------- Usage and Limitations ===================== These models have certain limitations that users should be aware of. Intended Usage -------------- Open Large Language Models (LLMs) have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development. • Content Creation and CommunicationText Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts.Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications.Text Summarization: Generate concise summaries of a text corpus, research papers, or reports. • Text Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts. • Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications. • Text Summarization: Generate concise summaries of a text corpus, research papers, or reports. • Research and EducationNatural Language Processing (NLP) Research: These models can serve as a foundation for researchers to experiment with NLP techniques, develop algorithms, and contribute to the advancement of the field.Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice.Knowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics. • Natural Language Processing (NLP) Research: These models can serve as a foundation for researchers to experiment with NLP techniques, develop algorithms, and contribute to the advancement of the field. • Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice. • Knowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics. Limitations ----------- • Training DataThe quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses.The scope of the training dataset determines the subject areas the model can handle effectively. • The quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses. • The scope of the training dataset determines the subject areas the model can handle effectively. • Context and Task ComplexityLLMs are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging.A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • LLMs are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging. • A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • Language Ambiguity and NuanceNatural language is inherently complex. LLMs might struggle to grasp subtle nuances, sarcasm, or figurative language. • Natural language is inherently complex. LLMs might struggle to grasp subtle nuances, sarcasm, or figurative language. • Factual AccuracyLLMs generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • LLMs generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • Common SenseLLMs rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. • LLMs rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. Model Data ========== Data used for model training and how the data was processed. Training Dataset ---------------- These models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 13t tokens and the 9B model was trained with 8t tokens. Here are the key components: • Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content. • Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions. • Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries. The combination of these diverse data sources is crucial for training a powerful language model that can handle a wide variety of different tasks and text formats. Data Preprocessing ------------------ Here are the key data cleaning and filtering methods applied to the training data: • CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content. • Sensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets. • Additional methods: Filtering based on content quality and safety in line withour policies. our policies (https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11) Implementation Information ========================== TensorRT-LLM ------------ The endpoint available on NGC catalog is accelerated by TensorRT-LLM, an open-source library for optimizing inference performance. Gemma is compatible across NVIDIA AI platforms—from the datacenter, cloud, to the local PC with RTX GPU systems. Gemma models use a vocabulary size of 256K and support a context length of up to 4K while using rotary positional embedding (RoPE). With support for Position Interpolation (PI) available in TensorRT-LLM, Gemma models using RoPE can support longer output sequence lengths at inference time while retaining original model architecture. Software -------- Training was done usingJAXandML Pathways. JAX (https://github.com/google/jax) ML Pathways (https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/) JAX allows researchers to take advantage of the latest generation of hardware, including TPUs, for faster and more efficient training of large models. ML Pathways is Google's latest effort to build artificially intelligent systems capable of generalizing across multiple tasks. This is specially suitable forfoundation models, including large language models like these ones. foundation models (https://ai.google/discover/foundation-models/) Together, JAX and ML Pathways are used as described in thepaper about the Gemini family of models; \"the 'single controller' programming model of Jax and Pathways allows a single Python process to orchestrate the entire training run, dramatically simplifying the development workflow.\" paper about the Gemini family of models (https://goo.gle/gemma2report) Evaluation ========== Model evaluation metrics and results. Benchmark Results ----------------- These models were evaluated against a large collection of different datasets and metrics to cover different aspects of text generation: MMLU (https://arxiv.org/abs/2009.03300) HellaSwag (https://arxiv.org/abs/1905.07830) PIQA (https://arxiv.org/abs/1911.11641) SocialIQA (https://arxiv.org/abs/1904.09728) BoolQ (https://arxiv.org/abs/1905.10044) WinoGrande (https://arxiv.org/abs/1907.10641) ARC-e (https://arxiv.org/abs/1911.01547) ARC-c (https://arxiv.org/abs/1911.01547) TriviaQA (https://arxiv.org/abs/1705.03551) Natural Questions (https://github.com/google-research-datasets/natural-questions) HumanEval (https://arxiv.org/abs/2107.03374) MBPP (https://arxiv.org/abs/2108.07732) GSM8K (https://arxiv.org/abs/2110.14168) MATH (https://arxiv.org/abs/2103.03874) AGIEval (https://arxiv.org/abs/2304.06364) BIG-Bench (https://arxiv.org/abs/2206.04615) Ethics and Safety ================= Ethics and safety evaluation approach and results. Evaluation Approach ------------------- Our evaluation methods include structured evaluations and internal red-teaming testing of relevant content policies. Red-teaming was conducted by a number of different teams, each with different goals and human evaluation metrics. These models were evaluated against a number of different categories relevant to ethics and safety, including: • Text-to-Text Content Safety: Human evaluation on prompts covering safety policies including child sexual abuse and exploitation, harassment, violence and gore, and hate speech. • Text-to-Text Representational Harms: Benchmark against relevant academic datasets such asWinoBiasandBBQ Dataset. WinoBias (https://arxiv.org/abs/1804.06876) BBQ Dataset (https://arxiv.org/abs/2110.08193v2) • Memorization: Automated evaluation of memorization of training data, including the risk of personally identifiable information exposure. • Large-scale harm: Tests for \"dangerous capabilities,\" such as chemical, biological, radiological, and nuclear (CBRN) risks. Evaluation Results ------------------ The results of ethics and safety evaluations are within acceptable thresholds for meetinginternal policiesfor categories such as child safety, content safety, representational harms, memorization, large-scale harms. On top of robust internal evaluations, the results of well-known safety benchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA are shown here. internal policies (https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11) Gemma 2.0 --------- RealToxicity (https://arxiv.org/abs/2009.11462) CrowS-Pairs (https://aclanthology.org/2020.emnlp-main.154/) BBQ Ambig (https://arxiv.org/abs/2110.08193v2) BBQ Disambig (https://arxiv.org/abs/2110.08193v2) Winogender (https://arxiv.org/abs/1804.09301) TruthfulQA (https://arxiv.org/abs/2109.07958) Winobias 1_2 (https://arxiv.org/abs/1804.06876) Winobias 2_2 (https://arxiv.org/abs/1804.06876) Toxigen (https://arxiv.org/abs/2203.09509) Ethical Considerations and Risks -------------------------------- The development of large language models (LLMs) raises several ethical concerns. In creating an open model, we have carefully considered the following: • Bias and FairnessLLMs trained on large-scale, real-world text data can reflect socio-cultural biases embedded in the training material. These models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card. • LLMs trained on large-scale, real-world text data can reflect socio-cultural biases embedded in the training material. These models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card. • Misinformation and MisuseLLMs can be misused to generate text that is false, misleading, or harmful.Guidelines are provided for responsible use with the model, see theResponsible Generative AI Toolkit. • LLMs can be misused to generate text that is false, misleading, or harmful. • Guidelines are provided for responsible use with the model, see theResponsible Generative AI Toolkit. Responsible Generative AI Toolkit (https://ai.google.dev/responsible) • Transparency and Accountability:This model card summarizes details on the models' architecture, capabilities, limitations, and evaluation processes.A responsibly developed open model offers the opportunity to share innovation by making LLM technology accessible to developers and researchers across the AI ecosystem. • This model card summarizes details on the models' architecture, capabilities, limitations, and evaluation processes. • A responsibly developed open model offers the opportunity to share innovation by making LLM technology accessible to developers and researchers across the AI ecosystem. Risks identified and mitigations: • Perpetuation of biases: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases. • Generation of harmful content: Mechanisms and guidelines for content safety are essential. Developers are encouraged to exercise caution and implement appropriate content safety safeguards based on their specific product policies and application use cases. • Misuse for malicious purposes: Technical limitations and developer and end-user education can help mitigate against malicious applications of LLMs. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in theGemma Prohibited Use Policy. Gemma Prohibited Use Policy (https://ai.google.dev/gemma/prohibited_use_policy) • Privacy violations: Models were trained on data filtered for removal of PII (Personally Identifiable Information). Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques. Benefits -------- At the time of release, this family of models provides high-performance open large language model implementations designed from the ground up for Responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown to provide superior performance to other, comparably-sized open model alternatives."
  },
  {
    "name": "gemma-2-27b-it",
    "nvidia_url": "https://build.nvidia.com/google/gemma-2-27b-it",
    "scraped_on": "2025-10-15T21:43:52.593570",
    "tags": [
      "chat",
      "code generation",
      "google",
      "language generation",
      "text-to-text"
    ],
    "model_card": "Gemma 2 Model Card ================== Description ----------- Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. References: =========== Author: GoogleModel Page:Gemma Author Model Page Gemma (https://ai.google.dev/gemma/docs) Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. Terms of Use ============ Terms Terms (https://ai.google.dev/gemma/terms) Prohibited uses of Gemma models are outlined in theGemma Prohibited Use Policy. Gemma Prohibited Use Policy (https://ai.google.dev/gemma/prohibited_use_policy) Model Information ================= Summary description and brief definition of inputs and outputs. Inputs and outputs ------------------ Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:One-Dimensional (1D)Other Properties Related to Output:Text can be question, a prompt, or a document to be summarized. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Output: Output: ======= Output Type(s):TextOutput Format(s):StringOutput Parameters:One-Dimensional (1D)Other Properties Related to Output:Generated English-language text in response to the input (e.g., an answer to the question, a summary of the document). Output Type(s): Output Format(s): Output Parameters: Other Properties Related to Output: Citation -------- Usage and Limitations ===================== These models have certain limitations that users should be aware of. Intended Usage -------------- Open Large Language Models (LLMs) have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development. • Content Creation and CommunicationText Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts.Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications.Text Summarization: Generate concise summaries of a text corpus, research papers, or reports. • Text Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts. • Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications. • Text Summarization: Generate concise summaries of a text corpus, research papers, or reports. • Research and EducationNatural Language Processing (NLP) Research: These models can serve as a foundation for researchers to experiment with NLP techniques, develop algorithms, and contribute to the advancement of the field.Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice.Knowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics. • Natural Language Processing (NLP) Research: These models can serve as a foundation for researchers to experiment with NLP techniques, develop algorithms, and contribute to the advancement of the field. • Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice. • Knowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics. Limitations ----------- • Training DataThe quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses.The scope of the training dataset determines the subject areas the model can handle effectively. • The quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses. • The scope of the training dataset determines the subject areas the model can handle effectively. • Context and Task ComplexityLLMs are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging.A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • LLMs are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging. • A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • Language Ambiguity and NuanceNatural language is inherently complex. LLMs might struggle to grasp subtle nuances, sarcasm, or figurative language. • Natural language is inherently complex. LLMs might struggle to grasp subtle nuances, sarcasm, or figurative language. • Factual AccuracyLLMs generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • LLMs generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • Common SenseLLMs rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. • LLMs rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. Model Data ========== Data used for model training and how the data was processed. Training Dataset ---------------- These models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 13t tokens and the 9B model was trained with 8t tokens. Here are the key components: • Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content. • Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions. • Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries. The combination of these diverse data sources is crucial for training a powerful language model that can handle a wide variety of different tasks and text formats. Data Preprocessing ------------------ Here are the key data cleaning and filtering methods applied to the training data: • CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content. • Sensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets. • Additional methods: Filtering based on content quality and safety in line withour policies. our policies (https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11) Implementation Information ========================== TensorRT-LLM ------------ The endpoint available on NGC catalog is accelerated by TensorRT-LLM, an open-source library for optimizing inference performance. Gemma is compatible across NVIDIA AI platforms—from the datacenter, cloud, to the local PC with RTX GPU systems. Gemma models use a vocabulary size of 256K and support a context length of up to 4K while using rotary positional embedding (RoPE). With support for Position Interpolation (PI) available in TensorRT-LLM, Gemma models using RoPE can support longer output sequence lengths at inference time while retaining original model architecture. Software -------- Training was done usingJAXandML Pathways. JAX (https://github.com/google/jax) ML Pathways (https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/) JAX allows researchers to take advantage of the latest generation of hardware, including TPUs, for faster and more efficient training of large models. ML Pathways is Google's latest effort to build artificially intelligent systems capable of generalizing across multiple tasks. This is specially suitable forfoundation models, including large language models like these ones. foundation models (https://ai.google/discover/foundation-models/) Together, JAX and ML Pathways are used as described in thepaper about the Gemini family of models; \"the 'single controller' programming model of Jax and Pathways allows a single Python process to orchestrate the entire training run, dramatically simplifying the development workflow.\" paper about the Gemini family of models (https://goo.gle/gemma2report) Evaluation ========== Model evaluation metrics and results. Benchmark Results ----------------- These models were evaluated against a large collection of different datasets and metrics to cover different aspects of text generation: MMLU (https://arxiv.org/abs/2009.03300) HellaSwag (https://arxiv.org/abs/1905.07830) PIQA (https://arxiv.org/abs/1911.11641) SocialIQA (https://arxiv.org/abs/1904.09728) BoolQ (https://arxiv.org/abs/1905.10044) WinoGrande (https://arxiv.org/abs/1907.10641) ARC-e (https://arxiv.org/abs/1911.01547) ARC-c (https://arxiv.org/abs/1911.01547) TriviaQA (https://arxiv.org/abs/1705.03551) Natural Questions (https://github.com/google-research-datasets/natural-questions) HumanEval (https://arxiv.org/abs/2107.03374) MBPP (https://arxiv.org/abs/2108.07732) GSM8K (https://arxiv.org/abs/2110.14168) MATH (https://arxiv.org/abs/2103.03874) AGIEval (https://arxiv.org/abs/2304.06364) BIG-Bench (https://arxiv.org/abs/2206.04615) Ethics and Safety ================= Ethics and safety evaluation approach and results. Evaluation Approach ------------------- Our evaluation methods include structured evaluations and internal red-teaming testing of relevant content policies. Red-teaming was conducted by a number of different teams, each with different goals and human evaluation metrics. These models were evaluated against a number of different categories relevant to ethics and safety, including: • Text-to-Text Content Safety: Human evaluation on prompts covering safety policies including child sexual abuse and exploitation, harassment, violence and gore, and hate speech. • Text-to-Text Representational Harms: Benchmark against relevant academic datasets such asWinoBiasandBBQ Dataset. WinoBias (https://arxiv.org/abs/1804.06876) BBQ Dataset (https://arxiv.org/abs/2110.08193v2) • Memorization: Automated evaluation of memorization of training data, including the risk of personally identifiable information exposure. • Large-scale harm: Tests for \"dangerous capabilities,\" such as chemical, biological, radiological, and nuclear (CBRN) risks. Evaluation Results ------------------ The results of ethics and safety evaluations are within acceptable thresholds for meetinginternal policiesfor categories such as child safety, content safety, representational harms, memorization, large-scale harms. On top of robust internal evaluations, the results of well-known safety benchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA are shown here. internal policies (https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11) Gemma 2.0 --------- RealToxicity (https://arxiv.org/abs/2009.11462) CrowS-Pairs (https://aclanthology.org/2020.emnlp-main.154/) BBQ Ambig (https://arxiv.org/abs/2110.08193v2) BBQ Disambig (https://arxiv.org/abs/2110.08193v2) Winogender (https://arxiv.org/abs/1804.09301) TruthfulQA (https://arxiv.org/abs/2109.07958) Winobias 1_2 (https://arxiv.org/abs/1804.06876) Winobias 2_2 (https://arxiv.org/abs/1804.06876) Toxigen (https://arxiv.org/abs/2203.09509) Ethical Considerations and Risks -------------------------------- The development of large language models (LLMs) raises several ethical concerns. In creating an open model, we have carefully considered the following: • Bias and FairnessLLMs trained on large-scale, real-world text data can reflect socio-cultural biases embedded in the training material. These models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card. • LLMs trained on large-scale, real-world text data can reflect socio-cultural biases embedded in the training material. These models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card. • Misinformation and MisuseLLMs can be misused to generate text that is false, misleading, or harmful.Guidelines are provided for responsible use with the model, see theResponsible Generative AI Toolkit. • LLMs can be misused to generate text that is false, misleading, or harmful. • Guidelines are provided for responsible use with the model, see theResponsible Generative AI Toolkit. Responsible Generative AI Toolkit (https://ai.google.dev/responsible) • Transparency and Accountability:This model card summarizes details on the models' architecture, capabilities, limitations, and evaluation processes.A responsibly developed open model offers the opportunity to share innovation by making LLM technology accessible to developers and researchers across the AI ecosystem. • This model card summarizes details on the models' architecture, capabilities, limitations, and evaluation processes. • A responsibly developed open model offers the opportunity to share innovation by making LLM technology accessible to developers and researchers across the AI ecosystem. Risks identified and mitigations: • Perpetuation of biases: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases. • Generation of harmful content: Mechanisms and guidelines for content safety are essential. Developers are encouraged to exercise caution and implement appropriate content safety safeguards based on their specific product policies and application use cases. • Misuse for malicious purposes: Technical limitations and developer and end-user education can help mitigate against malicious applications of LLMs. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in theGemma Prohibited Use Policy. Gemma Prohibited Use Policy (https://ai.google.dev/gemma/prohibited_use_policy) • Privacy violations: Models were trained on data filtered for removal of PII (Personally Identifiable Information). Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques. Benefits -------- At the time of release, this family of models provides high-performance open large language model implementations designed from the ground up for Responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown to provide superior performance to other, comparably-sized open model alternatives."
  },
  {
    "name": "starcoder2-7b",
    "nvidia_url": "https://build.nvidia.com/bigcode/starcoder2-7b",
    "scraped_on": "2025-10-15T21:43:52.172088",
    "tags": [
      "bigcode",
      "code completion",
      "code generation"
    ],
    "model_card": "StarCoder2 Model Overview ========================= Description: ============ StarCoder2-7b is a state-of-the-art language model with 7 billion parameters, trained on 17 programming languages using The Stack v2 dataset. It employs advanced techniques like Grouped Query Attention and sliding window attention to enhance its performance on coding tasks. The model is optimized to handle a context window of 16,384 tokens and was trained using the Fill-in-the-Middle objective on 3.5+ trillion tokens. Terms of Use ============ GOVERNING TERMS: Your use of this model is governed by theBigCode OpenRAIL-M v1 License Agreement. BigCode OpenRAIL-M v1 License Agreement (https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) References(s): ============== StarCoder2-7b on Hugging Face StarCoder2-7b on Hugging Face (https://huggingface.co/bigcode/starcoder2-7b) Model Architecture: =================== Architecture Type:Transformer decoderNetwork Architecture:Grouped Query Attention, sliding window attentionModel Version:2.0 Architecture Type: Network Architecture: Model Version: Input: ====== Input Format:TextInput Parameters:Temperature, Top P, Max Output Tokens Input Format: Input Parameters: Output: ======= Output Format:TextOutput Parameters:None Output Format: Output Parameters: Software Integration: ===================== Supported Hardware Platform(s):NVIDIA L4 GPUsSupported Operating System(s):Linux Supported Hardware Platform(s): Supported Operating System(s): Inference: ========== Engine:Triton Inference ServerTest Hardware:NVIDIA L4 systems Engine: Test Hardware:"
  },
  {
    "name": "phi-3-medium-128k-instruct",
    "nvidia_url": "https://build.nvidia.com/microsoft/phi-3-medium-128k-instruct",
    "scraped_on": "2025-10-15T21:43:48.890336",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "large language models",
      "microsoft",
      "text-to-text"
    ],
    "model_card": "Phi-3 Medium-128K-Instruct Model Card Model Summary ============= MIT (https://opensource.org/license/mit) Intended Use ============ Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. Data Overview ============= Training datasets ================= Our training data includes a wide variety of sources, totaling 4.8 trillion tokens (including 10% multilingual), and is a combination of 1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; 2) newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); 3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. Benchmark datasets ================== We evaluated the model across a breadth of public and internal benchmarks to understand the model capabilities in the most comprehensive way under multiple tasks and conditions. More specifically, • Reasoning:Winogrande: commonsense reasoning around pronoun resolutionPIQA: physical commonsense reasoning around everyday situationsARC-easy, ARC-challenge: grade-school multiple choice science questions at easy and challenge levelCommonsensQA: generic commonsense questionsMedQA: medical questions answeringSocial IQA: social commonsense intelligenceBoolQ: natural questions from contextTruthfulQA: grounded reasoning • Winogrande: commonsense reasoning around pronoun resolution • PIQA: physical commonsense reasoning around everyday situations • ARC-easy, ARC-challenge: grade-school multiple choice science questions at easy and challenge level • CommonsensQA: generic commonsense questions • MedQA: medical questions answering • Social IQA: social commonsense intelligence • BoolQ: natural questions from context • TruthfulQA: grounded reasoning • Language understanding:HellaSwag: commonsense natural language inference around everyday eventsANLI: adversarial natural language inferenceLAMBADA: word prediction given a passage. • HellaSwag: commonsense natural language inference around everyday events • ANLI: adversarial natural language inference • LAMBADA: word prediction given a passage. • World knowledge:Natural Questions: question about Wikipedia knowledgeTriviaQA: trivia question on general topics • Natural Questions: question about Wikipedia knowledge • TriviaQA: trivia question on general topics • Math:GSM8K: grade-school math word problemsGSM8K Hard: grade-school math word problems with large values and some absurdity.MATH: challenging competition math problems • GSM8K: grade-school math word problems • GSM8K Hard: grade-school math word problems with large values and some absurdity. • MATH: challenging competition math problems • Code:HumanEval, MBPP: python coding tasksSpider: SQL query tasks • HumanEval, MBPP: python coding tasks • Spider: SQL query tasks • Multilingual:MGSM: multilingual grade-school mathMEGA: multilingual NLP tasks • MGSM: multilingual grade-school math • MEGA: multilingual NLP tasks • Popular aggregated datasets: MMLU, BigBench-Hard, AGI Eval • Long context:GovReport, QMSum, SQuALITY, SummScreenFD: long context summarizationQasper: long context question answer • GovReport, QMSum, SQuALITY, SummScreenFD: long context summarization • Qasper: long context question answer • Multi-turn conversations:Data generated by In-house adversarial conversation simulation tool • Data generated by In-house adversarial conversation simulation tool • Single-turn trustworthiness evaluation:DecodingTrust: a collection of trustworthiness benchmark in eight different perspectivesXSTest: exaggerated safety evaluationToxigen: adversarial and hate speech detection • DecodingTrust: a collection of trustworthiness benchmark in eight different perspectives • XSTest: exaggerated safety evaluation • Toxigen: adversarial and hate speech detection • Red Team:Responses to prompts provided by AI Red Team at Microsoft • Responses to prompts provided by AI Red Team at Microsoft Safety ====== Approach ======== Phi-3 family of models has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated datasets. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and a modified version of RLHF (Reinforcement Learning from Human Feedback) by utilizing human-labeled and synthetic datasets, including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories. Safety Evaluation and Red-Teaming ================================= Prior to release, Phi-3 family of models followed a multi-faceted evaluation approach. Quantitative evaluation was conducted with multiple open-source safety benchmarks and in-house tools utilizing adversarial conversation simulation. For qualitative safety evaluation, we collaborated with the AI Red Team at Microsoft to assess safety risks posed by Phi-3-Medium in both average and adversarial user scenarios. The assessment was done in predetermined eight risk categories with automated scoring followed by thorough manual reviews of the model responses. ** Please refer to the technical report for more details of safety alignment Model Quality ============= To understand the capabilities, we compare Phi-3 Medium with a set of models over a variety of benchmarks using Microsoft's internal benchmark platform BabelBench (See Appendix A for benchmark methodology). At the high-level overview of the model quality on representative benchmarks: 74.7 72.3 74.1 80.7 72.7 73.2 83.8 We take a closer look at different categories across 80 public benchmark datasets at the table below: Overall, the Phi-3 Medium-128K-Instruct with only 14B-param achieves a similar level of language understanding, code, and math as much larger models. Moreover, the model outperforms bigger models in reasoning capability and only behind GPT-4-Turbo. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, we believe such weakness can be resolved by augmenting Phi-3-Medium with a search engine. Long Context ============ Phi-3 Medium-128K-Instruct supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA. We see with just 14B params the Phi-3 Medium outperforms models with the same parameters size, and competitive with models on a much bigger size such as Mixtral 8x22B. 22.3 11.2 21.5 16.5 25.7 27.4 Usage ===== Input formats ------------- Given the nature of the training data, the Phi-3 Medium-128K-Instruct model is best suited for prompts using the chat format as follows: <|user|>How to explain Internet for a medieval knight?<|end|><|assistant|> Loading the model locally ------------------------- After obtaining the Phi-3 Medium-128K-Instruct model checkpoints, users can use this sample code for inference. Cross Platform Support ====================== ONNX runtimenow supports Phi3 small models across platforms and hardware. ONNX runtime (http://aka.ms/phi3-optimizations) Optimized phi-3 models are also published here in ONNX format, to run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the precision best suited to each of these targets. DirectML GPU acceleration is supported for Windows desktops GPUs (AMD, Intel, and NVIDIA). Along with DML, ONNX Runtime provides cross platform support for Phi3 mini across a range of devices CPU, GPU, and mobile. Here are some of the optimized configurations we have added: • ONNX models for int4 DML: Quantized to int4 via AWQ • ONNX model for fp16 CUDA • ONNX model for int4 CUDA: Quantized to int4 via RTN • ONNX model for int4 CPU and Mobile: Quantized to int4 via RTN Responsible AI Considerations ============================= Like other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: • Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. • Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. • Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. • Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. • Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses. Developers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: • Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. • High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. • Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). • Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. • Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Appendix A: Benchmark Methodology ================================= We include a brief word on methodology here - and in particular, how we think about optimizing prompts. In an ideal world, we wouldnever change any promptsin our benchmarks to ensure it is always an apples-to-apples comparison when comparing different models. Indeed, this is our default approach, and is the case in the vast majority of models we have run to date. never change any prompts There are, however, some exceptions to this. In some cases, we see a model that performs worse than expected on a given evaldue to a failure to respect the output format. For example: due to a failure to respect the output format • A Claude model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with “Sure, I can help with that. …” which may break the parser. In such cases, we have opted to try differentsystem messages(e.g. “You must always respond to a question” or “Get to the point!”). system messages • With LLaMA-1 models, we observed that few shots actually hurt model performance. In this case we did allow running the benchmarks with 0-shots for all cases. • We have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, we do allow for model-specific mappings for chat to completion prompts. However,we do not: we do not • Pick different few-shot examples. Few shots will always be the same when comparing different models. • Change prompt format: e.g. if it is an A/B/C/D multiple choice, we do not tweak this to 1/2/3/4 multiple choice."
  },
  {
    "name": "maisi",
    "nvidia_url": "https://build.nvidia.com/nvidia/maisi",
    "scraped_on": "2025-10-15T21:43:48.448655",
    "tags": [
      "image generation",
      "medical imaging",
      "nvidia",
      "nvidia nim"
    ],
    "model_card": "Model Overview ============== Description: ============ NVIDIA MAISI (Medical AI for Synthetic Imaging) is a state-of-the-art three-dimensional (3D) Latent Diffusion Model designed for generating high-quality synthetic CT images with or without anatomical annotations. This AI model excels in data augmentation and creating realistic medical imaging data to supplement limited datasets due to privacy concerns or rare conditions. It can also significantly enhance the performance of other medical imaging AI models by generating diverse and realistic training data. MAISI offers several key features: • Generates high-resolution 3D CT images up to 512 × 512 × 768 voxels • Supports variable voxel sizes ranging from 0.5mm to 5.0mm • Capable of annotating up to 127 anatomical classes, including organs and tumors • Allows controllable anatomy size for 10 specific classes • Produces paired segmentation masks By providing these capabilities, MAISI is a valuable tool for researchers advancing AI applications in healthcare. However, it is important to note that this model is intended for research purposes only and not for clinical usage. Terms of Use ============ By using this model, you are agreeing to theterms and conditionsof the license. terms and conditions (https://docs.nvidia.com/ai-foundation-models-community-license.pdf) References: =========== [1]Guo, P., Zhao, C., Yang, D., Xu, Z., Nath, V., Tang, Y., Simon, B., Belue, M., Harmon, S., Turkbey, B. and Xu, D., 2024. MAISI: Medical AI for Synthetic Imaging. arXiv preprint arXiv:2409.11169.https://arxiv.org/abs/2409.11169[2] Rombach, Robin, et al. \"High-resolution image synthesis with latent diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf[3] Lvmin Zhang, Anyi Rao, Maneesh Agrawala; “Adding Conditional Control to Text-to-Image Diffusion Models.” Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 3836-3847.https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf https://arxiv.org/abs/2409.11169 https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf Model Architecture: =================== Architecture Type:Convolutional Neural Network (CNN)Network Architecture:ControlNet + 3D UNet + attention blocks Architecture Type: Network Architecture: Input: ====== num_output_samples ------------------ Input Type:IntegerInput Format:Single integer valueInput Parameters:Required input indicates the number of synthetic images the model will generate. Input Type: Input Format: Input Parameters: body_region ----------- Input Type:ListInput Format:Array of StringsInput Parameters:Required input indicates the region of body the generated CT will focus on Input Type: Input Format: Input Parameters: • Options: [\"head\", \"chest\", \"thorax\", \"abdomen\", \"pelvis\", \"lower\"] anatomy_list ------------ Input Type:ListInput Format:Array of StringsInput Parameters:Optional list of 127 anatomical classes (listed in the Additional Information section) Input Type: Input Format: Input Parameters: output_size ----------- Input Type:ListInput Format:Array of 3 IntegersInput Parameters:Optional list of 3 numbers that indicate the x, y, and z size of the CT image. Input Type: Input Format: Input Parameters: • x- and y-axes: 128, 256, 384, 512 • z-axis: 128, 256, 384, 512, 640, 768 spacing ------- Input Type:ListInput Format:Array of FloatsInput Parameters:Optional list of floats that indicate the spacing of the CT image Input Type: Input Format: Input Parameters: • Each element must be in the range: 0.5 to 5.0 controllable_anatomy_size ------------------------- Input Type:ListInput Format:Array of Tuples (String, Float)Input Parameters:Optional list of tuples for up to 10 different anatomies. Each tuple consists of an (organ_name, size_value) pair, playground has limited numbers of anatomies, the model can support full 127 anatomies. Input Type: Input Format: Input Parameters: • organ_name options: [\"liver\", \"gallbladder\", \"stomach\", \"pancreas\", \"colon\", \"lung tumor\", \"bone lesion\", \"hepatic tumor\", \"colon cancer primaries\", \"pancreatic tumor\"] • size_value range: 0.0 to 1.0, or -1 (means not exist/delete this organ) Output: ======= Output Type(s):Image(s)Output Format:(Neuroimaging Informatics Technology Initiative) NIfTI, (Digital Imaging and Communications in Medicine) DICOM, and (Nearly Raw Raster Data) NrrdOutput Parameters:Three-Dimensional (3D)Output Description:Synthetic CT image with dimensions up to 512x512x768 and spacing between 0.5mm and 5.0mm, reflecting controllable anatomy sizes as specified. If requested in input parameters, an additional NIfTI file containing the corresponding label map for the anatomy_list is also provided. Output Type(s): Output Format: Output Parameters: Output Description: Software Integration: ===================== Runtime Engine(s):MONAI Core v.1.3.2 Runtime Engine(s): Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper [Preferred/Supported] Operating System(s): • Linux Model Version(s): ================= 0.3.1 Inference: ========== Engine:PyTorchTest Hardware:A100 with at least 80GB memory for 512x512x512 imagesH100 with at least 80GB memory for 512x512x512 images Engine: Test Hardware: Additional Information: ======================= The current list of classes available within MAISI:\"liver\": 1,\"spleen\": 3,\"pancreas\": 4,\"right kidney\": 5,\"aorta\": 6,\"inferior vena cava\": 7,\"right adrenal gland\": 8,\"left adrenal gland\": 9,\"gallbladder\": 10,\"esophagus\": 11,\"stomach\": 12,\"duodenum\": 13,\"left kidney\": 14,\"bladder\": 15,\"portal vein and splenic vein\": 17,\"small bowel\": 19,\"brain\": 22,\"lung tumor\": 23,\"pancreatic tumor\": 24,\"hepatic vessel\": 25,\"hepatic tumor\": 26,\"colon cancer primaries\": 27,\"left lung upper lobe\": 28,\"left lung lower lobe\": 29,\"right lung upper lobe\": 30,\"right lung middle lobe\": 31,\"right lung lower lobe\": 32,\"vertebrae L5\": 33,\"vertebrae L4\": 34,\"vertebrae L3\": 35,\"vertebrae L2\": 36,\"vertebrae L1\": 37,\"vertebrae T12\": 38,\"vertebrae T11\": 39,\"vertebrae T10\": 40,\"vertebrae T9\": 41,\"vertebrae T8\": 42,\"vertebrae T7\": 43,\"vertebrae T6\": 44,\"vertebrae T5\": 45,\"vertebrae T4\": 46,\"vertebrae T3\": 47,\"vertebrae T2\": 48,\"vertebrae T1\": 49,\"vertebrae C7\": 50,\"vertebrae C6\": 51,\"vertebrae C5\": 52,\"vertebrae C4\": 53,\"vertebrae C3\": 54,\"vertebrae C2\": 55,\"vertebrae C1\": 56,\"trachea\": 57,\"left iliac artery\": 58,\"right iliac artery\": 59,\"left iliac vena\": 60,\"right iliac vena\": 61,\"colon\": 62,\"left rib 1\": 63,\"left rib 2\": 64,\"left rib 3\": 65,\"left rib 4\": 66,\"left rib 5\": 67,\"left rib 6\": 68,\"left rib 7\": 69,\"left rib 8\": 70,\"left rib 9\": 71,\"left rib 10\": 72,\"left rib 11\": 73,\"left rib 12\": 74,\"right rib 1\": 75,\"right rib 2\": 76,\"right rib 3\": 77,\"right rib 4\": 78,\"right rib 5\": 79,\"right rib 6\": 80,\"right rib 7\": 81,\"right rib 8\": 82,\"right rib 9\": 83,\"right rib 10\": 84,\"right rib 11\": 85,\"right rib 12\": 86,\"left humerus\": 87,\"right humerus\": 88,\"left scapula\": 89,\"right scapula\": 90,\"left clavicula\": 91,\"right clavicula\": 92,\"left femur\": 93,\"right femur\": 94,\"left hip\": 95,\"right hip\": 96,\"sacrum\": 97,\"left gluteus maximus\": 98,\"right gluteus maximus\": 99,\"left gluteus medius\": 100,\"right gluteus medius\": 101,\"left gluteus minimus\": 102,\"right gluteus minimus\": 103,\"left autochthon\": 104,\"right autochthon\": 105,\"left iliopsoas\": 106,\"right iliopsoas\": 107,\"left atrial appendage\": 108,\"brachiocephalic trunk\": 109,\"left brachiocephalic vein\": 110,\"right brachiocephalic vein\": 111,\"left common carotid artery\": 112,\"right common carotid artery\": 113,\"costal cartilages\": 114,\"heart\": 115,\"left kidney cyst\": 116,\"right kidney cyst\": 117,\"prostate\": 118,\"pulmonary vein\": 119,\"skull\": 120,\"spinal cord\": 121,\"sternum\": 122,\"left subclavian artery\": 123,\"right subclavian artery\": 124,\"superior vena cava\": 125,\"thyroid gland\": 126,\"vertebrae S1\": 127,\"bone lesion\": 128,\"airway\": 132 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nv-embedqa-mistral-7b-v2",
    "nvidia_url": "https://build.nvidia.com/nvidia/nv-embedqa-mistral-7b-v2",
    "scraped_on": "2025-10-15T21:43:48.093357",
    "tags": [
      "embedding",
      "nemo retriever",
      "nvidia",
      "retrieval augmented generation"
    ],
    "model_card": "Model Overview ============== Description ----------- The NVIDIA Retrieval QA Mistral 7B Embedding model is an embedding model optimized for text question-answering retrieval. An embedding model is a crucial component of a text retrieval system, as it transforms textual information into dense vector representations. They are typically transformer encoders that process tokens of input text (for example, question, passage) to output an embedding. This model is ready for commercial use. NVIDIA Retrieval QA Mistral 7B Embedding model is part of the NVIDIA NeMo Retriever, which provides state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can also readily customize them for their domain-specific use cases, such as Information Technology, Human Resource help assistants, and Research & Development research assistants. Intended use ------------ The NVIDIA Retrieval QA Mistral 7B Embedding model is most suitable for users who want to build a question and answer application over a large text corpus, leveraging the latest dense retrieval technologies. License/Terms of use -------------------- The use of this model is governed by theNVIDIA AI Foundation Models Community License Agreementand theApache License 2.0. NVIDIA AI Foundation Models Community License Agreement (https://developer.nvidia.com/downloads/nv-ai-foundation-models-license) Apache License 2.0 (https://choosealicense.com/licenses/apache-2.0/) Technology can have a profound impact on people and the world, and NVIDIA is committed to enabling trust and transparency in AI development. NVIDIA encourages users to adopt principles of AI ethics and trustworthiness to guide your business decisions by following the guidelines in the NVIDIA AI Foundation Models Community License Agreement. Model Architecture ------------------ Architecture Type:TransformerNetwork Architecture:Fine-tuned Mistral 7B foundation modelEmbedding Dimension:4096Parameter Count:7.1 billion Architecture Type: Network Architecture: Embedding Dimension: Parameter Count: The NVIDIA Retrieval QA Mistral 7B Embedding model is a transformer encoder - a fine-tuned version ofMistral 7B, with 32 layers and 4096 as embedding size, which is trained on public datasets. Mistral Models are pre-trained with casual attention. As our research demonstrated that bi-directional attention improved the performance, we converted the model to bi-directional attention. Embedding models for text retrieval are typically trained using a bi-encoder architecture. This involves encoding a pair of sentences (for example, query and chunked passages) independently using the embedding model. Contrastive learning is used to maximize the similarity between the query and the passage that contains the answer, while minimizing the similarity between the query and sampled negative passages not useful to answer the question. Mistral 7B (https://huggingface.co/mistralai/Mistral-7B-v0.1) Model Version(s) ---------------- NVIDIA Retrieval QA Mistral 7B Embedding v2 Short name: NV-EmbedQA-Mistral-7B-v2 Training Dataset & Evaluation ============================= Training Dataset ---------------- The development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset namedMS MARCOrestricts commercial licensing, limiting the use of these models in commercial settings. To address this, we created our own training dataset blend based on public QA datasets, which each has a license for commercial applications. The pretrainedMistral-7B-v0.1embedding model was fine-tuned with contrastive learning with the prefix of “query:” for questions and “passage:” for context passages, using a mixture of commercially-viable public datasets. MS MARCO (https://microsoft.github.io/msmarco/) Mistral-7B-v0.1 (https://huggingface.co/mistralai/Mistral-7B-v0.1) The training dataset details are as follows: Use Case:Information retrieval for question and answering over text documents.Data Sources:Public datasets licensed for commercial use.Language:English (US), potential support for other languages (in research)Volume:600k samples from public datasetsData Collection Method by dataset:UnknownLabeling Method by dataset:Unknown Use Case: Data Sources: Language: Volume: Data Collection Method by dataset: Labeling Method by dataset: Evaluation Results ------------------ We evaluated the NVIDIA Retrieval QA Mistral 7B Embedding model in comparison to literature open & commercial retriever models on academic benchmarks for question-answering -NQ,HotpotQAandFiQA(Finance Q&A)from BeIR benchmark, and TechQA dataset. Note that the model was evaluated offline on A100 GPUs using the model's PyTorch checkpoint. In this benchmark, the metric used was Recall@5. NQ (https://huggingface.co/datasets/BeIR/nq) HotpotQA (https://huggingface.co/datasets/hotpot_qa) FiQA(Finance Q&A) (https://huggingface.co/datasets/BeIR/fiqa) Open & Commercial Retrieval Models Average Recall@5 on NQ, HotpotQA, FiQA, TechQA dataset Data Collection Method by dataset:UnknownLabeling Method by dataset:UnknownProperties:The evaluation datasets are based on the MTEB/BEIR TextQA, andTechQA datasetwhich are 4 public datasets. The size ranges between 10,000s up to 5M depending on the dataset. Data Collection Method by dataset: Labeling Method by dataset: Properties: TechQA dataset (https://huggingface.co/datasets/PrimeQA/TechQA/tree/main) Technical Details ================= Input ----- Input Type:TextInput Format:List of stringsOther Properties Related to Input:The model was trained with input length up to 512 tokens, whereas the Mistral-7B model has a theoretical attention span of approximately 131K tokens. Input Type: Input Format: Other Properties Related to Input: Output ------ Output Type:FloatsOutput Format:List of float arraysOther Properties Related to Output:Model outputs embedding vectors of dimension 4096 for each text string. Output Type: Output Format: Other Properties Related to Output: Software Integration -------------------- Runtime:NeMo Retriever Text Embedding NIMSupported Hardware Microarchitecture Compatibility:NVIDIA Ampere, NVIDIA Hopper, NVIDIA LovelaceSupported Operating System(s):LinuxEngine:TensorRTTest Hardware:See Support Matrix fromNIM documentation. Runtime: Supported Hardware Microarchitecture Compatibility: Supported Operating System(s): Engine: TensorRT (https://developer.nvidia.com/tensorrt-getting-started) Test Hardware: NIM documentation (https://docs.nvidia.com/nim/nemo-retriever/text-embedding/latest/overview.html) We evaluated the models optimized for different hardware on a small sample dataset of 600 queries. Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ tab for the Explainability, Bias, Safety & Security, and Privacy subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nv-embedqa-e5-v5",
    "nvidia_url": "https://build.nvidia.com/nvidia/nv-embedqa-e5-v5",
    "scraped_on": "2025-10-15T21:43:44.974669",
    "tags": [
      "embedding",
      "nemo retriever",
      "nvidia",
      "retrieval augmented generation",
      "run-on-rtx",
      "text-to-embedding"
    ],
    "model_card": "Model Overview ============== Description ----------- The NVIDIA Retrieval QA E5 Embedding Model is an embedding model optimized for text question-answering retrieval. An embedding model is a crucial component of a text retrieval system, as it transforms textual information into dense vector representations. They are typically transformer decoders that process tokens of input text (for example, question, passage) to output an embedding. This model is ready for commercial use. NVIDIA Retrieval QA E5 Embedding Model is a part of NVIDIA NeMo Retriever, which provides state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can also readily customize them for their domain-specific use cases, such as Information Technology, Human Resource help assistants, and Research & Development research assistants. Intended use ------------ The NVIDIA Retrieval QA E5 Embedding Model is most suitable for users who want to build a question and answer application over a large text corpus, leveraging the latest dense retrieval technologies. License/Terms of use -------------------- The use of this model is governed by theNVIDIA AI Foundation Models Community License Agreementand theMIT License (MIT). NVIDIA AI Foundation Models Community License Agreement (https://developer.nvidia.com/downloads/nv-ai-foundation-models-license) MIT License (MIT) (https://github.com/microsoft/unilm/blob/master/LICENSE) Technology can have a profound impact on people and the world, and NVIDIA is committed to enabling trust and transparency in AI development. NVIDIA encourages users to adopt principles of AI ethics and trustworthiness to guide your business decisions by following the guidelines in the NVIDIA AI Foundation Models Community License Agreement. Model Architecture ------------------ Architecture Type:TransformerNetwork Architecture:Fine-tuned E5-Large-Unsupervised retrieverEmbedding Dimension:1024Parameter Count:335 million Architecture Type: Network Architecture: Embedding Dimension: Parameter Count: The NVIDIA Retrieval QA E5 Embedding Model is a transformer encoder - a finetuned version ofE5-Large-Unsupervised, with 24 layers and an embedding size of 1024, which is trained on public datasets. The AdamW optimizer is employed incorporating 100 warm up steps and 5e-6 learning rate with WarmupDecayLR scheduler. Embedding models for text retrieval are typically trained using a bi-encoder architecture. This involves encoding a pair of sentences (for example, query and chunked passages) independently using the embedding model. Contrastive learning is used to maximize the similarity between the query and the passage that contains the answer, while minimizing the similarity between the query and sampled negative passages not useful to answer the question. E5-Large-Unsupervised (https://huggingface.co/intfloat/e5-large-unsupervised) Model Version(s) ---------------- NVIDIA Retrieval QA E5 Embedding Model v5 Short name: NV-EmbedQA-E5-v5 Training Dataset & Evaluation ============================= Training Dataset ---------------- The development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset named MS MARCO restricts ‌commercial licensing, limiting the use of these models in commercial settings. To address this, we created our own training dataset blend based on public QA datasets, which each have a license for commercial applications. The training dataset details are as follows: Use Case:Information retrieval for question and answering over text documents.Data Sources:Public datasets licensed for commercial use.Language:English (US)Volume:400k samples from public datasets Use Case: Data Sources: Language: Volume: Data Collection Method by dataset:UnknownLabeling Method by dataset:Unknown Data Collection Method by dataset: Labeling Method by dataset: Evaluation Results ------------------ Properties: We evaluated the NVIDIA Retrieval QA E5 Embedding Model in comparison to literature open & commercial retriever models on academic benchmarks for question-answering -NQ,HotpotQAandFiQA(Finance Q&A)from BeIR benchmark and TechQA dataset. Note that the model was evaluated offline on A100 GPUs using the model's PyTorch checkpoint. In this benchmark, the metric used was Recall@5. NQ (https://huggingface.co/datasets/BeIR/nq) HotpotQA (https://huggingface.co/datasets/hotpot_qa) FiQA(Finance Q&A) (https://huggingface.co/datasets/BeIR/fiqa) Open & Commercial Retrieval Models Average Recall@5 on NQ, HotpotQA, FiQA, TechQA dataset Data Collection Method by dataset:UnknownLabeling Method by dataset:UnknownProperties:The evaluation datasets are based on the MTEB/BEIR TextQA and TechQA, which are 4 public datasets. The size ranges between 10,000s up to 5M depending on the dataset. Data Collection Method by dataset: Labeling Method by dataset: Properties: Technical Details ================= Input ----- Input Type:textInput Format:list of stringsOther Properties Related to Input:The model's maximum context length is 512 tokens. Texts longer than maximum length must either be chunked or truncated. Input Type: Input Format: Other Properties Related to Input: Output ------ Output Type:floatsOutput Format:list of float arraysOther Properties Related to Output:Model outputs embedding vectors of dimension 1024 for each text string Output Type: Output Format: Other Properties Related to Output: Software Integration -------------------- Runtime:NeMo Retriever Text Embedding NIMSupported Hardware Microarchitecture Compatibility:NVIDIA Ampere, NVIDIA Hopper, NVIDIA LovelaceSupported Operating System(s):LinuxEngine:TensorRTTest Hardware:See Support Matrix fromNIM documentation. Runtime: Supported Hardware Microarchitecture Compatibility: Supported Operating System(s): Engine: TensorRT (https://developer.nvidia.com/tensorrt-getting-started) Test Hardware: NIM documentation (https://docs.nvidia.com/nim/nemo-retriever/text-embedding/latest/overview.html) We evaluated the models optimized for different hardware on a small sample dataset of 600 queries. Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ tab for the Explainability, Bias, Safety & Security, and Privacy subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nv-rerankqa-mistral-4b-v3",
    "nvidia_url": "https://build.nvidia.com/nvidia/nv-rerankqa-mistral-4b-v3",
    "scraped_on": "2025-10-15T21:43:44.583754",
    "tags": [
      "nemo retriever",
      "nvidia",
      "reranking",
      "retrieval augmented generation"
    ],
    "model_card": "Model Overview ============== Description ----------- The NVIDIA Retrieval QA Mistral 4B Reranking Model is a model optimized for providing a logit score that represents how relevant a document(s) is to a given query. The ranking model is a component in a text retrieval system to improve the overall accuracy. A text retrieval system often uses an embedding model (dense) or lexical search (sparse) index to return relevant text passages given the input. A ranking model can be used to rerank the potential candidates into a final order. Ranking model has the question-passage pairs as an input and therefore, can process cross attention between the words. It would not be feasible to apply a Ranking model on all documents in the knowledge base, therefore, ranking models are often deployed in combination with embedding models. This model is ready for commercial use. NVIDIA Retrieval QA Mistral 4B Reranking Model is part of the NVIDIA NeMo Retriever, which provides state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can also readily customize them for their domain-specific use cases, such as Information Technology, Human Resource help assistants, and Research & Development research assistants. License/Terms of use -------------------- The use of this model is governed by theNVIDIA AI Foundation Models Community License Agreementand theApache License 2.0. NVIDIA AI Foundation Models Community License Agreement (https://developer.nvidia.com/downloads/nv-ai-foundation-models-license) Apache License 2.0 (https://choosealicense.com/licenses/apache-2.0/) Intended use ------------ The NVIDIA Retrieval QA Ranking model is most suitable for users who want to improve their retrieval systems by reranking a set of candidates for a given question. Model Architecture: Mistral-4B Ranker ------------------------------------- Architecture Type:TransformerNetwork Architecture:Fine-tuned Mistral 7B foundation model Architecture Type: Network Architecture: The NVIDIA Retrieval QA Ranking Model is a transformer encoder - a LoRA finetuned version ofMistral-7B-v0.1 LLMthat uses only the first 16 layers (resulting in a 4B parameters model) for higher throughput. We employ bi-directional attention when finetuning for higher accuracy. The last embedding output by the decoder model is used with a mean pooling strategy, and a binary classification head is fine-tuned for the ranking task. Mistral-7B-v0.1 LLM (https://huggingface.co/mistralai/Mistral-7B-v0.1) Ranking models for text ranking are typically trained as a cross-encoder for sentence classification. This involves predicting relevancy of a sentence pair (for example, question and chunked passages). The CrossEntropy loss is used to maximize the likelihood for passages containing information to answer the question and minimize the likelihood for (negative) passages which do not contain information to answer the question. The model was trained on public datasets described in the Dataset and Training section. Model Version(s) ---------------- NVIDIA Retrieval QA Text Reranking Mistral 4B v3 Short name: NV-RerankQA-Mistral-4B-v3 Training Dataset & Evaluation ============================= Training Dataset ---------------- The development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset namedMSMARCOrestricts ‌commercial licensing, limiting the use of these models in commercial settings. To address this, we created our own training dataset blend based on public QA datasets, which each have a license for commercial applications. MSMARCO (https://microsoft.github.io/msmarco/) The training dataset details are as follows: Use Case: Information retrieval for question and answering over text documents. Use Case Data Sources: Public datasets licensed for commercial use. Data Sources Language: English (US)Volume: 300k samples from public dataset Language Volume Data Collection Method by dataset: UnknownLabeling Method by dataset: Unknown Data Collection Method by dataset Labeling Method by dataset Evaluation Results ------------------ We evaluated the NVIDIA Retrieval QA Ranking Models in comparison to literature open & commercial retriever models on academic benchmarks for question-answering -NQ,HotpotQAandFiQA(Finance Q&A)from BEIR benchmark andTechQAdataset. In this benchmark, the metric used was Recall@5. As described, we need to apply the ranking model on the output of an embedding model. NQ (https://huggingface.co/datasets/BeIR/nq) HotpotQA (https://huggingface.co/datasets/hotpot_qa) FiQA(Finance Q&A) (https://huggingface.co/datasets/BeIR/fiqa) TechQA (https://huggingface.co/datasets/PrimeQA/TechQA/tree/main) Open & Commercial Retrieval Models Average Recall@5 on NQ, HotpotQA, FiQA, TechQA dataset Data Collection Method by dataset: Unknown Data Collection Method by dataset Labeling Method by dataset: Unknown Labeling Method by dataset Properties: The evaluation datasets are based on three MTEB/BEIR TextQA datasets, and the TechQA dataset, which are all public datasets. The size ranges between 10,000s up to 5M depending on the dataset. Properties Technical Details ================= Input ----- Input Type:Pair of TextsInput Format:List of text pairsOther Properties Related to Input:The model's maximum context length is 512 tokens. Texts longer than maximum length must either be chunked or truncated. Input Type: Input Format: Other Properties Related to Input: Output ------ Output Type:FloatsOutput Format:List of float arraysOther Properties Related to Output:Each the probability score (or raw logits) The user can decide if a Sigmoid activation function is applied to the logits. Output Type: Output Format: Other Properties Related to Output: Software Integration -------------------- Runtime:NeMo Retriever Text Embedding NIMSupported Hardware Microarchitecture Compatibility:NVIDIA Ampere, NVIDIA Hopper, NVIDIA LovelaceSupported Operating System(s):LinuxEngine:TensorRTTest Hardware:See Support Matrix fromNIM documentation. Runtime: Supported Hardware Microarchitecture Compatibility: Supported Operating System(s): Engine: TensorRT (https://developer.nvidia.com/tensorrt-getting-started) Test Hardware: NIM documentation (https://docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/overview.html) We evaluated the models optimized for different hardware on a small sample dataset of 600 queries. Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ tab for the Explainability, Bias, Safety & Security, and Privacy subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "mistral-nemo-12b-instruct",
    "nvidia_url": "https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct",
    "scraped_on": "2025-10-15T21:43:41.429962",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "nv-mistralai",
      "run-on-rtx",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description: ============ Mistral-NeMo is a Large Language Model (LLM) composed of 12B parameters. This model leads accuracy on popular benchmarks across common sense reasoning, coding, math, multilingual and multi-turn chat tasks; it significantly outperforms existing models smaller or similar in size. This model is ready for commercial use. Key features ------------ • Released under the Apache 2 License • Pre-trained and instructed versions • Trained with a 128k context window • Trained on a large proportion of multilingual and code data • Drop-in replacement of Mistral 7B Joint-Party Community Consideration =================================== This model was a jointly trained by Mistral and NVIDIA. License & Terms of use ====================== Your use of this API is governed bythe NVIDIA API Trial Service Terms of Use; and the use of this model is governed bythe NVIDIA AI Foundation Models Community License. Mistral NeMo-12B is released under the Apache 2.0 license. the NVIDIA API Trial Service Terms of Use (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) the NVIDIA AI Foundation Models Community License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-ai-foundation-models-community-license-agreement/) References(s): ============== Mistral NeMo 12BBlogpost Blogpost (https://mistral.ai/news/mistral-nemo/) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:MistralModel Version:0.1 Architecture Type: Network Architecture: Model Version: This transformer model has the following characteristics: • Layers: 40 • Dim: 5,120 • Head dim: 128 • Hidden dim: 14,436 • Activation Function: SwiGLU • Number of heads: 32 • Number of kv-heads: 8 (GQA) • Rotary embeddings (theta = 1M) • Vocabulary size: 2**17 ~= 128k Input • Input Type: Text • Input Format: String • Input Parameters: max_tokens, temperature, top_p, stop, frequency_penalty, presence_penalty, seed Output • Output Type: Text • Output Format: String Software Integration: ===================== • Supported Hardware Platform(s): NVIDIA Hopper • Preferred Operating System(s): Linux Benchmarks ---------- Main benchmarks --------------- • HellaSwag (0-shot): 83.5% • Winogrande (0-shot): 76.8% • OpenBookQA (0-shot): 60.6% • CommonSenseQA (0-shot): 70.4% • TruthfulQA (0-shot): 50.3% • MMLU (5-shot): 68.0% • TriviaQA (5-shot): 73.8% • NaturalQuestions (5-shot): 31.2% Multilingual benchmarks ----------------------- • MMLUFrench: 62.3%German: 62.7%Spanish: 64.6%Italian: 61.3%Portuguese: 63.3%Russian: 59.2%Chinese: 59.0%Japanese: 59.0% • French: 62.3% • German: 62.7% • Spanish: 64.6% • Italian: 61.3% • Portuguese: 63.3% • Russian: 59.2% • Chinese: 59.0% • Japanese: 59.0% Instruct benchmarks ------------------- • MT Bench (dev): 7.84 • MixEval Hard: 0.534 • IFEval-v5: 0.629 • Wildbench: 42.57 Inference ========= Engine:TensorRT-LLMTest Hardware:H100 Engine: Test Hardware: Ethical Considerations: ======================= When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.1-8b-instruct",
    "nvidia_url": "https://build.nvidia.com/meta/llama-3_1-8b-instruct",
    "scraped_on": "2025-10-15T21:43:38.308388",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "meta",
      "run-on-rtx",
      "text-to-text"
    ],
    "model_card": "Model Information ================= The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. Model Developer: Meta Model Developer Llama 3.1 Systems ================= Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community withsafeguardsthat developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All ourreference implementationsdemos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. safeguards (https://llama.meta.com/trust-and-safety/) reference implementations (https://github.com/meta-llama/llama-agentic-system) Intended Use ============ Intended Use CasesLlama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases. Intended Use Cases Out-of-scopeUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**. Out-of-scope **Note: Llama 3.1 has been trained on a broader collection of languages than the 10 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner. New Capabilities ================ Note that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases. Tool-use:Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. Tool-use: Multilinguality:Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. Multilinguality: Model Architecture:Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Model Architecture: Supported languages:English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai. Supported languages: Llama 3.1 family of models. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability. Llama 3.1 family of models Model Release Date:July 23, 2024. Model Release Date: Status:This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback. Status: LicenseA custom commercial license, the Llama 3.1 Community License, is available at:https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE License https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the modelREADME. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please gohere. README (https://github.com/meta-llama/llama3) here Hardware And Software ===================== Training FactorsWe used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure. Training Factors Training Energy UseTraining utilized a cumulative of39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. Training Energy Use 39.3 Training Greenhouse Gas EmissionsEstimated total location-based greenhouse gas emissions were11,390tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq. Training Greenhouse Gas Emissions 11,390 The methodology used to determine training energy use and greenhouse gas emissions can be foundhere. Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others. here (https://arxiv.org/pdf/2204.05149) Training Data ============= Overview:Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. Overview: Data Freshness:The pretraining data has a cutoff of December 2023. Data Freshness: Benchmarks - English Text ========================= In this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. Base pretrained models ---------------------- 85.9 84.8 Instruction Tuned Models ------------------------ 96.9 88.6 88.5 58.7 Multilingual Benchmarks ======================= Responsibility & Safety ======================= As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks: • Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. • Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm. • Provide protections for the community to help prevent the misuse of our models. Provide protections for the community to help prevent the misuse of our models. Responsible Deployment ====================== Llama is a foundational technology designed to be used in a variety of use cases, examples on how Meta's Llama models have been responsibly deployed can be found in ourCommunity Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to theResponsible Use Guideto learn more. Community Stories webpage (https://llama.meta.com/community-stories/) Responsible Use Guide (https://llama.meta.com/responsible-use-guide/) Llama 3.1 Instruct ================== Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. Fine-Tuning Data ---------------- We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We've developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. Refusals And Tone ----------------- Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. Evaluations =========== We evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application. Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization. Red Teaming =========== For both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. . Critical And Other Risks ======================== We specifically focused our efforts on mitigating the following critical risk areas: 1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness ------------------------------------------------------------------------------------------- To assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. 2. Child Safety --------------- Child Safety risk assessments were conducted using a team of experts, to assess the model's capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. 3. Cyber Attack Enablement -------------------------- Our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed. Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Our study of Llama-3.1-405B's social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more. Community ========= Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository. Github repository (https://github.com/meta-llama/PurpleLlama) We also set up theLlama Impact Grantsprogram to identify and support the most compelling applications of Meta's Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be foundhere. Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community. Llama Impact Grants (https://llama.meta.com/llama-impact-grants/) here (https://llama.meta.com/llama-impact-grants/#finalists) output reporting mechanism (https://developers.facebook.com/llama_output_feedback) bug bounty program (https://www.facebook.com/whitehat) Ethical Considerations And Limitations ====================================== The core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. But Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including ourResponsible Use Guide,Trust and Safetysolutions, and otherresourcesto learn more about responsible development. Responsible Use Guide (https://llama.meta.com/responsible-use-guide) Trust and Safety (https://llama.meta.com/trust-and-safety/) resources (https://llama.meta.com/docs/get-started/)"
  },
  {
    "name": "llama-3.1-70b-instruct",
    "nvidia_url": "https://build.nvidia.com/meta/llama-3_1-70b-instruct",
    "scraped_on": "2025-10-15T21:43:37.875729",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "meta",
      "text-to-text"
    ],
    "model_card": "Model Information ================= The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. Model Developer: Meta Model Developer Llama 3.1 Systems ================= Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community withsafeguardsthat developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All ourreference implementationsdemos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. safeguards (https://llama.meta.com/trust-and-safety/) reference implementations (https://github.com/meta-llama/llama-agentic-system) Intended Use ============ Intended Use CasesLlama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases. Intended Use Cases Out-of-scopeUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**. Out-of-scope **Note: Llama 3.1 has been trained on a broader collection of languages than the 10 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner. New Capabilities ================ Note that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases. Tool-use:Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. Tool-use: Multilinguality:Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. Multilinguality: Model Architecture:Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Model Architecture: Supported languages:English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai. Supported languages: Llama 3.1 family of models. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability. Llama 3.1 family of models Model Release Date:July 23, 2024. Model Release Date: Status:This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback. Status: LicenseA custom commercial license, the Llama 3.1 Community License, is available at:https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE License https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the modelREADME. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please gohere. README (https://github.com/meta-llama/llama3) here Hardware And Software ===================== Training FactorsWe used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure. Training Factors Training Energy UseTraining utilized a cumulative of39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. Training Energy Use 39.3 Training Greenhouse Gas EmissionsEstimated total location-based greenhouse gas emissions were11,390tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq. Training Greenhouse Gas Emissions 11,390 The methodology used to determine training energy use and greenhouse gas emissions can be foundhere. Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others. here (https://arxiv.org/pdf/2204.05149) Training Data ============= Overview:Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. Overview: Data Freshness:The pretraining data has a cutoff of December 2023. Data Freshness: Benchmarks - English Text ========================= In this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. Base pretrained models ---------------------- 85.9 84.8 Instruction Tuned Models ------------------------ 96.9 88.6 88.5 58.7 Multilingual Benchmarks ======================= Responsibility & Safety ======================= As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks: • Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. • Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm. • Provide protections for the community to help prevent the misuse of our models. Provide protections for the community to help prevent the misuse of our models. Responsible Deployment ====================== Llama is a foundational technology designed to be used in a variety of use cases, examples on how Meta's Llama models have been responsibly deployed can be found in ourCommunity Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to theResponsible Use Guideto learn more. Community Stories webpage (https://llama.meta.com/community-stories/) Responsible Use Guide (https://llama.meta.com/responsible-use-guide/) Llama 3.1 Instruct ================== Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. Fine-Tuning Data ---------------- We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We've developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. Refusals And Tone ----------------- Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. Evaluations =========== We evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application. Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization. Red Teaming =========== For both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. . Critical And Other Risks ======================== We specifically focused our efforts on mitigating the following critical risk areas: 1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness ------------------------------------------------------------------------------------------- To assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. 2. Child Safety --------------- Child Safety risk assessments were conducted using a team of experts, to assess the model's capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. 3. Cyber Attack Enablement -------------------------- Our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed. Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Our study of Llama-3.1-405B's social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more. Community ========= Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository. Github repository (https://github.com/meta-llama/PurpleLlama) We also set up theLlama Impact Grantsprogram to identify and support the most compelling applications of Meta's Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be foundhere. Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community. Llama Impact Grants (https://llama.meta.com/llama-impact-grants/) here (https://llama.meta.com/llama-impact-grants/#finalists) output reporting mechanism (https://developers.facebook.com/llama_output_feedback) bug bounty program (https://www.facebook.com/whitehat) Ethical Considerations And Limitations ====================================== The core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. But Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including ourResponsible Use Guide,Trust and Safetysolutions, and otherresourcesto learn more about responsible development. Responsible Use Guide (https://llama.meta.com/responsible-use-guide) Trust and Safety (https://llama.meta.com/trust-and-safety/) resources (https://llama.meta.com/docs/get-started/)"
  },
  {
    "name": "llama-3.1-405b-instruct",
    "nvidia_url": "https://build.nvidia.com/meta/llama-3_1-405b-instruct",
    "scraped_on": "2025-10-15T21:43:37.361408",
    "tags": [
      "chat",
      "code generation",
      "meta",
      "synthetic data generation"
    ],
    "model_card": "Model Information ================= The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. Model Developer: Meta Model Developer Llama 3.1 Systems ================= Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community withsafeguardsthat developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All ourreference implementationsdemos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. safeguards (https://llama.meta.com/trust-and-safety/) reference implementations (https://github.com/meta-llama/llama-agentic-system) Intended Use ============ Intended Use CasesLlama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases. Intended Use Cases Out-of-scopeUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**. Out-of-scope **Note: Llama 3.1 has been trained on a broader collection of languages than the 10 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner. New Capabilities ================ Note that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases. Tool-use:Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. Tool-use: Multilinguality:Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. Multilinguality: Model Architecture:Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Model Architecture: Supported languages:English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai. Supported languages: Llama 3.1 family of models. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability. Llama 3.1 family of models Model Release Date:July 23, 2024. Model Release Date: Status:This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback. Status: LicenseA custom commercial license, the Llama 3.1 Community License, is available at:https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE License https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the modelREADME. For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please gohere. README (https://github.com/meta-llama/llama3) here Hardware And Software ===================== Training FactorsWe used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure. Training Factors Training Energy UseTraining utilized a cumulative of39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. Training Energy Use 39.3 Training Greenhouse Gas EmissionsEstimated total location-based greenhouse gas emissions were11,390tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq. Training Greenhouse Gas Emissions 11,390 The methodology used to determine training energy use and greenhouse gas emissions can be foundhere. Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others. here (https://arxiv.org/pdf/2204.05149) Training Data ============= Overview:Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. Overview: Data Freshness:The pretraining data has a cutoff of December 2023. Data Freshness: Benchmarks - English Text ========================= In this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. Base pretrained models ---------------------- 85.9 84.8 Instruction Tuned Models ------------------------ 96.9 88.6 88.5 58.7 Multilingual Benchmarks ======================= Responsibility & Safety ======================= As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks: • Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. • Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm. • Provide protections for the community to help prevent the misuse of our models. Provide protections for the community to help prevent the misuse of our models. Responsible Deployment ====================== Llama is a foundational technology designed to be used in a variety of use cases, examples on how Meta's Llama models have been responsibly deployed can be found in ourCommunity Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to theResponsible Use Guideto learn more. Community Stories webpage (https://llama.meta.com/community-stories/) Responsible Use Guide (https://llama.meta.com/responsible-use-guide/) Llama 3.1 Instruct ================== Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. Fine-Tuning Data ---------------- We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We've developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. Refusals And Tone ----------------- Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. Evaluations =========== We evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application. Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization. Red Teaming =========== For both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. . Critical And Other Risks ======================== We specifically focused our efforts on mitigating the following critical risk areas: 1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness ------------------------------------------------------------------------------------------- To assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. 2. Child Safety --------------- Child Safety risk assessments were conducted using a team of experts, to assess the model's capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. 3. Cyber Attack Enablement -------------------------- Our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed. Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Our study of Llama-3.1-405B's social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more. Community ========= Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository. Github repository (https://github.com/meta-llama/PurpleLlama) We also set up theLlama Impact Grantsprogram to identify and support the most compelling applications of Meta's Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be foundhere. Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community. Llama Impact Grants (https://llama.meta.com/llama-impact-grants/) here (https://llama.meta.com/llama-impact-grants/#finalists) output reporting mechanism (https://developers.facebook.com/llama_output_feedback) bug bounty program (https://www.facebook.com/whitehat) Ethical Considerations And Limitations ====================================== The core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. But Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including ourResponsible Use Guide,Trust and Safetysolutions, and otherresourcesto learn more about responsible development. Responsible Use Guide (https://llama.meta.com/responsible-use-guide) Trust and Safety (https://llama.meta.com/trust-and-safety/) resources (https://llama.meta.com/docs/get-started/)"
  },
  {
    "name": "baichuan2-13b-chat",
    "nvidia_url": "https://build.nvidia.com/baichuan-inc/baichuan2-13b-chat",
    "scraped_on": "2025-10-15T21:43:34.003272",
    "tags": [
      "baichuan-inc",
      "chat",
      "chinese language generation",
      "text translation",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description: ------------ Baichuan 2 is the new generation of large-scale open-source language models launched by Baichuan Intelligence inc.. It is trained on a high-quality corpus with 2.6 trillion tokens and has achieved the best performance in authoritative Chinese and English benchmarks of the same size. This 13B version for chat models is fully open to academic research. Developers can also use them for free in commercial applications after obtaining an official commercial license. Evaluation ---------- Baichuan2-13B-Chat is tested on authoritative Chinese-English datasets across six domains: General, Legal, Medical, Mathematics, Code, and Multilingual Translation. For more detailed evaluation results of original models, please refer toGitHub. GitHub (https://github.com/baichuan-inc/Baichuan2) Terms and Conditions -------------------- We hereby declare that our team has not developed any applications based on Baichuan 2 models, not on iOS, Android, the web, or any other platform. We strongly call on all users not to use Baichuan 2 models for any activities that harm national / social security or violate the law. Also, we ask users not to use Baichuan 2 models for Internet services that have not undergone appropriate security reviews and filings. We hope that all users can abide by this principle and ensure that the development of technology proceeds in a regulated and legal environment. If any problems arise due to the use of Baichuan 2 open-source models, including but not limited to data security issues, public opinion risks, or any risks and problems brought about by the model being misled, abused, spread or improperly exploited, we will not assume any responsibility. Third-Party Community Consideration ----------------------------------- This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Baichuan-Inc'sModel Card Model Card (https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat) References(s): -------------- GitHubHuggingFaceTechnical Report GitHub (https://github.com/baichuan-inc/Baichuan2) HuggingFace (https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/tree/main) Technical Report (https://arxiv.org/abs/2309.10305) License, Acceptable Use, and Research Privacy Policy ---------------------------------------------------- By using this model, you are agreeing to the terms and conditions of theApache 2.0andCommunity License for Baichuan2 Model. Apache 2.0 (https://github.com/baichuan-inc/Baichuan2/blob/main/LICENSE) Community License for Baichuan2 Model (https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/resolve/main/Baichuan%202%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) Model Architecture: ------------------- Architecture Type:TransformerFine-tuned from model:Baichuan2 Architecture Type: Fine-tuned from model: Input: ------ Input Type:TextInput Format:StringInput Parameters:Temperature, Top P, Max Output Tokens Input Type: Input Format: Input Parameters: Output: ------- Output Type:TextOutput Format:String Output Type: Output Format: Inference: ========== Engine:Triton TensorRT-LLMTest Hardware:L40 Engine: Test Hardware:"
  },
  {
    "name": "mamba-codestral-7b-v0.1",
    "nvidia_url": "https://build.nvidia.com/mistralai/mamba-codestral-7b-v01",
    "scraped_on": "2025-10-15T21:43:30.748678",
    "tags": [
      "chat",
      "code completion",
      "code generation",
      "mistralai"
    ],
    "model_card": "Model Overview ============== Description: ============ Codestral Mamba is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. This broad language base ensures Codestral Mamba can assist developers in various coding environments and projects. This model is ready for commercial use and testing purposes. Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; seeCodestral's Mamba 7B Hugging Face Model Card. Codestral's Mamba 7B Hugging Face Model Card (https://huggingface.co/mistralai/mamba-codestral-7B-v0.1) Terms of Use ============ By using this software or model, you are agreeing to theterms and conditionsof the license, acceptable use policy and Mistral's privacy policy. Mamba-Codestral-7B-v0.1 is released under the Apache 2.0 license. terms and conditions (https://mistral.ai/terms-of-service/) References(s): ============== Mamba Codestral 7Bblogpost blogpost (https://mistral.ai/news/codestral-mamba/) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Mamba Codestral 7B v0.1Model Version:0.1 Architecture Type: Network Architecture: Model Version: Input: ====== Input Format:TextInput Parameters:Max Tokens, Temperature, Top PMax Input Tokens:4096 Input Format: Input Parameters: Max Input Tokens: Output: ======= Output Format:TextOutput Parameters:NoneMax Output Tokens:4096 Output Format: Output Parameters: Max Output Tokens: Software Integration: ===================== Supported Hardware Platform(s):NVIDIA Ampere, NVIDIA Hopper, NVIDIA TuringSupported Operating System(s):Linux Supported Hardware Platform(s): Supported Operating System(s): Inference: ========== Engine:TRT-LLMTest Hardware:L40S Engine: Test Hardware: Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "chatglm3-6b",
    "nvidia_url": "https://build.nvidia.com/thudm/chatglm3-6b",
    "scraped_on": "2025-10-15T21:43:27.602253",
    "tags": [
      "chat",
      "code generation",
      "regional language generation",
      "text translation",
      "text-to-text",
      "thudm"
    ],
    "model_card": "Model Overview ============== Description =========== ChatGLM3 is a generation of pre-trained dialogue models jointly released by Zhipu AI and Tsinghua KEG. ChatGLM3-6B is the open-source model in the ChatGLM3 series, maintaining many excellent features of the first two generations such as smooth dialogue and low deployment threshold. It adopts a more diverse training dataset, and a newly designed Prompt format, and has a more comprehensive open-source series. All these weights are fully open for academic research, and free commercial use is also allowed after registration via a questionnaire. Terms and Conditions ==================== We hereby declare that our team has not developed any applications based on ChatGLM3 models, not on iOS, Android, the web, or any other platform. We strongly call on all users not to use ChatGLM3 models for any activities that harm national / social security or violate the law. Also, we ask users not to use ChatGLM3 models for Internet services that have not undergone appropriate security reviews and filings. We hope that all users can abide by this principle and ensure that the development of technology proceeds in a regulated and legal environment. If any problems arise due to the use of ChatGLM3 open-source models, including but not limited to data security issues, public opinion risks, or any risks and problems brought about by the model being misled, abused, spread or improperly exploited, we will not assume any responsibility. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to THUDM'sModel Card. Model Card (https://huggingface.co/THUDM/chatglm3-6b) License, Acceptable Use, and Research Privacy Policy ---------------------------------------------------- By using this model, you are agreeing to the terms and conditions of theApache 2.0andChatGLM3-6B License. Apache 2.0 (https://github.com/THUDM/ChatGLM3/blob/main/LICENSE) ChatGLM3-6B License (https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE) Only for internal study and test purposes. Please follow the Model License and Terms of Use, and please contact ZhipuAI(https://open.bigmodel.cn/mla/form) for any commercial use. https://open.bigmodel.cn/mla/form The ChatGLM3-6B open-source model aims to promote the development of large-model technology. Developers and everyone are earnestly requested to comply with these rule: • Comply with applicable laws, regulations and policies. • Don't perform or facilitate any activities that may harm the safety, wellbeing, or rights of others, or for any services that have not been evaluated and registered. • Don't distribute output from ChatGLM3-6B open-source model and the service to misinform, misrepresent, mislead others or use in any improper purpose . Although every effort has been made to ensure the compliance and accuracy of the data at various stages of model training, due to the smaller scale of the ChatGLM3-6B model and the influence of probabilistic randomness factors, the accuracy of output content cannot be guaranteed. The model output is also easily misled by user input. Developers and users should build safeguards and assume risks and liabilities in the use of open-source models and codes. References ========== GitHub:https://github.com/THUDM/ChatGLM3HuggingFace:https://huggingface.co/THUDM/chatglm3-6bTechnical Report:https://arxiv.org/abs/2103.10360 https://github.com/THUDM/ChatGLM3 https://huggingface.co/THUDM/chatglm3-6b https://arxiv.org/abs/2103.10360 Model Developer:Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University Model Developer: Model Release Date:October 22, 2023 Model Release Date: Model Architecture: ------------------- Architecture Type:TransformerFine-tuned from model:ChatGLM3-6B base model Architecture Type: Fine-tuned from model: Input: ------ Input Type:TextInput Format:StringInput Parameters:Temperature, Top P, Max Output Tokens Input Type: Input Format: Input Parameters: Output: ------- Output Type:TextOutput Format:String Output Type: Output Format: Model Version:ChatGLM3-6Bv1.1.0 Model Version ChatGLM3-6B Evaluation ========== ChatGLM3-6B conducted performance tests on 8 typical Chinese-English datasets including: general language test, mathematics, code, and multilingual translation. For more detailed evaluation results of original models, please refer toGitHub. GitHub (https://github.com/THUDM/ChatGLM3/blob/main/README_en.md) Inference: ========== Engine:Triton TensorRT-LLMTest Hardware:L40 Engine: Test Hardware:"
  },
  {
    "name": "usdvalidate",
    "nvidia_url": "https://build.nvidia.com/nvidia/usdvalidate",
    "scraped_on": "2025-10-15T21:43:24.333073",
    "tags": [
      "digital twin",
      "nvidia",
      "openusd",
      "synthetic data generation",
      "usd",
      "validation",
      "visualization 3d"
    ],
    "model_card": "Model Overview ============== Description: ============ USD Validate is a service for rule based and visual validation of usdz files. It is based on the OmniverseEmbedded Web Viewerand theAsset Validator Extension. Embedded Web Viewer (https://docs.omniverse.nvidia.com/embedded-web-viewer/latest/) Asset Validator Extension (https://docs.omniverse.nvidia.com/kit/docs/asset-validator/latest/index.html) This service is for demonstration purposes and not for production usage. References(s): ============== • Omniverse Embedded Web Viewer Omniverse Embedded Web Viewer (https://docs.omniverse.nvidia.com/embedded-web-viewer/latest/) • Asset Validator Extension Asset Validator Extension (https://docs.omniverse.nvidia.com/kit/docs/asset-validator/latest/index.html) • OpenUSD USDChecker OpenUSD USDChecker (https://openusd.org/release/toolset.html#usdchecker) Model Architecture: =================== Architecture Type:Not Applicable (N/A)Network Architecture:Not Applicable (N/A) Architecture Type: Network Architecture: Input: ====== Input Type(s):3D File, Validation Rule Identifiers, and Viewport CommandsInput Format(s):USDZ, String Array, and Webrtc Data Channel Input Type(s): Input Format(s): Other Properties Related to Input: ---------------------------------- • MaterialX, OpenPbr and MDL materials are currently unsupported in USDZ. • UDIM texture tiles are currently unsupported in USDZ Output: ======= Output Type(s):Interactive Viewport and Validation Results SUmmaryOutput Format:WebRTC stream and JsonOther Properties Related to Output:The output of this service is currently limited to the webRTC stream / data channel only. Output Type(s): Output Format: Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s): • Not Applicable (N/A) Supported Hardware Microarchitecture Compatibility: • NVIDIA Lovelace • NVIDIA Ampere Preferred/Supported Operating System(s): ======================================== • Linux • Cuda > 12.2.2 • NVIDIA driver > 535.129.03"
  },
  {
    "name": "eyecontact",
    "nvidia_url": "https://build.nvidia.com/nvidia/eyecontact",
    "scraped_on": "2025-10-15T21:43:24.007399",
    "tags": [
      "digital human",
      "nvidia",
      "nvidia maxine",
      "telepresence"
    ],
    "model_card": "Model Overview ============== Description: ============ The Maxine Eye Contact model redirects eye gaze for video conference applications. The model estimates the gaze direction of the input eye gaze and synthesizes a redirected gaze using a region of interest around one’s eyes known as an eye patch. The encoder encodes the image’s contents into latent representations for eye gaze, head pose and environmental conditions. A transformation is applied to these latent representations to align them with the redirection angle provided by the user. The transformed representations when fed to the decoder result in the generation of an output eye patch with eyes redirected in the desired angle. More information about eye contact can be found in the developer bloghere. here (https://developer.nvidia.com/blog/improve-human-connection-in-video-conferences-with-nvidia-maxine-eye-contact/) The use of NVIDIA Maxine Eye Contact is available as a demonstration of the input and output of the Gaze Estimation transformer model. As such the user may submit a reference \"video\" and download the generated gaze redirected video for evaluation under the terms of theNVIDIA MAXINE EVALUATION LICENSE AGREEMENT. NVIDIA MAXINE EVALUATION LICENSE AGREEMENT (https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf) References(s): ============== • Rochelle, et al. “Improve Human Connection in Video Conferences with NVIDIA Maxine Eye Contact”, Jan 21, 2023, NVIDIA Technical Blog. Improve Human Connection in Video Conferences with NVIDIA Maxine Eye Contact (https://developer.nvidia.com/blog/improve-human-connection-in-video-conferences-with-nvidia-maxine-eye-contact/) Model Architecture: =================== Architecture Type:Convolution Neural Network (CNN)Network Architecture:Encoder-DecoderThe network architecture includes a transforming encoder and decoder network. The encoder encodes the image’s contents into latent representations for (a) the image’s non-subject related factors(e.g., environmental lighting, shadows, image white balance and hue, blurriness); (b) subjected-related factors (e.g., skin color, face/eye shape, eye glasses); (c) eye gaze and (d) head pose. Architecture Type: Network Architecture: A rotation applied to the individual latent factors affects a corresponding change in the appearance of the image to alter the factor. In this application, we apply a rotation to the gaze related latent factor. The transformed latent factors are fed to the decoder to generate the transformed version of the input with a gaze redirected angle. Input: ====== Input Type(s):Image: Angle vectorInput Format(s):Image: Red, Green, Blue (RGB), Angle vector: RadiansInput Parameters:Input Image:(256 x 64) 2D redirection angle vector [2,1]Other Properties Related to Input:Input image of resolution 256 x 64 normalized RGB in range of [0, 1]. Redirection gaze angle is a float32 value representing the pitch and yaw angles in radians. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):Image, Estimated Angle Vector,Landmarks VectorOutput Format:Image: Red, Green, Blue (RGB), Estimated Angle Vector: Radians, Landmarks Vector: Float32Output Parameters:Image: [256 x64], Estimated Angle Vector[2,1] , Landmarks Vector [2,1]Other Properties Related to Output:Output image is normalized by dividing by 255. Estimated gaze angle is a float32 representing the pitch and yaw angles in radians. Redirected eye landmarks are represented by 12 fiducial points marking the outline of the eyes, each value represented by x and y positions of the point in float32 format. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: ===================== 0.8.8.0 Supported Operating System(s): ============================== • Ubuntu 18.04 • Ubuntu 20.04 • Ubuntu 22.04 • Debian 11 • Rocky 8.7 • Windows 10 • Windows 11 Model Version(s): ================= [Maxine- 0.8.6.0] Supported Hardware Microarchitecture Compatability: =================================================== • [Turing] • [Ampere] • [ADA] • [Blackwell] Training and Evaluation Dataset: ================================ Data Collection Method by dataset: • Hybrid: Human and Synthetic Labeling Method by dataset: • Automated Properties (Quantity, Dataset Descriptions, Sensor(s)):Training dataset contains approximately 2 million images of rectangular eye patches of people captured with different environmental conditions, head poses and gaze angles. The dataset contain a combination of real data and synthetic data. Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: ------------------- Data Collection Method by dataset:Automated, Human Data Collection Method by dataset: Properties (Quantity, Dataset Descriptions, Sensor(s)): Internally-captured dataset of 65 videos, each between 30 seconds to two (2) minutes of a single person between 1 to 3 feet in front of the camera conducting a video conference. The dataset varies in terms of quality, lighting, head pose, gaze angles and other diversity factors such as race, eye color, and gender. Inference: ========== Engine:TensorRT,TritonTest Hardware: Engine: TensorRT (https://developer.nvidia.com/tensorrt) Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware: • CUDA 12.8 compatible hardware versions of Desktop and Servers. Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "usdsearch",
    "nvidia_url": "https://build.nvidia.com/nvidia/usdsearch",
    "scraped_on": "2025-10-15T21:43:20.517211",
    "tags": [
      "digital twin",
      "nvidia",
      "nvidia nim",
      "openusd",
      "synthetic data generation",
      "text-to-3d",
      "usd"
    ],
    "model_card": "USD Search ========== Model Overview ============== USD Search is an AI-powered search for OpenUSD data, three dimensional (3D) models, images, and assets using text or image-based inputs. It leverages NVCLIP, which is a NVIDIA commercial version of the \"Contrastive Language-Image Pre-Training (CLIP)\" model that transforms an image into textual embeddings. References: =========== • Radford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021. Model Architecture: =================== Architecture Type:Transformer-based architecture Architecture Type: Input: ====== Input Type(s):Text or ImageInput Format(s):Text, or Red, Green, Blue (RGB)Other Properties Related to Input:The model accepts either text or image input, but not both simultaneously Input Type(s): Input Format(s): Other Properties Related to Input: Output: ======= Output Type(s):ListOutput Format:Rendered thumbnails, asset metadataOther Properties Related to Output:The output of this model is a sorted-by-relevance list of OpenUSD assets. List contains rendered thumbnails and associated metadata containing URL pointing to the location of the asset in the backend database. Output Type(s): Output Format: Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s): • TensorRT Supported Hardware Architecture(s): • NVIDIA Ampere • NVIDIA Hopper • NVIDIA Lovelace Supported Operating System(s): • Linux Model Version(s): ================= • nv_clip_224_vit_h - NVCLIP ViT-H with 224 resolution. Training & Evaluation: ====================== No additional training or evaluation in addition to what has been done for the NVCLIP model. Using this Model ================ These models need to be used with NVIDIA hardware and software. For hardware, the models can run on any of the latest NVIDIA GPUs since NVIDIA Ampere. Training Dataset: ================= Data Collection Method by dataset: • Automated Labeling Method by dataset: • Automated Properties: Evaluation Dataset: =================== Link:https://www.image-net.org/ Link: https://www.image-net.org/ Data Collection Method by dataset: • Unknown Labeling Method by dataset: • Unknown Properties:50,000 validation images fromImageNet datasetThe performance details of the underlying NVCLIP model is noted below. Properties: ImageNet dataset (https://www.image-net.org/download.php) Methodology and KPI ------------------- The performance of zero shot accuracy of NVCLIP on ImageNet validation dataset. Inference: ========== Engine:TensorRTTest Hardware: Engine: Test Hardware: • A100 • L40 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Promise and the Explainability, Bias, Safety & Security, and Privacy Subcards."
  },
  {
    "name": "gemma-2-2b-it",
    "nvidia_url": "https://build.nvidia.com/google/gemma-2-2b-it",
    "scraped_on": "2025-10-15T21:43:17.337351",
    "tags": [
      "chat",
      "code generation",
      "google",
      "language generation",
      "text-to-text"
    ],
    "model_card": "Gemma 2 Model Card ================== Author: Google Author Description ----------- Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as a laptop, desktop or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. This model is ready for commercial and non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. See the link to Non-NVIDIAGemma Model Card. Gemma Model Card (https://ai.google.dev/gemma/docs) Terms of Use ============ Terms Terms (https://ai.google.dev/gemma/terms) Prohibited uses of Gemma models are outlined in theGemma Prohibited Use Policy. Gemma Prohibited Use Policy (https://ai.google.dev/gemma/prohibited_use_policy) Citation -------- Model Information ================= Summary description and brief definition of inputs and outputs. Usage and Limitations ===================== These models have certain limitations that users should be aware of. Intended Usage -------------- Open Large Language Models (LLMs) have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development. • Content Creation and CommunicationText Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts.Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications.Text Summarization: Generate concise summaries of a text corpus, research papers, or reports. • Text Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts. • Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications. • Text Summarization: Generate concise summaries of a text corpus, research papers, or reports. • Research and EducationNatural Language Processing (NLP) Research: These models can serve as a foundation for researchers to experiment with NLP techniques, develop algorithms, and contribute to the advancement of the field.Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice.Knowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics. • Natural Language Processing (NLP) Research: These models can serve as a foundation for researchers to experiment with NLP techniques, develop algorithms, and contribute to the advancement of the field. • Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice. • Knowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics. Limitations ----------- • Training DataThe quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses.The scope of the training dataset determines the subject areas the model can handle effectively. • The quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses. • The scope of the training dataset determines the subject areas the model can handle effectively. • Context and Task ComplexityLLMs are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging.A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • LLMs are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging. • A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • Language Ambiguity and NuanceNatural language is inherently complex. LLMs might struggle to grasp subtle nuances, sarcasm, or figurative language. • Natural language is inherently complex. LLMs might struggle to grasp subtle nuances, sarcasm, or figurative language. • Factual AccuracyLLMs generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • LLMs generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • Common SenseLLMs rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. • LLMs rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Gemma-2Model Version:0.1 Architecture Type: Network Architecture: Model Version: Input and Output ---------------- Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:One-Dimensional (1D)Other Properties Related to Output:Text can be question, a prompt, or a document to be summarized. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Output: Output: ======= Output Type(s):TextOutput Format(s):StringOutput Parameters:One-Dimensional (1D)Other Properties Related to Output:Generated English-language text in response to the input (e.g., an answer to the question, a summary of the document). Output Type(s): Output Format(s): Output Parameters: Other Properties Related to Output: Model Data ========== Data used for model training and how the data was processed. Training Dataset ---------------- These models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 13 trillion tokens and the 9B model was trained with 8 trillion tokens, and 2B model was trained with 2 trillion tokens. Here are the key components: • Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content. • Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions. • Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries. The combination of these diverse data sources is crucial for training a powerful language model that can handle a wide variety of different tasks and text formats. Data Preprocessing ------------------ Here are the key data cleaning and filtering methods applied to the training data: • CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content. • Sensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets. • Additional methods: Filtering based on content quality and safety in line withour policies. our policies (https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11) Implementation Information ========================== TensorRT-LLM ------------ The endpoint available on NGC catalog is accelerated by TensorRT-LLM, an open-source library for optimizing inference performance. Gemma is compatible across NVIDIA AI platforms—from the datacenter, cloud, to the local PC with RTX GPU systems. Gemma models use a vocabulary size of 256K and support a context length of up to 4K while using rotary positional embedding (RoPE). With support for Position Interpolation (PI) available in TensorRT-LLM, Gemma models using RoPE can support longer output sequence lengths at inference time while retaining original model architecture. Software -------- Training was done usingJAXandML Pathways. JAX (https://github.com/google/jax) ML Pathways (https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/) JAX allows researchers to take advantage of the latest generation of hardware, including TPUs, for faster and more efficient training of large models. ML Pathways is Google's latest effort to build artificially intelligent systems capable of generalizing across multiple tasks. This is specially suitable forfoundation models, including large language models like these ones. foundation models (https://ai.google/discover/foundation-models/) Together, JAX and ML Pathways are used as described in thepaper about the Gemini family of models; \"the 'single controller' programming model of Jax and Pathways allows a single Python process to orchestrate the entire training run, dramatically simplifying the development workflow.\" paper about the Gemini family of models (https://goo.gle/gemma2report) Evaluation ========== Model evaluation metrics and results. Benchmark Results ----------------- These models were evaluated against a large collection of different datasets and metrics to cover different aspects of text generation: MMLU (https://arxiv.org/abs/2009.03300) HellaSwag (https://arxiv.org/abs/1905.07830) PIQA (https://arxiv.org/abs/1911.11641) SocialIQA (https://arxiv.org/abs/1904.09728) BoolQ (https://arxiv.org/abs/1905.10044) WinoGrande (https://arxiv.org/abs/1907.10641) ARC-e (https://arxiv.org/abs/1911.01547) ARC-c (https://arxiv.org/abs/1911.01547) TriviaQA (https://arxiv.org/abs/1705.03551) Natural Questions (https://github.com/google-research-datasets/natural-questions) HumanEval (https://arxiv.org/abs/2107.03374) MBPP (https://arxiv.org/abs/2108.07732) GSM8K (https://arxiv.org/abs/2110.14168) MATH (https://arxiv.org/abs/2103.03874) AGIEval (https://arxiv.org/abs/2304.06364) DROP (https://arxiv.org/abs/1903.00161) BIG-Bench (https://arxiv.org/abs/2206.04615) Ethics and Safety ================= Ethics and safety evaluation approach and results. Evaluation Approach ------------------- Our evaluation methods include structured evaluations and internal red-teaming testing of relevant content policies. Red-teaming was conducted by a number of different teams, each with different goals and human evaluation metrics. These models were evaluated against a number of different categories relevant to ethics and safety, including: • Text-to-Text Content Safety: Human evaluation on prompts covering safety policies including child sexual abuse and exploitation, harassment, violence and gore, and hate speech. • Text-to-Text Representational Harms: Benchmark against relevant academic datasets such asWinoBiasandBBQ Dataset. WinoBias (https://arxiv.org/abs/1804.06876) BBQ Dataset (https://arxiv.org/abs/2110.08193v2) • Memorization: Automated evaluation of memorization of training data, including the risk of personally identifiable information exposure. • Large-scale harm: Tests for \"dangerous capabilities,\" such as chemical, biological, radiological, and nuclear (CBRN) risks. Evaluation Results ------------------ The results of ethics and safety evaluations are within acceptable thresholds for meetinginternal policiesfor categories such as child safety, content safety, representational harms, memorization, large-scale harms. On top of robust internal evaluations, the results of well-known safety benchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA are shown here. internal policies (https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11) Gemma 2.0 --------- RealToxicity (https://arxiv.org/abs/2009.11462) CrowS-Pairs (https://aclanthology.org/2020.emnlp-main.154/) BBQ Ambig (https://arxiv.org/abs/2110.08193v2) BBQ Disambig (https://arxiv.org/abs/2110.08193v2) Winogender (https://arxiv.org/abs/1804.09301) TruthfulQA (https://arxiv.org/abs/2109.07958) Winobias 1_2 (https://arxiv.org/abs/1804.06876) Winobias 2_2 (https://arxiv.org/abs/1804.06876) Toxigen (https://arxiv.org/abs/2203.09509) Dangerous Capability Evaluations ================================ Evaluation Approach ------------------- We evaluated a range of dangerous capabilities: • Offensive cybersecurity:To assess the model's potential for misuse in cybersecurity contexts, we utilized both publicly available Capture-the-Flag (CTF) platforms like InterCode-CTF and Hack the Box, as well as internally developed CTF challenges. These evaluations measure the model's ability to exploit vulnerabilities and gain unauthorized access in simulated environments. Offensive cybersecurity: • Self-proliferation:We evaluated the model's capacity for self-proliferation by designing tasks that involve resource acquisition, code execution, and interaction with remote systems. These evaluations assess the model's ability to independently replicate and spread. Self-proliferation: • Persuasion:To evaluate the model's capacity for persuasion and deception, we conducted human persuasion studies. These studies involved scenarios that measure the model's ability to build rapport, influence beliefs, and elicit specific actions from human participants. Persuasion: Evaluation Results ------------------ All evaluations are described in detail inEvaluating Frontier Models for Dangerous Capabilitiesand in brief in theGemma 2 technical report. Evaluating Frontier Models for Dangerous Capabilities (https://arxiv.org/abs/2403.13793) Gemma 2 technical report (https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf) Ethical Considerations and Risks -------------------------------- The development of large language models (LLMs) raises several ethical concerns. In creating an open model, we have carefully considered the following: • Bias and FairnessLLMs trained on large-scale, real-world text data can reflect socio-cultural biases embedded in the training material. These models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card. • LLMs trained on large-scale, real-world text data can reflect socio-cultural biases embedded in the training material. These models underwent careful scrutiny, input data pre-processing described and posterior evaluations reported in this card. • Misinformation and MisuseLLMs can be misused to generate text that is false, misleading, or harmful.Guidelines are provided for responsible use with the model, see theResponsible Generative AI Toolkit. • LLMs can be misused to generate text that is false, misleading, or harmful. • Guidelines are provided for responsible use with the model, see theResponsible Generative AI Toolkit. Responsible Generative AI Toolkit (https://ai.google.dev/responsible) • Transparency and Accountability:This model card summarizes details on the models' architecture, capabilities, limitations, and evaluation processes.A responsibly developed open model offers the opportunity to share innovation by making LLM technology accessible to developers and researchers across the AI ecosystem. • This model card summarizes details on the models' architecture, capabilities, limitations, and evaluation processes. • A responsibly developed open model offers the opportunity to share innovation by making LLM technology accessible to developers and researchers across the AI ecosystem. Risks identified and mitigations: • Perpetuation of biases: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases. • Generation of harmful content: Mechanisms and guidelines for content safety are essential. Developers are encouraged to exercise caution and implement appropriate content safety safeguards based on their specific product policies and application use cases. • Misuse for malicious purposes: Technical limitations and developer and end-user education can help mitigate against malicious applications of LLMs. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in theGemma Prohibited Use Policy. Gemma Prohibited Use Policy (https://ai.google.dev/gemma/prohibited_use_policy) • Privacy violations: Models were trained on data filtered for removal of PII (Personally Identifiable Information). Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques. Benefits -------- At the time of release, this family of models provides high-performance open large language model implementations designed from the ground up for Responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown to provide superior performance to other, comparably-sized open model alternatives."
  },
  {
    "name": "shieldgemma-9b",
    "nvidia_url": "https://build.nvidia.com/google/shieldgemma-9b",
    "scraped_on": "2025-10-15T21:43:16.939639",
    "tags": [
      "google",
      "guardrail",
      "text-to-text"
    ],
    "model_card": "ShieldGemma Model Card ====================== Author: Google Author Description ----------- ShieldGemma is a series of safety content moderation models built uponGemma 2that target four harm categories (sexually explicit, dangerous content, hate, and harassment). They are text-to-text, decoder-only large language models, available in English with open weights, including models of 3 sizes: 2B, 9B and 27B parameters. Gemma 2 (https://ai.google.dev/gemma#gemma-2) This model is ready for commercial and non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. See the link to Non-NVIDIAShield Gemma Model Card. Shield Gemma Model Card (https://ai.google.dev/gemma/docs/shieldgemma/model_card) Terms of Use: ============= TermsProhibited uses of Gemma models are outlined in theGemma Prohibited Use Policy. Terms (https://ai.google.dev/gemma/terms) Gemma Prohibited Use Policy (https://ai.google.dev/gemma/prohibited_use_policy) Usage and Limitations ===================== These models have certain limitations that users should be aware of. Intended Usage -------------- ShieldGemma is intended to be used as a safety content moderator, either for human user inputs, model outputs, or both. These models are part of theResponsible Generative AI Toolkit, which is a set of guidelines, tools, datasets, and models aimed to improve the safety of AI applications as part of the Gemma ecosystem. Responsible Generative AI Toolkit (https://ai.google.dev/responsible) Limitations ----------- All the usual limitations for large language models apply; see theGemma model cardfor more details. Additionally, there are limited benchmarks that can be used to evaluate content moderation, so training and evaluation data might not be representative of real-world scenarios. Gemma model card (https://ai.google.dev/gemma/docs/model_card_2) ShieldGemma is also highly sensitive to the specific user-provided descriptions of safety principles, and might perform unpredictably under conditions that require a good understanding of language ambiguity and nuance. As with other models in the Gemma ecosystem, ShieldGemma is subject to Google'sprohibited use policies. prohibited use policies (https://ai.google.dev/gemma/prohibited_use_policy) Model Information ================= Summary description and brief definition of inputs and outputs. Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Gemma-2Model Version:0.1 Architecture Type: Network Architecture: Model Version: Input and Output ---------------- Input Type(s):TextInput Format(s):StringInput Parameters:One-Dimensional (1D)Other Properties Related to Input:Text string containing a preamble, the text to be classified, a set of policies, and the prompt epilogue. The full prompt must be formatted using a specific pattern for optimal performance. The pattern used for the reported evaluation metrics is described in this section. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output Type(s):TextOutput Format(s):StringOutput Parameters:One-Dimensional (1D)Other Properties Related to Input:Text string, which will start with the token \"Yes\" or \"No\" and represent whether the user input or model output violate the provided policies. Output Type(s): Output Format(s): Output Parameters: Other Properties Related to Input: The prompt pattern contains the following components, in order: • Preamble, establishing the model as a policy expert, based on theLLM-as-a-judgetechnique. LLM-as-a-judge (https://arxiv.org/abs/2306.05685) • User prompt, wrapped with<start_of_turn>and<end_of_turn>control tokens. • Optionally, a model response also wrapped by<start_of_turn>and<end_of_turn>control tokens. • Description of the safety policy. • Epilogue, requesting the model to classify the text. Here is an example of a prompt used to evaluate the user prompt[User Prompt]: Guidelines to use in prompts ---------------------------- ShieldGemma uses different phrasing for its safety guidelines depending on whether the content being classified is only user-provided content (the Prompt-only use case, typically for input filtering) or both user-provided and model-generated content (the Prompt-Response use case, typically for output filtering). Use Case 1: Prompt-only Content Classification Use Case 2: Prompt-Response Content Classification Citation -------- Model Data ========== Data used for model training and how the data was processed. Training Dataset ---------------- The base models were trained on a dataset of text data that includes a wide variety of sources, see theGemma 2 documentationfor more details. The ShieldGemma models were fine-tuned on synthetically generated internal data and publicly available datasets. More details can be found in the ShieldGemmatechnical report. Gemma 2 documentation (https://ai.google.dev/gemma#gemma-2) technical report (https://storage.googleapis.com/deepmind-media/gemma/shieldgemma-report.pdf) Implementation Information ========================== Details about the model internals. TensorRT-LLM ------------ The endpoint available on NGC catalog is accelerated by TensorRT-LLM, an open-source library for optimizing inference performance. Gemma is compatible across NVIDIA AI platforms—from the datacenter, cloud, to the local PC with RTX GPU systems. Software -------- Training was done usingJAXandML Pathways. For more details refer to theGemma2 model card. JAX (https://github.com/google/jax) ML Pathways (https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/) Gemma2 model card (https://ai.google.dev/gemma/docs/model_card_2) Evaluation ========== Model evaluation metrics and results. Benchmark Results ----------------- These models were evaluated against both internal and external datasets. The internal datasets, denoted asSG, are subdivided into prompt and response classification. Evaluation results are based on Optimal F1(left)/AU-PRC(right); higher is better. OpenAI Mod (https://github.com/openai/moderation-api-release) ToxicChat (https://arxiv.org/abs/2310.17389) Ethics and Safety ================= Evaluation Approach ------------------- Although the ShieldGemma models are generative models, they are designed to be run inscoring modeto predict the probability that the next token wouldYesorNo. Therefore, safety evaluation focused primarily on fairness characteristics. scoring mode Evaluation Results ------------------ These models were assessed for ethics, safety, and fairness considerations and met internal guidelines. Ethical Considerations and Risks -------------------------------- The development of large language models (LLMs) raises several ethical concerns. We have carefully considered multiple ethical aspects in the development of these models. Refer to theGemma 2 model cardfor more details. Gemma 2 model card (https://ai.google.dev/gemma/docs/model_card_2) Benefits -------- At the time of release, this family of models provides high-performance open large language model implementations designed from the ground up for Responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown to provide superior performance to other, comparably-sized open model alternatives."
  },
  {
    "name": "proteinmpnn",
    "nvidia_url": "https://build.nvidia.com/ipd/proteinmpnn",
    "scraped_on": "2025-10-15T21:43:13.577561",
    "tags": [
      "biology",
      "bionemo",
      "drug discovery",
      "ipd",
      "nim",
      "protein generation"
    ],
    "model_card": "Model Overview ============== Description: ============ ProteinMPNN (Protein Message Passing Neural Network) is a deep learning-based graph neural network designed to predict amino acid sequences for given protein backbones. This network leverages evolutionary, functional, and structural information to generate sequences that are likely to fold into the desired 3D structures. This model is available for commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. License/Terms of Use: --------------------- This model is released under theMIT License. MIT License (https://github.com/dauparas/ProteinMPNN/blob/8907e6671bfbfc92303b5f79c4b5e6ce47cdef57/LICENSE) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. References: =========== Model Architecture: =================== Architecture Type:Protein Amino Acid Sequence PredictionNetwork Architecture:ProteinMPNN Architecture Type: Network Architecture: Input: ====== Input Type(s):Protein in Protein Data Bank (PDB) formatInput Format(s):StringInput Parameters:One-Dimensional (1D) Input Type(s): Input Format(s): Input Parameters: Output: ======= Output Type(s):Amino Acid SequenceOutput Format:Multi-FASTA (text file)Output Parameters:1D Output Type(s): Output Format: Output Parameters: Software Integration: ===================== Runtime Engine(s): • Triton Supported Hardware Microarchitecture Compatibility: • NVIDIA Hopper • NVIDIA Ampere • NVIDIA Lovelace • NVIDIA Turing [Preferred/Supported] Operating System(s): • Linux Model Version(s): ================= ProteinMPNN 1.0.1 Training & Evaluation: ====================== Training Dataset: ================= Link:The Protein Data Bank Link: The Protein Data Bank (https://www.rcsb.org/) ** Data Collection Method by dataset • Hybrid: Automatic, Human For PDB dataset, scientists worldwide submit structural data determined by X-ray crystallography or cryo-electron microscopy (cryo-EM). This includes atomic coordinates, experimental data, and metadata about the biological macromolecules. ** Labeling Method by dataset • Hybrid: Automatic, Human For PDB dataset, expert biocurators review the submitted data to ensure accuracy and completeness. This involves checking the plausibility of the data and annotating it with relevant biological and chemical information. CATH 4.1 dataset is derived from the PDB dataset. The CATH (Class, Architecture, Topology, Homologous superfamily) database hierarchically classifies protein domain structures that are obtained from protein structures deposited in the PDB. The data in CATH are specifically sourced from PDB files and include structures determined at a resolution of 4 angstrom or better. The classification process involves both manual and automated methods to ensure accurate domain identification and classification. For ProteinMPNN, the data underwent quality filtering to ensure high accuracy, this involved removing structures with low resolution and potential errors. Properties (Quantity, Dataset Descriptions, Sensor(s)):Model was trained by Institute for Protein Design. The dataset for training consisted of 23,358 sequences. Dataset: CATH 4.2, PDB. Sensors: X-ray crystallography, cryoEM. Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: =================== ** Data Collection Method by dataset • Automatic: random splits from PDB dataset. ** Labeling Method by dataset • Automatic: random splits from PDB dataset. The training, validation, and test splits were derived from protein assemblies in the PDB, which includes structures determined by X-ray crystallography or cryo-electron microscopy (cryoEM). The dataset was divided into random splits with 23,358 sequences for training, 1,464 for validation, and 1,529 for testing. Inference: ========== Engine:TritonTest Hardware: Engine: Test Hardware: • A100 • L40 • H100 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "parakeet-ctc-0.6b-asr",
    "nvidia_url": "https://build.nvidia.com/nvidia/parakeet-ctc-0_6b-asr",
    "scraped_on": "2025-10-15T21:43:10.423410",
    "tags": [
      "asr",
      "batch",
      "english",
      "fast",
      "nvidia",
      "nvidia nim",
      "run-on-rtx",
      "speech-to-text",
      "streaming"
    ],
    "model_card": "Model Overview ============== Parakeet-CTC-XL-0.6B (around 600M parameters) is trained on ASRSet with over 35000 hours of English (en-US) speech. The model transcribes speech in lower case English alphabet along with spaces and apostrophes. Description =========== Parakeet transcribes audio into text, using spaces and apostrophes where needed Terms of use ============ By using this software or microservice, you are agreeing to theterms and conditionsof the license and acceptable use policy. terms and conditions (https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/) Disclaimer ========== AI models generate responses and outputs based on complex algorithms and machine learning techniques, and those responses or outputs may be inaccurate or indecent. By testing this model, you assume the risk of any harm caused by any response or output of the model. Please do not upload any confidential information or personal data. Your use is logged for security. References ========== • Paper Paper (https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) • Fast-Conformer-CTC Model Fast-Conformer-CTC Model (https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) • Conformer Conformer (https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html#conformer) Model Architecture ================== Architecture Type:Parakeet-CTC (also known as FastConformer-CTC), which is an optimized version of Conformer model with 8x depthwise-separable convolutional downsampling with CTC lossNetwork Architecture:Parakeet-CTC-XL-0.6B Architecture Type: Network Architecture: Input ===== Input Type(s):Audio in EnglishInput Format(s):Linear PCM 16-bit 1 channelOther Properties Related to Input:Maximum Length in seconds specific to GPU Memory, No Pre-Processing Needed, Mono channel is required Input Type(s): Input Format(s): Other Properties Related to Input: Output ====== Output Type(s):Text String in English with TimestampsOutput Parameters:1-DimensionOther Properties Related to Output:No Maximum Character Length, Does not handle special characters Output Type(s): Output Parameters: Other Properties Related to Output: Software Integration ==================== Runtime Engine(s): • Riva 2.15.0 or Higher Supported Operating System(s): • Linux Model Version ============= Parakeet-0.6b-ctc-en-us-asr-set-6.0 Inference ========= Engine:TritonTest Hardware: Engine: Test Hardware: • NVIDIA A2 • NVIDIA A10 • NVIDIA A16 • NVIDIA A30 • NVIDIA A40 • NVIDIA A100 • NVIDIA H100 • NVIDIA L4 • NVIDIA L40 • GeForce RTX 40xx • GeForce RTX 50xx • Blackwell RTX 60xx Ethical Considerations (For NVIDIA Models Only): ================================================ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) GOVERNING TERMS: ================ This trial is governed by the NVIDIA API Trial Terms of Service (found athttps://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf). The use of this model is governed by the AI Foundation Models Community License Agreement (found at NVIDIA Agreements | Enterprise Software | NVIDIA AI Foundation Models Community License Agreement). https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf"
  },
  {
    "name": "parakeet-ctc-1.1b-asr",
    "nvidia_url": "https://build.nvidia.com/nvidia/parakeet-ctc-1_1b-asr",
    "scraped_on": "2025-10-15T21:43:06.963603",
    "tags": [
      "asr",
      "batch",
      "english",
      "nvidia",
      "nvidia nim",
      "speech-to-text",
      "streaming"
    ],
    "model_card": "Model Overview ============== Parakeet is a major step forward in the evolution of conversational AI. Its exceptional accuracy, coupled with the flexibility and ease of use offered by NeMo, empowers developers to create more natural and intuitive voice-powered applications. The possibilities are endless, from enhancing the accuracy of virtual assistants to enabling seamless real-time communication. Description =========== Parakeet transcribes audio into text, using spaces and apostrophes where needed Terms of use ============ GOVERNING TERMS: Use of this model is governed by theNVIDIA Community Model License Agreement. NVIDIA Community Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Disclaimer ========== AI models generate responses and outputs based on complex algorithms and machine learning techniques, and those responses or outputs may be inaccurate or indecent. By testing this model, you assume the risk of any harm caused by any response or output of the model. Please do not upload any confidential information or personal data. Your use is logged for security. References ========== • Fast Conformer Fast Conformer (https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) Model Architecture ================== Architecture Type:Convolutional Neural Network + TransformerNetwork Architecture:Fast Conformer Encoder with CTC Decoder Architecture Type: Network Architecture: Input ===== Input Type(s):Audio in EnglishInput Format(s):Linear PCM 16-bit 1 channel Input Type(s): Input Format(s): Output ====== Output Type(s):Text String in English with Timestamps Output Type(s): Software Integration ==================== Runtime Engine(s): • Riva 2.15.0 or Higher Supported Operating System(s): • Linux Model Version ============= Parakeet-1.1b-ctc-en-us-asr-set-6.0 Inference ========= Engine:TritonTest Hardware: Engine: Test Hardware: • NVIDIA A2 • NVIDIA A10 • NVIDIA A16 • NVIDIA A30 • NVIDIA A40 • NVIDIA A100 • NVIDIA H100 • NVIDIA L4 • NVIDIA L40 • GeForce RTX 40xx • GeForce RTX 50xx • Blackwell RTX 60xx Ethical Considerations (For NVIDIA Models Only): ================================================ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "megatron-1b-nmt",
    "nvidia_url": "https://build.nvidia.com/nvidia/megatron-1b-nmt",
    "scraped_on": "2025-10-15T21:43:06.479327",
    "tags": [
      "neural machine translation",
      "nvidia",
      "nvidia nim",
      "text translation"
    ],
    "model_card": "Model Outputs.AI models generate responses and outputs that may be inaccurate, harmful, biased or indecent. NVIDIA hosts third-party models that could contain political content or potentially misleading information. NVIDIA does not endorse, verify or assume responsibility for a third-party model. By using an API Service, you assume the risk of any harm caused by any response or output of any model. Model Outputs. Privacy.Your input and output will be recorded to provide you with this trial experience and to improve NVIDIA products and services, including AI models, in accordance with ourPrivacy Policy. Do not upload any confidential information or personal data unless expressly permitted. Your use is logged for security, fraud or abuse monitoring and shared with third party service providers for this purpose. If the demo necessarily requires the input of personal data, logging for product development purposes will be turned off. Privacy. Privacy Policy (https://www.nvidia.com/en-us/about-nvidia/privacy-policy/)"
  },
  {
    "name": "nv-grounding-dino",
    "nvidia_url": "https://build.nvidia.com/nvidia/nv-grounding-dino",
    "scraped_on": "2025-10-15T21:43:03.223155",
    "tags": [
      "computer vision",
      "deepstream",
      "nvidia",
      "nvidia nim",
      "object detection"
    ],
    "model_card": "Pretrained Grounding DINO with Commercial License ================================================= Description =========== Open vocabulary object detection is a computer vision technique that can detect one or multiple objects in a frame based on the text input. Object detection recognizes the individual objects in an image and places bounding boxes around the object. This model card contains pre-trained weights for theGrounding DINOobject detection networks pretrained on the commercial dataset. Note that the model in this model card can be used for commercial purpose. Grounding DINO (https://arxiv.org/abs/2303.05499) References ========== • Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L., Shum, H.: DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. • Ziu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., Zhang, L.: Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection. • Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: PRe-training of Deep Bidirectional Transformers for Language Understanding. • Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. License ======= The licenses to use this model is covered by the Model EULA. By downloading the unpruned or pruned version of the model, you accept the terms and conditions of theselicenses licenses (https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/) Model Architecture ================== Architecture Type:Transformer-based Network Architecture Architecture Type: Network Architecture • Backbone: Window attention based Transformers called Swin-Tiny and BERT-Base text encoder. • Encoder: 6 layers of Multi-headed attention with deformable attention layers. • Decoders: 6 layers of Multi-headed attention with deformable attention layers. More Details • The models in this instance are object detectors that take RGB images and list of phrases as input and produce bounding boxes and classes as output. More specifically, this model was trained withSwin-Tiny backbonethat was trained using a supervised manner on NVIDIA proprietary data called NVImageNetv2, which allows commercial usage. In addition, BERT-Base was used as the starting weight for text tower. Finally, Grounding DINO was trained end-to-end on about 1.8M images that were collected from publicly available datasets. Note that we ensured that all the raw images used during training have commercial licenses to ensure safe commercial usage. Swin-Tiny backbone (https://arxiv.org/pdf/2103.14030) Input ----- Input Type(s):Image and list of captions of tokenized through HuggingFaceInput Format(s):Red, Green, Blue (RGB) and tokenized inputs. Can support any input resolution and images do not need any additional pre-processing (e.g. alpha channels or bits)Input Parameters:Multiple dimensions. See below for detailed input shapesOther Properties Related to Input: Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: • inputs: B X 3 X 544 X 960 (Batch Size x Channel x Height x Width) • input_ids: B x 256 (Batch Size x Max Token Length ) • attention_mask: B x 256 (Batch Size x Max Token Length ) • position_ids: B x 256 (Batch Size x Max Token Length ) • token_type_ids: B x 256 (Batch Size x Max Token Length ) • text_token_mask: B x 256 x 256 (Batch Size x Max Token Length x Max Token Length) • Because ONNX / TensorRT can't take string as input, we've offloaded tokenizer outside of the model graph. SeeTAO-Deployrepo on running tokenization through HuggingFace. TAO-Deploy (https://github.com/NVIDIA/tao_deploy) Output ------ Output Type(s):Bounding Boxes and Confidence Scores for each detected object in the input image.Output Formats:One Dimensional (1D), Two Dimensional (2D) vectorsOther Properties Related to Output: Output Type(s): Output Formats: Other Properties Related to Output: • pred_logits: B x 900 (Batch Size x Number of Queries) • pred_boxes: B x 900 x 4 (Batch Size x Number of Queries x Coordinates incxcywhformat) Software Integration ==================== Runtime Engine(s): • TAO - 5.5.0 Supported Hardware Architecture(s): • NVIDIA Ampere • NVIDIA Jetson • NVIDIA Hopper • NVIDIA Lovelace • NVIDIA Pascal • NVIDIA Turing • NVIDIA Volta Supported Operating System(s): • Linux • Linux 4 Tegra Model Versions ============== • grounding_dino_swin_tiny_commercial_trainable_v1.0- Pre-trained Swin-Tiny Grounding DINO model for finetune. grounding_dino_swin_tiny_commercial_trainable_v1.0 • grounding_dino_swin_tiny_commercial_deployable_v1.0- Swin-Tiny Grounding DINO model deployable. grounding_dino_swin_tiny_commercial_deployable_v1.0 Training & Evaluation: ====================== This model was trained using thegrounding_dinoentrypoint in TAO. The training algorithm optimizes the network to minimize the localization and contrastive embedding loss between text and visual features. Using this Model ================ These models need to be used with NVIDIA hardware and software. For hardware, the models can run on any NVIDIA GPU including NVIDIA Jetson devices. These models can only be used withTrain Adapt Optimize (TAO) Toolkit, orTensorRT. Train Adapt Optimize (TAO) Toolkit (https://developer.nvidia.com/tao-toolkit) TensorRT (https://developer.nvidia.com/tensorrt) The intended use for these models is detecting objects in a color (RGB) image. The model can be used to detect objects from photos and videos by using appropriate video or image decoding and pre-processing. These models are intended for training and fine-tune with the TAO Toolkit and your datasets for object detection. High-fidelity models can be trained with new use cases. A Jupyter Notebook is available as a part of the pytorchTAO containerand can be used to re-train. TAO container (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/containers/tao-toolkit/tags) The models are also intended for easy edge deployment using TensorRT. Training Dataset ================ Training Data ------------- Grounding DINO was pretrained on wide range of commercial datasets where the annotations were either human generated or pseudo-labeled. The model was trained on 1,815,237 images and 14,794,974 instances of both object detection (OD) and grounding annotations. Please refer to below section for details of every dataset used to train Grounding DINO. Evaluation Data =============== Data Collection Method by dataset: • Unknown Labeling Method by dataset: • Human. Properties: • COCO validation dataset with 5,000 images. Methodology and KPI ------------------- The key performance indicator is the mean average precision (mAP), following the standard evaluation protocol for object detection. The KPI for the evaluation data are: Model Limitation ================ Grounding DINO was trained on images collected from the web and text data of everyday noun phrases. The model might not perform well on different data distributions. Conducting further fine-tuning on the target domain is recommended to get a higher mAP. Technical Blogs =============== • Train like a ‘pro’ without being an AI expert using TAO AutoML Train like a ‘pro’ without being an AI expert using TAO AutoML (https://developer.nvidia.com/blog/training-like-an-ai-pro-using-tao-automl/) • Create Custom AI models using NVIDIA TAO Toolkit with Azure Machine Learning Create Custom AI models using NVIDIA TAO Toolkit with Azure Machine Learning (https://developer.nvidia.com/blog/creating-custom-ai-models-using-nvidia-tao-toolkit-with-azure-machine-learning/) • Developing and Deploying AI-powered Robots with NVIDIA Isaac Sim and NVIDIA TAO Developing and Deploying AI-powered Robots with NVIDIA Isaac Sim and NVIDIA TAO (https://developer.nvidia.com/blog/developing-and-deploying-ai-powered-robots-with-nvidia-isaac-sim-and-nvidia-tao/) • Learn endless ways to adapt and supercharge your AI workflows with TAO -Whitepaper Whitepaper (https://developer.nvidia.com/tao-toolkit-usecases-whitepaper/1-introduction) • Customize Action Recognition with TAO and deploy with DeepStream Customize Action Recognition with TAO and deploy with DeepStream (https://developer.nvidia.com/blog/developing-and-deploying-your-custom-action-recognition-application-without-any-ai-expertise-using-tao-and-deepstream/) • Read the two-part blog on training and optimizing 2D body pose estimation model with TAO -Part 1|Part 2 Part 1 (https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-1) Part 2 (https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-2) • Learn how to train areal-time License plate detection and recognition appwith TAO and DeepStream. real-time License plate detection and recognition app (https://developer.nvidia.com/blog/creating-a-real-time-license-plate-detection-and-recognition-app) • Model accuracy is extremely important; learn how you can achievestate of the art accuracy for classification and object detection modelsusing TAO. state of the art accuracy for classification and object detection models (https://developer.nvidia.com/blog/preparing-state-of-the-art-models-for-classification-and-object-detection-with-tao-toolkit/) Suggested Reading ================= • More information on TAO Toolkit and pre-trained models can be found at theNVIDIA Developer Zone NVIDIA Developer Zone (https://developer.nvidia.com/tao-toolkit) • Refer to theTAO documentation TAO documentation (https://docs.nvidia.com/tao/tao-toolkit/index.html) • Read theTAO Toolkit Quick Start Guideandrelease notes. TAO Toolkit Quick Start Guide (https://docs.nvidia.com/tao/tao-toolkit/index.html) release notes (https://docs.nvidia.com/tao/tao-toolkit/text/release_notes.html) • If you have any questions or feedback, please refer to the discussions on theTAO Toolkit Developer Forums TAO Toolkit Developer Forums (https://forums.developer.nvidia.com/c/accelerated-computing/intelligent-video-analytics/tao-toolkit/17) • Deploy your models for video analytics application using theDeepStream SDK. DeepStream SDK (https://developer.nvidia.com/deepstream-sdk) • Deploy your models inRivafor ConvAI use case. Riva (https://developer.nvidia.com/riva) Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Promise and the Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "rakutenai-7b-chat",
    "nvidia_url": "https://build.nvidia.com/rakuten/rakutenai-7b-chat",
    "scraped_on": "2025-10-15T21:42:59.888679",
    "tags": [
      "chat",
      "language generation",
      "large language models",
      "rakuten",
      "text-to-text"
    ],
    "model_card": "Model Description ================= Rakuten AI 7b chat model is a conversational AI system designed for seamless, natural interactions. With 7 billion parameters, it excels in understanding and generating human-like text in both English and Japanese. Its extensive training dataset includes diverse sources, making it suitable for customer service, virtual assistants, and other applications. Optimized for performance, the Rakutan AI 7b enhances user experience with advanced language processing capabilities and robust multilingual support, particularly excelling in Japanese-language use cases. This model is ready for commercial use. Model DeveloperRakuten Group, Inc. Model Developer LicenseThis model is licensed underApache License, Version 2.0. License Apache License, Version 2.0. (https://www.apache.org/licenses/LICENSE-2.0) Language(s)Japanese, English Language(s) Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIA7B Chat Model Card. 7B Chat Model Card (https://huggingface.co/Rakuten/RakutenAI-7B-chat) References: =========== RakutenAI 7B chatModel Cardon Hugging FaceRakutenAI 7B chatpaper Model Card (https://huggingface.co/Rakuten/RakutenAI-7B-chat) paper (https://arxiv.org/abs/2403.15484) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Mistral-7BModel Version:0.1 Architecture Type: Network Architecture: Model Version: Input: ====== Input Type(s):TextInput Format:StringInput Parameters:Max Tokens, Temperature, Top P Input Type(s): Input Format: Input Parameters: Limitations & Bias: ------------------- The suite of RakutenAI-7B models is capable of generating human-like text on a wide range of topics. However, like all LLMs, they have limitations and can produce biased, inaccurate, or unsafe outputs. Please exercise caution and judgement while interacting with them. Output: ======= Output Type(s):TextOutput Format:String Output Type(s): Output Format: Software Integration: ===================== Supported Hardware Platform(s):NVIDIA LovelaceSupported Operating System(s):Linux Supported Hardware Platform(s): Supported Operating System(s): Training Datasets: ================== • JSNLI JSNLI (https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E6%9C%AC%E8%AA%9ESNLI%28JSNLI%29%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88) • RTE RTE (https://nlp.ist.i.kyoto-u.ac.jp/?Textual+Entailment+%E8%A9%95%E4%BE%A1%E3%83%87%E3%83%BC%E3%82%BF) • KUCI KUCI (https://nlp.ist.i.kyoto-u.ac.jp/?KUCI) • BELEBELE BELEBELE (https://huggingface.co/datasets/facebook/belebele) • JCS JCS (https://aclanthology.org/2022.lrec-1.317/) • JNLI JNLI (https://aclanthology.org/2022.lrec-1.317/) • Dolly-15K Dolly-15K (https://huggingface.co/datasets/databricks/databricks-dolly-15k) • OpenAssistant1 OpenAssistant1 (https://huggingface.co/datasets/OpenAssistant/oasst1) Inference: ========== Engine:Triton Inference ServerTest Hardware:NVIDIA L40 Systems Engine: Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "rakutenai-7b-instruct",
    "nvidia_url": "https://build.nvidia.com/rakuten/rakutenai-7b-instruct",
    "scraped_on": "2025-10-15T21:42:56.772706",
    "tags": [
      "chat",
      "language generation",
      "large language models",
      "rakuten",
      "text-to-text"
    ],
    "model_card": "Model Description ================= RakutenAI-7B is a systematic initiative that brings the latest technologies to the world of Japanese LLMs. RakutenAI-7B achieves the best scores on the Japanese language understanding benchmarks while maintaining a competitive performance on the English test sets among similar models such as OpenCalm, Elyza, Youri, Nekomata and Swallow. RakutenAI-7B leverages the Mistral model architecture and is based on Mistral-7B-v0.1 pre-trained checkpoint, exemplifying a successful retrofitting of the pre-trained model weights. Moreover, we extend Mistral's vocabulary from 32k to 48k to offer a better character-per-token rate for Japanese. This model is ready for commercial use. Model DeveloperRakuten Group, Inc. Model Developer LicenseThis model is licensed underApache License, Version 2.0. License Apache License, Version 2.0. (https://www.apache.org/licenses/LICENSE-2.0) Language(s)Japanese, English Language(s) Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIA7B Instruct Model Card. 7B Instruct Model Card (https://huggingface.co/Rakuten/RakutenAI-7B-instruct) References: =========== RakutenAI 7B instructModel Cardon Hugging FaceRakutenAI 7B instructpaper Model Card (https://huggingface.co/Rakuten/RakutenAI-7B-instruct) paper (https://arxiv.org/abs/2403.15484) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Mistral-7BModel Version:0.1 Architecture Type: Network Architecture: Model Version: Input: ====== Input Type(s):TextInput Format:StringInput Parameters:Max Tokens, Temperature, Top P Input Type(s): Input Format: Input Parameters: Limitations & Bias: ------------------- The suite of RakutenAI-7B models is capable of generating human-like text on a wide range of topics. However, like all LLMs, they have limitations and can produce biased, inaccurate, or unsafe outputs. Please exercise caution and judgement while interacting with them. Output: ======= Output Type(s):TextOutput Format:String Output Type(s): Output Format: Software Integration: ===================== Supported Hardware Platform(s):NVIDIA LovelaceSupported Operating System(s):Linux Supported Hardware Platform(s): Supported Operating System(s): Training Datasets: ================== • JSNLI JSNLI (https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E6%9C%AC%E8%AA%9ESNLI%28JSNLI%29%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88) • RTE RTE (https://nlp.ist.i.kyoto-u.ac.jp/?Textual+Entailment+%E8%A9%95%E4%BE%A1%E3%83%87%E3%83%BC%E3%82%BF) • KUCI KUCI (https://nlp.ist.i.kyoto-u.ac.jp/?KUCI) • BELEBELE BELEBELE (https://huggingface.co/datasets/facebook/belebele) • JCS JCS (https://aclanthology.org/2022.lrec-1.317/) • JNLI JNLI (https://aclanthology.org/2022.lrec-1.317/) • Dolly-15K Dolly-15K (https://huggingface.co/datasets/databricks/databricks-dolly-15k) • OpenAssistant1 OpenAssistant1 (https://huggingface.co/datasets/OpenAssistant/oasst1) Inference: ========== Engine:Triton Inference ServerTest Hardware:NVIDIA L40 Systems Engine: Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nv-dinov2",
    "nvidia_url": "https://build.nvidia.com/nvidia/nv-dinov2",
    "scraped_on": "2025-10-15T21:42:53.648426",
    "tags": [
      "computer vision",
      "deepstream",
      "image-to-embedding",
      "nvidia",
      "nvidia nim",
      "object classification"
    ],
    "model_card": "TAO Commercial Pretrained NV-Dinov2 Model ========================================= Model Overview ============== This model card contains pretrained weights of NV-Dinov2 model which can be used as a backbone for most of the popular computer vision tasks such as Classification, Segmentation, Detection. This model is ready for commercial use. These weights that may be used as a starting point with the classification, segmentation, detection, change detection applications in Train Adapt Optimize (TAO) Toolkit to facilitate transfer learning. References ========== • Oquab, Maxime, et al. \"Dinov2: Learning robust visual features without supervision.\" arXiv preprint arXiv:2304.07193 (2023). Model Architecture ================== Architecture Type:Transformer-Based Architecture Type: NV-Dinov2 is a visual foundational model trained on NVIDIA proprietary large scale dataset.Dinov2is a self-supervised learning method that uses a combination of two SSL techniques : DINO and iBOT. These models simplify the use of images in any system by producing all purpose visual features, that is, features that work across image distributions and tasks without finetuning. Trained on large curated datasets, our model has learnt robust fine-grained representation useful for localization and classification tasks. This model can be used as a foundation model for a variety of downstream tasks with few labeled examples. For more details on the method see: Dinov2. Dinov2 (https://arxiv.org/abs/2304.07193) Input: ====== Input Types:ImagesInput Formats:Red, Green, Blue (RGB)Input Parameters:Three-Dimensional (3D)Other Properties Related to Input:Minimum Resolution:224 x 224Maximum Resolution:518 x 518Alpha Channel:No alpha channel Input Types: Input Formats: Input Parameters: Other Properties Related to Input: Minimum Resolution: Maximum Resolution: Alpha Channel: • Input image format: RGB Image of dimensions: 224 X 224 X 3 (H x W x C) Note: ViT-G was fine-tuned for high-resolution images. It works for any input resolutions between 224 X 224 X 3 -> 518 x 518 x 3. Channel Ordering of the Input: NCHW, where N = Batch Size, C = number of channels (3), H = Height of images (336), W = Width of the images (336) Output: ======= Output Types:Embedding - Float tensorOutput Format:3D VectorOther Properties Related to Output:Alpha Channel:No alpha channel Output Types: Output Format: Other Properties Related to Output: Alpha Channel: The output of this model is an embedding of an input image of size 1024 for ViT-L variant and 1536 for ViT-G. Software Integration: ===================== Runtime Engines: • TAO - 5.2 Supported Hardware Architectures: • NVIDIA Ampere • NVIDIA Jetson • NVIDIA Hopper • NVIDIA Lovelace • NVIDIA Pascal • NVIDIA Turing • NVIDIA Volta Supported Operating Systems: • Linux • Linux 4 Tegra Model Versions: =============== • NV-DinoV2-224-V0- 130M internal dataset pre-trained model at 224 resolution. NV-DinoV2-224-V0 • NV-DinoV2-518-V0- 700M internal dataset pre-trained model at 518 resolution. NV-DinoV2-518-V0 Training & Evaluation ===================== This model was trained using our implementation of DINOV2 on NVIDIA-commercial dataset. Using this Model ================ These models need to be used with NVIDIA hardware and software. For hardware, the models can run on any NVIDIA GPU including NVIDIA Jetson devices. These models can only be used with theTrain Adapt Optimize (TAO) Toolkit, orTensorRT. Train Adapt Optimize (TAO) Toolkit (https://developer.nvidia.com/tao-toolkit) TensorRT (https://developer.nvidia.com/tensorrt) The primary use case for these models is getting feature embeddings from images. These embeddings can then be used for downstream tasks such as classification, segmentation, and detection by adding relevant heads. These models are intended for training and fine-tuning using the TAO Toolkit and your datasets for image comparison. High-fidelity models can be trained on new use cases. A Jupyter Notebook is available as a part of theTAO containerand can be used to re-training. TAO container (https://ngc.nvidia.com/catalog/containers/nvidia:tao:tao-toolkit) The models are also intended for edge deployment using TensorRT. Using the Model with TAO ------------------------ To use these models as pretrained weights for transfer learning, use the snippet below as a template for themodelandtraincomponent of the experiment spec file to train a Dinov2 Classification model. For more information on the experiment spec file, see theTAO Toolkit User Guide. TAO Toolkit User Guide (https://docs.nvidia.com/tao/tao-toolkit/index.html) For ViT-L NV-Dinov2: Training Dataset ================ Data Collection Method by dataset: • Automated Labeling Method by dataset: • Automated Properties: Evaluation Dataset: =================== Link:https://www.image-net.org/ Link: https://www.image-net.org/ Data Collection Method by dataset: • Unknown Labeling Method by dataset: • Unknown Properties:50,000 validatio images fromImageNet dataset Properties: ImageNet dataset (https://www.image-net.org/download.php) Methodology and KPI ------------------- The key performance indicator is the accuracy, following the standard evaluation protocol for image classification. The KPI for the evaluation data are reported below. Inference: ========== Engine:Tensor(RT)Test Hardware: Engine: Test Hardware: • A2 • A30 • DGX H100 • DGX A100 • L4 • L40 • NVIDIA T4 • AGX Orin 64GB • Orin NX16GB • T4 The inference is run on the provided unpruned model at FP16 precision. The inference performance is run usingtrtexecon Orin, Orin NX and NVIDIA T4, and Ampere GPUs. The Jetson devices are running at Max-N configuration for maximum GPU frequency. The performance shown here is the inference only performance. The end-to-end performance with streaming video data might vary depending on other bottlenecks in the hardware and software. trtexec (https://github.com/NVIDIA/TensorRT/tree/main/samples/trtexec) NVDinoV2 (224x224 resolution) Technical Blogs =============== • Learn how to transformIndustrial Defect Detection with NVIDIA TAO and Vision AI Models. Industrial Defect Detection with NVIDIA TAO and Vision AI Models (https://developer.nvidia.com/blog/transforming-industrial-defect-detection-with-nvidia-tao-and-vision-ai-models/) • Read the 2 part blog on training and optimizing 2D body pose estimation model with TAO -Part 1|Part 2. Part 1 (https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-1) Part 2 (https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-2) • Learn how to trainreal-time license plate detection and recognition appwith TAO and DeepStream. real-time license plate detection and recognition app (https://developer.nvidia.com/blog/creating-a-real-time-license-plate-detection-and-recognition-app) • Model accuracy is extremely important, learn how you can achievestate of the art accuracy for classification and object detection modelsusing TAO. state of the art accuracy for classification and object detection models (https://developer.nvidia.com/blog/preparing-state-of-the-art-models-for-classification-and-object-detection-with-tao-toolkit/) • Learn how to train theInstance segmentation model using MaskRCNN with TAO. Instance segmentation model using MaskRCNN with TAO (https://developer.nvidia.com/blog/training-instance-segmentation-models-using-maskrcnn-on-tao-toolkit/) • Read the technical tutorial on howPeopleNet model can be trained with custom data using Transfer Learning Toolkit. PeopleNet model can be trained with custom data using Transfer Learning Toolkit (https://devblogs.nvidia.com/training-custom-pretrained-models-using-tlt/) • Learn how totrain and deploy real-time intelligent video analytics apps and services using DeepStream SDK. train and deploy real-time intelligent video analytics apps and services using DeepStream SDK (https://developer.nvidia.com/blog/building-iva-apps-using-deepstream-5-0-updated-for-ga/) Suggested Reading ================= • More information on about TAO Toolkit and pre-trained models can be found at theNVIDIA Developer Zone. NVIDIA Developer Zone (https://developer.nvidia.com/tao-toolkit) • Read theTAO Quick Startguide andrelease notes. TAO Quick Start (https://docs.nvidia.com/tao/tao-toolkit/text/quick_start_guide/index.html) release notes (https://docs.nvidia.com/tao/tao-toolkit/text/release_notes.html) • If you have any questions or feedback, see the discussions onTAO Toolkit Developer Forums. TAO Toolkit Developer Forums (https://forums.developer.nvidia.com/c/accelerated-computing/intelligent-video-analytics/tao-toolkit/17) • Deploy your model on the edge using DeepStream. Learn more aboutDeepStream SDK. DeepStream SDK (https://developer.nvidia.com/deepstream-sdk) Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Promise and the Explainability, Bias, Safety & Security, and Privacy Subcards."
  },
  {
    "name": "phi-3.5-mini-instruct",
    "nvidia_url": "https://build.nvidia.com/microsoft/phi-3_5-mini",
    "scraped_on": "2025-10-15T21:42:50.521429",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "large language models",
      "microsoft",
      "text-to-text"
    ],
    "model_card": "Model Summary ------------- Description Release date License MIT (https://opensource.org/license/mit) Architecture Inputs Context length Outputs Status Supported languages Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. Resources --------- 🏡Phi-3 Portal📰Phi-3 Microsoft Blog📖Phi-3 Technical Report🛠️Phi-3 on Azure AI Studio👩‍🍳Phi-3 Cookbook Phi-3 Portal (https://azure.microsoft.com/en-us/products/phi-3) Phi-3 Microsoft Blog (https://aka.ms/phi3.5-techblog) Phi-3 Technical Report (https://arxiv.org/abs/2404.14219) Phi-3 on Azure AI Studio (https://aka.ms/phi3-azure-ai) Phi-3 Cookbook (https://github.com/microsoft/Phi-3CookBook) Release Notes ============= This is an update over the June 2024 instruction-tuned Phi-3-mini release based on valuable user feedback. The model used better post-training techniques and additional data leading to substantial gains on multilingual, multi-turn conversation quality, and reasoning capability. We believe most use cases will benefit from this release, but we encourage users to test in their particular AI applications. We appreciate the enthusiastic adoption of the Phi-3 model family, and continue to welcome all feedback from the community. Intended Use ============ Primary Use Cases ----------------- The model is intended for broad commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require: • Memory/compute constrained environments • Latency bound scenarios • Strong reasoning (especially code, math and logic) Our model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. Out-of-Scope Use Cases ---------------------- Our models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models, as well as performance difference across languages, as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. Usage ===== Input Formats ------------- Given the nature of the training data, the Phi-3.5-mini-instruct model is best suited for prompts using the chat format as follows: Loading the model locally ------------------------- After obtaining the Phi-3.5-mini-instruct model checkpoints, users can use this sample code for inference. Model Version(s): ================= [Use unique identifier for identifying the model in the future- this may be part of our internal naming, specifying variation like \"pruned,\" \"unpruned,\" \"trained,\" or \"deployable\" or \"quantized\" where necessary and including a versioning number like vX.X along with short description differentiating if multiple versions are available] Datasets ======== Dates Training time Training data Our training data includes a wide variety of sources, totaling 3.4 trillion tokens, and is a combination of • publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; • newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); • high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. We are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in thePhi-3 Technical Report. Phi-3 Technical Report (https://arxiv.org/pdf/2404.14219) Multilingual ------------ The table below highlights multilingual capability of Phi-3.5-mini on multilingual MMLU, MEGA, and multilingual MMLU-pro datasets. Overall, we observed that even with just 3.8B active parameters, the model is very competitive on multilingual tasks in comparison to other models with a much bigger active parameters. Average 55.2 52.3 47.9 55.3 47.5 59.6 64.3 76.6 Long Context ------------ Phi-3.5-mini supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, long document information retrieval. Phi-3.5-mini outperforms Gemma-2 family which only supports 8K context length and is competitive with other much larger open-weight models such as Llama-3.1-8B-Instruct, Mistral-7B-Instruct-v0.3, and Mistral-Nemo-12B-Instruct-2407. Average 26.1 25.5 24.4 24.5 27.0 25.4 RULER: a retrieval-based benchmark for long context understanding Phi-3.5-mini-instruct 84.1 Llama-3.1-8B-instruct 88.3 Mistral-Nemo-12B-instruct-2407 66.2 RepoQA: a benchmark for long context code understanding Phi-3.5-mini-instruct 77 Llama-3.1-8B-instruct 71 Mistral-7B-instruct-v0.3 62 Inference: ========== Engine:Tensor(RT)Test Hardware [Name the specific test hardware model]: Engine: Test Hardware [Name the specific test hardware model]: • 512 A100-80G Responsible AI Considerations ============================= Like other models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: • Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. • Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. • Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. • Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. • Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses. Developers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: • Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. • High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. • Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). • Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. • Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. • Identification of individuals: models with vision capabilities may have the potential to uniquely identify individuals in images. Safety post-training steers the model to refuse such requests, but developers should consider and implement, as appropriate, additional mitigations or user consent flows as required in their respective jurisdiction, (e.g., building measures to blur faces in image inputs before processing). Securities and AI Concerns: =========================== Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Trademarks ========== This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must followMicrosoft’s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies. Microsoft’s Trademark & Brand Guidelines (https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks)"
  },
  {
    "name": "mistral-nemo-minitron-8b-base",
    "nvidia_url": "https://build.nvidia.com/nvidia/mistral-nemo-minitron-8b-base",
    "scraped_on": "2025-10-15T21:42:47.289818",
    "tags": [
      "chat",
      "language generation",
      "nvidia",
      "small language model",
      "text-to-text"
    ],
    "model_card": "Mistral-NeMo-Minitron-8B-Base ============================= Model Overview ============== Mistral-NeMo-Minitron-8B-Base is a base text-to-text model that can be adopted for a variety of natural language generation tasks. It is a large language model (LLM) obtained by pruning and distilling the Mistral-NeMo 12B; specifically, we prune the embedding dimension and MLP intermediate dimension in the model. Following pruning, we perform continued training with distillation using 380 billion tokens to arrive at the final model; we use the continuous pre-training data corpus used in Nemotron-4 15B for this purpose. Model Developer:NVIDIA Model Developer: Model Dates:Mistral-NeMo-Minitron-8B-Base was trained between July 24, 2024 and August 10, 2024. Model Dates: License ======= This model is released under theNVIDIA Open Model License Agreement. Additional Terms:Apache 2.0 License Agreement. NVIDIA Open Model License Agreement (https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf) Apache 2.0 License Agreement (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) Model Architecture ================== Mistral-NeMo-Minitron-8B-Base uses a model embedding size of 4096, 32 attention heads, MLP intermediate dimension of 11520, with 40 layers in total. Additionally, it uses Grouped-Query Attention (GQA) and Rotary Position Embeddings (RoPE). Architecture Type:Transformer Decoder (Auto-Regressive Language Model) Architecture Type: Network Architecture:Mistral-NeMo Network Architecture: Input Type(s):Text Input Type(s): Input Format(s):String Input Format(s): Input Parameters:One Dimensional (1D) Input Parameters: Other Properties Related to Input:Works well within 8k characters or less. Other Properties Related to Input: Output Type(s):Text Output Type(s): Output Format:String Output Format: Output Parameters:1D Output Parameters: Other Properties Related to Output:None Other Properties Related to Output: Software Integration ==================== Runtime Engine(s): • NeMo 24.05 Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace Operating System(s): • Linux Dataset & Training ================== Data Collection Method by Dataset:Automated Data Collection Method by Dataset: Labeling Method by Dataset:Not Applicable Labeling Method by Dataset: Properties:The training corpus for Mistral-NeMo-Minitron-8B-Base consists of English and multilingual text, as well as code. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. The corpus spans domains including legal, math, science, finance, and more. In our continued training set, we introduce a small portion of question-answering, and alignment style data to improve model performance. Properties: Data Freshness:Training was done in 2024, the pretraining data has a cutoff of June 2023. Data Freshness: Evaluation Results ================== 5-shot performance.Language Understanding evaluated usingMassive Multitask Language Understanding: 5-shot performance. Massive Multitask Language Understanding (https://arxiv.org/abs/2009.03300) Zero-shot performance.Evaluated using select datasets from theLM Evaluation Harnesswith additions: Zero-shot performance. LM Evaluation Harness (https://github.com/EleutherAI/lm-evaluation-harness) Code generation performance.Evaluated usingMBPP: Code generation performance. MBPP (https://github.com/google-research/google-research/tree/master/mbpp) Inference ========= Engine:TensorRT-LLM Engine: Test Hardware:NVIDIA A100 Test Hardware: DType:BFloat16 DType: Limitations =========== The model was trained on data that contains toxic language, unsafe content, and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive. Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) References ========== • Compact Language Models via Pruning and Knowledge Distillation Compact Language Models via Pruning and Knowledge Distillation (https://arxiv.org/abs/2407.14679) • LLM Pruning and Distillation in Practice LLM Pruning and Distillation in Practice"
  },
  {
    "name": "nemotron-mini-4b-instruct",
    "nvidia_url": "https://build.nvidia.com/nvidia/nemotron-mini-4b-instruct",
    "scraped_on": "2025-10-15T21:42:46.837193",
    "tags": [
      "chat",
      "language generation",
      "nvidia",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description: ============ Nemotron-Mini-4B Instruct is a model for generating responses for roleplaying, retrieval augmented generation, and function calling. It is a small language model (SLM) optimized through distillation, pruning and quantization for speed and on-device deployment. VRAM usage has been minimized to approximately 2 GB, providing significantly faster time to first token compared to LLMs. This model is ready for commercial use. License/Terms of Use: --------------------- NVIDIA AI Foundation Models Community License Agreement NVIDIA AI Foundation Models Community License Agreement (https://developer.nvidia.com/downloads/nv-ai-foundation-models-license) References ========== Please refer to theUser Guideto use the model and use a suggested guideline for prompts. User Guide Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Decoder-only Architecture Type: Network Architecture: Limitations =========== The model was trained on data that contains toxic language and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive. This issue could be exacerbated without the use of the recommended prompt template. This issue could be exacerbated without the use of the recommended prompt template. Input: ====== Input Type(s):Text (Prompt)Input Format(s):StringInput Parameters:One Dimensional (1D)Other Properties Related to Input:The model has a maximum of 4096 input tokens. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):Text (Response)Output Format:StringOutput Parameters:1DOther Properties Related to Output:The model has a maximum of 4096 input tokens. Maximum output for both versions can be set apart from input. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Prompt Format: ============== We recommend using the following prompt template, which was used to fine-tune the model. The model may not perform optimally without it. Single Turn Tool use Software Integration: (On-Device) ================================= Runtime(s):AI Inference Manager (NVAIM) Version 1.0.0Toolkit:NVAIMSeethis documentfor details on how to integrate the model into NVAIM. Runtime(s): Toolkit: this document Supported Hardware Platform(s):GPU supporting DirectX 11/12 and Vulkan 1.2 or higher Supported Hardware Platform(s): [Preferred/Supported] Operating System(s): • Windows Software Integration: (Cloud) ============================= Toolkit:NVIDIA NIMSeethis documentfor details on how to integrate the model into NVAIM. Toolkit: this document [Preferred/Supported] Operating System(s): • Linux Model Version(s) ---------------- Nemotron-4-4B-instruct Training & Evaluation: ====================== Training Dataset: ================= ** Data Collection Method by dataset • Hybrid: Automated, Human ** Labeling Method by dataset • Hybrid: Automated, Human Properties (Quantity, Dataset Descriptions, Sensor(s)): Trained on approximately 10000 Game/Non-Playable Character (NPC) dialog turns from domain chat data. Evaluation Dataset: =================== ** Data Collection Method by dataset • Hybrid: Automated, Human ** Labeling Method by dataset • Human Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluated on approximately Game/NPC 1000 dialog turns from domain chat data. Inference: ========== Engine:TRT-LLMTest Hardware [Name the specific test hardware model]: Engine: Test Hardware [Name the specific test hardware model]: • A100 • A10g • H100 • L40s Supported Hardware Platform(s):L40s, A10g, A100, H100 Supported Hardware Platform(s): Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "jamba-1.5-mini-instruct",
    "nvidia_url": "https://build.nvidia.com/ai21labs/jamba-1_5-mini-instruct",
    "scraped_on": "2025-10-15T21:42:46.475568",
    "tags": [
      "ai21labs",
      "chat",
      "language generation",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description: ============ Jamba 1.5 miniis a state-of-the-art, hybrid SSM-Transformer instruction following foundation model. It's a Mixture-of-Expert model with 52B total parameters and 12B active parameters. Jamba 1.5 mini The Jamba family of models are the most powerful & efficient long-context models on the market, and the only ones with an effective context window of 256K. For long context input, they deliver up to 2.5X faster inference than leading models of comparable sizes. Jamba supports function calling/tool use, structured output (JSON), and grounded generation with citation mode and documents API. Jamba officially supports English, French, Spanish, Portuguese, German, Arabic and Hebrew, but can also work in many other languages. Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. Jamba 1.5 mini is developed by AI21 Labs and is available under the Jamba Open Model License for research and non-commercial use. For commercial use requiring self-deployment, a Jamba Commercial License must be acquired by contacting AI21 Labs. Terms of Use ============ GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service; and the use of this model is governed by theJamba Open License Agreement. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) Jamba Open License Agreement (https://assets.ngc.nvidia.com/products/api-catalog/legal/Jamba_Open_Model_License_Agreement.pdf) References(s): ============== Jamba 1.5blogpost blogpost (https://www.ai21.com/blog/announcing-jamba-model-family) Model Architecture: =================== Architecture Type:Jamba (Joint Attention Mamba)Network Architecture:JambaModel Version:1.5 Architecture Type: Network Architecture: Model Version: Input: ====== Input Type:TextInput Format:StringInput Parameters:One Dimensional (1D), Max Tokens, Temperature, Top PMax Input Tokens:256,000 Input Type: Input Format: Input Parameters: Max Input Tokens: Output: ======= Output Type:TextOutput Format:StringOutput Parameters:1DMax Output Tokens:256,000 Output Type: Output Format: Output Parameters: Max Output Tokens: Software Integration: ===================== Supported Hardware Platform(s):NVIDIA Ampere, NVIDIA HopperSupported Operating System(s):Linux Supported Hardware Platform(s): Supported Operating System(s): Benchmarks: =========== Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "phi-3.5-vision-instruct",
    "nvidia_url": "https://build.nvidia.com/microsoft/phi-3_5-vision-instruct",
    "scraped_on": "2025-10-15T21:42:43.320161",
    "tags": [
      "image-to-text",
      "language generation",
      "microsoft",
      "vision assistant",
      "visual question answering"
    ],
    "model_card": "Model Summary ------------- Description Release date License MIT (https://opensource.org/license/mit) Architecture Inputs Context length Outputs Status Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. Resources --------- 🏡Phi-3 Portal📰Phi-3 Microsoft Blog📖Phi-3 Technical Report🛠️Phi-3 on Azure AI Studio👩‍🍳Phi-3 Cookbook Phi-3 Portal (https://azure.microsoft.com/en-us/products/phi-3) Phi-3 Microsoft Blog (https://aka.ms/phi3.5-techblog) Phi-3 Technical Report (https://arxiv.org/abs/2404.14219) Phi-3 on Azure AI Studio (https://aka.ms/phi3-azure-ai) Phi-3 Cookbook (https://github.com/microsoft/Phi-3CookBook) Release Notes ============= In this release, the model enables multi-frame image understanding and reasoning which is based on valuable customer feedback. The hero example multi-frame capabilities include detailed image comparison, multi-image summarization/storytelling and video summarization, which have broad applications in many scenarios. We also observed performance improvement on most single image benchmarks, e.g., boosting MMMU performance from 40.2 to 43.0, MMBench performance from 80.5 to 81.9, document understanding benchmark TextVQA from 70.9 to 72.0. We believe most use cases will benefit from this release, but we encourage users to test the new model in their AI applications. We appreciate the enthusiastic adoption of the Phi-3 model family and continue to welcome all the feedback from the community. Intended Use ============ Primary Use Cases ----------------- The model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require: • Memory/compute constrained environments • Latency bound scenarios • General image understanding • Optical character recognition • Chart and table understanding • Multiple image comparison • Multi-image or video clip summarization Our model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. Out-of-Scope Use Cases ---------------------- Our models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. Usage ===== Input Formats ------------- Given the nature of the training data, the Phi-3.5-vision model is best suited for prompts using the chat format as follows: Single image: Multi-turn conversations: For multi-image usage, add multiple image placeholders in the front of the prompts. <|image_{}|> index should start from 1. One example of prompt is shown as follows: Loading the model locally ------------------------- After obtaining the Phi-3.5-vision-instruct model checkpoints, users can use this sample code for inference. Model Version(s): ================= [Use unique identifier for identifying the model in the future- this may be part of our internal naming, specifying variation like \"pruned,\" \"unpruned,\" \"trained,\" or \"deployable\" or \"quantized\" where necessary and including a versioning number like vX.X along with short description differentiating if multiple versions are available] Datasets ======== Dates Training time Training data Our training data includes a wide variety of sources, and is a combination of • publicly available documents filtered rigorously for quality, selected high-quality educational data and code; • selected high-quality image-text interleave data; • newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides, newly created multi-image and video data, e.g., short video clips/pair of two similar images; • high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. The data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data. Below are the comparison results on existing multi-image benchmarks. On average, our model outperforms competitor models on the same size and competitive with much bigger models on multi-frame capabilities and video summarization. BLINK: a benchmark with 14 visual tasks that humans can solve very quickly but are still hard for current multimodal LLMs. Overall 57.0 53.1 45.9 45.4 45.1 51.9 56.5 61.0 63.2 Video-MME: comprehensively assess the capabilities of MLLMs in processing video data, covering a wide range of visual domains, temporal durations, and data modalities. Overall 50.8 50.2 49.9 52.6 62.3 61.2 55.9 62.6 68.4 Inference: ========== Engine:Tensor(RT)Test Hardware [Name the specific test hardware model]: Engine: Test Hardware [Name the specific test hardware model]: • 256 A100-80G Responsible AI Considerations ============================= Like other models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: • Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. • Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. • Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. • Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. • Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses. Developers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: • Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. • High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. • Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). • Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. • Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. • Identification of individuals: models with vision capabilities may have the potential to uniquely identify individuals in images. Safety post-training steers the model to refuse such requests, but developers should consider and implement, as appropriate, additional mitigations or user consent flows as required in their respective jurisdiction, (e.g., building measures to blur faces in image inputs before processing). Security and AI Concerns: ========================= Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Trademarks ========== This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must followMicrosoft’s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies. Microsoft’s Trademark & Brand Guidelines (https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks)"
  },
  {
    "name": "llama-3-swallow-70b-instruct-v0.1",
    "nvidia_url": "https://build.nvidia.com/tokyotech-llm/llama-3-swallow-70b-instruct-v01",
    "scraped_on": "2025-10-15T21:42:40.204955",
    "tags": [
      "chat",
      "large language model",
      "regional language generation",
      "tokyotech-llm"
    ],
    "model_card": "Llama3 Swallow ============== Our Swallow model has undergone continual pre-training from theLlama 3 family, primarily with the addition of Japanese language data. The Instruct versions use supervised fine-tuning (SFT) and Chat Vector. Llama 3 family (https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6) Model Release Updates ===================== We are excited to share the release schedule for our latest models: • July 1, 2024: Released theLlama-3-Swallow-8B-v0.1,Llama-3-Swallow-8B-Instruct-v0.1,Llama-3-Swallow-70B-v0.1, andLlama-3-Swallow-70B-Instruct-v0.1. July 1, 2024 Llama-3-Swallow-8B-v0.1 (https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1) Llama-3-Swallow-8B-Instruct-v0.1 (https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1) Llama-3-Swallow-70B-v0.1 (https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1) Llama-3-Swallow-70B-Instruct-v0.1 (https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1) Swallow Model Index =================== Link (https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-v0.1) Link (https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1) Link (https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-v0.1) Link (https://huggingface.co/tokyotech-llm/Llama-3-Swallow-70B-Instruct-v0.1) This repository provides large language models developed bySwallow-LLM. Read ourblog post. Swallow-LLM (https://swallow-llm.github.io/) blog post (https://zenn.dev/tokyotech_lm/articles/f65989d76baf2c) Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link toNon-NVIDIA Model Card. Non-NVIDIA Model Card (https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md) License ======= META LLAMA 3 COMMUNITY LICENSE META LLAMA 3 COMMUNITY LICENSE (https://llama.meta.com/llama3/license/) Model Details ============= • Model type: Please refer toLlama 3 MODEL_CARDfor details on the model architecture. Model type Llama 3 MODEL_CARD (https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md) • Languages: Japanese English Languages • Library:Megatron-LM Library Megatron-LM (https://github.com/NVIDIA/Megatron-LM) • Tokenizer: Please refer toLlama 3 blogfor details on the tokenizer. Tokenizer Llama 3 blog (https://ai.meta.com/blog/meta-llama-3/) • Contact: swallow[at]nlp.c.titech.ac.jp Contact Model Architecture: =================== Architecture Type:Transformer Architecture Type: Risks and Limitations ===================== The models released here are still in the early stages of our research and development and have not been tuned to ensure outputs align with human intent and safety considerations. Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:One Dimensional (1D) Input Type(s): Input Format(s): Input Parameters: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:1D Output Type(s): Output Format: Output Parameters: Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper • NVIDIA Lovelace Preferred Operating System(s): • Linux Training Dataset: ================= Instruction Tuning ------------------ The following datasets were used for the instruction tuning. • OpenAssistant Conversations Dataset EN top-1 thread OpenAssistant Conversations Dataset EN top-1 thread (https://huggingface.co/datasets/OpenAssistant/oasst2) • OpenAssistant Conversations Datasetwas used, where human utterances are included but the responses are not used. Instead, the responses were generated using theMixtral-8x7B-Instruct-v0.1model. OpenAssistant Conversations Dataset (https://huggingface.co/datasets/llm-jp/oasst1-21k-ja) Mixtral-8x7B-Instruct-v0.1 (https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) Model Performance ================= Japanese tasks -------------- English tasks ------------- MT-Bench JA =========== Evaluation Benchmarks ===================== Japanese evaluation benchmarks ------------------------------ We used llm-jp-eval(v1.3.0), JP Language Model Evaluation Harness(commit #9b42d41) and Code Generation LM Evaluation Harness(commit #0261c52). The details are as follows: • Multiple-choice question answering (JCommonsenseQA [Kurihara et al., 2022]) • Open-ended question answering (JEMHopQA [Ishii et al., 2024]) • Open-ended question answering (NIILC [関根, 2003]) • Machine reading comprehension (JSQuAD [Kurihara et al., 2022]) • Automatic summarization (XL-Sum [Hasan et al., 2021]) • Machine translation (WMT2020 ja-en [Barrault et al., 2020]) • Machine translation (WMT2020 en-ja [Barrault et al., 2020]) • Mathematical reasoning (MGSM [Shi et al., 2023]) • Academic exams (JMMLU [尹ら, 2024]) • Code generation (JHumanEval [佐藤ら, 2024]) English evaluation benchmarks ----------------------------- We used the Language Model Evaluation Harness(v.0.4.2) and Code Generation LM Evaluation Harness(commit #0261c52). The details are as follows: • Multiple-choice question answering (OpenBookQA [Mihaylov et al., 2018]) • Open-ended question answering (TriviaQA [Joshi et al., 2017]) • Machine reading comprehension (SQuAD2 [Rajpurkar et al., 2018]) • Commonsense reasoning (XWINO [Tikhonov and Ryabinin, 2021]) • Natural language inference (HellaSwag [Zellers et al., 2019]) • Mathematical reasoning (GSM8K [Cobbe et al., 2021]) • Reasoning (BBH (BIG-Bench-Hard) [Suzgun et al., 2023]) • Academic exams (MMLU [Hendrycks et al., 2021]) • Code generation (HumanEval [Chen et al., 2021]) MT-Bench JA ----------- We usedJapanese MT-Benchto assess the instruction-following capabilities of models. We utilized the following settings: Japanese MT-Bench (https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_question) • Implemantation: FastChat [Zheng+, 2023] (commit #e86e70d0) • Question:Nejumi LLM-Leaderboard NEO, mtbench_ja_question_v3 Nejumi LLM-Leaderboard NEO, mtbench_ja_question_v3 (https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_question/v3) • Reference Answer:Nejumi LLM-Leaderboard NEO, mtbench_ja_referenceanswer_v1 Nejumi LLM-Leaderboard NEO, mtbench_ja_referenceanswer_v1 (https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_referenceanswer/v1) • Prompt for Judge:Nejumi LLM-Lederboard NEO, mtbench_ja_prompt_v1 Nejumi LLM-Lederboard NEO, mtbench_ja_prompt_v1 (https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_prompt/v1) • Judge:gpt-4-1106-preview • Scoring: Absolute scale normalized to a 0-1 range, averaged over five runs. Inference: ========== Engine:TensorRT-LLMTest Hardware: Engine: Test Hardware: • NVIDIA H100x4 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Authors ======= Here are the team members: • FromTokyo Institute of Technology Okazaki Laboratory, the following members:Naoaki OkazakiSakae MizukiYoumi MaKoki MaedaKakeru HattoriMasanari OhiTaihei ShiotaniKoshiro Saito Tokyo Institute of Technology Okazaki Laboratory (https://www.nlp.c.titech.ac.jp/index.en.html) • Naoaki Okazaki Naoaki Okazaki (https://www.chokkan.org/index.ja.html) • Sakae Mizuki Sakae Mizuki (https://s-mizuki-nlp.github.io/) • Youmi Ma Youmi Ma (https://www.nlp.c.titech.ac.jp/member/youmi.en.html) • Koki Maeda Koki Maeda (https://sites.google.com/view/silviase) • Kakeru Hattori Kakeru Hattori (https://aya-se.vercel.app/) • Masanari Ohi Masanari Ohi (https://sites.google.com/view/masanariohi) • Taihei Shiotani Taihei Shiotani (https://github.com/inatoihs) • Koshiro Saito Koshiro Saito (https://sites.google.com/view/koshiro-saito) • FromTokyo Institute of Technology YOKOTA Laboratory, the following members:Rio YokotaKazuki FujiiTaishi NakamuraTakumi OkamotoIshida Shigeki Tokyo Institute of Technology YOKOTA Laboratory (https://www.rio.gsic.titech.ac.jp/en/index.html) • Rio Yokota Rio Yokota (https://twitter.com/rioyokota) • Kazuki Fujii Kazuki Fujii (https://twitter.com/okoge_kaz) • Taishi Nakamura Taishi Nakamura (https://twitter.com/Setuna7777_2) • Takumi Okamoto Takumi Okamoto (https://www.linkedin.com/in/takumi-okamoto) • Ishida Shigeki Ishida Shigeki (https://www.wantedly.com/id/reborn27) • FromArtificial Intelligence Research Center, AIST, Japan, the following members:Hiroya Takamura Artificial Intelligence Research Center, AIST, Japan (https://www.airc.aist.go.jp/en/teams/) • Hiroya Takamura Hiroya Takamura (https://sites.google.com/view/hjtakamura) How to Cite =========== If you find our work helpful, please feel free to cite us. Citations --------- Acknowledgements ================ We thank Meta Research for releasing Llama 3 under an open license for others to build on. Our project is supported by theLarge Generative AI Development Support Programof the National Institute of Advanced Industrial Science and Technology. Large Generative AI Development Support Program (https://abci.ai/en/link/lfm_support_program.html)"
  },
  {
    "name": "llama-3-taiwan-70b-instruct",
    "nvidia_url": "https://build.nvidia.com/yentinglin/llama-3-taiwan-70b-instruct",
    "scraped_on": "2025-10-15T21:42:36.904685",
    "tags": [
      "chat",
      "code generation",
      "large language models",
      "regional language generation",
      "yentinglin"
    ],
    "model_card": "Model Description ================= Llama-3-Taiwan-70B is a large language model finetuned for Traditional Mandarin and English users. It has strong capabilities in language understanding, generation, reasoning, and multi-turn dialogue. Key features include: • 70B parameters • Languages: Traditional Mandarin (zh-tw), English (en) • Finetuned on High-quality Traditional Mandarin and English corpus covering general knowledge as well as industrial knowledge in legal, manufacturing, medical, and electronics domains • 8K context length • Open model released under the Llama-3 license Llama 3 is a large language AI model comprising a collection of models capable of generating text and code in response to prompts. Meta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. Model DeveloperMeta Model Developer LicenseA custom commercial license is available at:https://llama.meta.com/llama3/license License https://llama.meta.com/llama3/license Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the modelREADME. For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please gohere. README (https://github.com/meta-llama/llama3) here (https://github.com/meta-llama/llama-recipes) Model Release DateApril 18, 2024. Model Release Date StatusThis is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback. Status Llama 3 Family of Models. Llama 3 Family of Models Intended Use ============ Intended Use CasesLlama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. Intended Use Cases Out-of-scopeUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**. Out-of-scope **Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy. Model Architecture • Architecture Type: Transformer • Network Architecture: Llama 3 Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Input • Input Format: Text • Input Parameters: Temperature, TopP Output • Output Format: Text and code • Output Parameters: Max output tokens Training Data Params Context length GQA Token count Knowledge cutoff Hardware and Software ===================== Inference: ========== Engine:TritonTest Hardware: Engine: Test Hardware: • H100 Training FactorsWe used custom training libraries, Meta's Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute. Training Factors Carbon Footprint Pretraining utilized a cumulative7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Meta’s sustainability program. Carbon Footprint Pretraining utilized a cumulative Time (GPU hours) Power Consumption (W) Carbon Emitted(tCO2eq) CO2 emissions during pre-training. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others. CO2 emissions during pre-training Training Dataset ================ OverviewLlama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data. Overview Data FreshnessThe pretraining data has a cutoff of March 2023 for the 7B and December 2023 for the 70B models respectively. Data Freshness Benchmarks ========== In this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology seehere. here (https://github.com/meta-llama/llama3/blob/main/eval_methodology.md) Base pretrained models ---------------------- Category Benchmark Llama 3 8B Llama2 7B Llama2 13B Llama 3 70B Llama2 70B Instruction tuned models ------------------------ Benchmark Llama 3 8B Llama 2 7B Llama 2 13B Llama 3 70B Llama 2 70B Responsibility & Safety ----------------------- We believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community. Foundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications. Rather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience. As part of the Llama 3 release, we updated ourResponsible Use Guideto outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources includingMeta Llama Guard 2andCode Shieldsafeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide aReference Implementationto get you started. Responsible Use Guide (https://llama.meta.com/responsible-use-guide/) Meta Llama Guard 2 (https://llama.meta.com/purple-llama/) Code Shield (https://llama.meta.com/purple-llama/) Reference Implementation (https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai) Llama 3-Instruct ---------------- As outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. Safety For our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous, and interpretable. Refusals In addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. We built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. Responsible Release ------------------- In addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. Misuse If you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found athttps://llama.meta.com/llama3/use-policy/. https://llama.meta.com/llama3/use-policy/ Critical risks -------------- CBRNE(Chemical, Biological, Radiological, Nuclear, and high yield Explosives) We have conducted a two fold assessment of the safety of the model in this area: • Iterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks. • Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model). Cyber Security -------------- We have evaluated Llama 3 with CyberSecEval, Meta’s cybersecurity safety eval suite, measuring Llama 3’s propensity to suggest insecure code when used as a coding assistant, and Llama 3’s propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models ofequivalent coding capability. equivalent coding capability (https://huggingface.co/spaces/facebook/CyberSecEval) Child Safety ------------ Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. Community --------- Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository. Github repository (https://github.com/meta-llama/PurpleLlama) Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community. output reporting mechanism (https://developers.facebook.com/llama_output_feedback) bug bounty program (https://www.facebook.com/whitehat) Ethical Considerations and Limitations ====================================== The core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. But Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporatingPurple Llamasolutions into your workflows and specificallyLlama Guardwhich provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety. Purple Llama (https://github.com/facebookresearch/PurpleLlama) Llama Guard (https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) Please see the Responsible Use Guide available athttp://llama.meta.com/responsible-use-guide http://llama.meta.com/responsible-use-guide Citation Instructions ===================== @article{llama3modelcard, title={Llama-3-Taiwan-70B-Instruct}, author={yentinglin}, year={2024}, url = {https://huggingface.co/yentinglin/Llama-3-Taiwan-70B-Instruct} https://huggingface.co/yentinglin/Llama-3-Taiwan-70B-Instruct} (https://huggingface.co/yentinglin/Llama-3-Taiwan-70B-Instruct%7D) } Contributors ============ Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos"
  },
  {
    "name": "alphafold2",
    "nvidia_url": "https://build.nvidia.com/deepmind/alphafold2",
    "scraped_on": "2025-10-15T21:42:33.670482",
    "tags": [
      "biology",
      "bionemo",
      "deepmind",
      "drug discovery",
      "nim",
      "protein folding"
    ],
    "model_card": "Model Overview ============== Description: ============ AlphaFold2 is a deep learning model for protein structure prediction developed by the research group at DeepMind, an artificial intelligence (AI) research lab owned by Google (jumper2021alphafold). AlphaFold2 builds on the success of its predecessor, AlphaFold, and represents a significant breakthrough in the field of protein structure prediction. This model is available for commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. License / Terms of Use ---------------------- The AlphaFold2 code is released under theApache 2.0 License. The model parameters are licensed under theCC BY 4.0 License. Apache 2.0 License (https://github.com/google-deepmind/alphafold/blob/main/LICENSE) CC BY 4.0 License (https://github.com/google-deepmind/alphafold?tab=readme-ov-file#model-parameters) References: =========== Model Architecture: =================== Architecture Type:Protein Structure PredictionNetwork Architecture:AlphaFold2 Architecture Type: Network Architecture: Input Type(s):Protein Sequence, Relax Prediction (Default True)Input Format(s):String (less than or equal to 4096 characters), booleanInput Parameters:1DOther Properties Related to Input:NA Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):Protein Structure(s) in PDB FormatOutput Format:PDB (text file)Output Parameters:1DOther Properties Related to Output:Pose (num_atm_ x 3) Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s): • Python Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere [Preferred/Supported] Operating System(s): • [Linux] Model Version(s): ================= AlphaFold2 2.3.2 Training & Evaluation: ====================== Training Dataset: ================= Link:A description of the training dataset and relevant download links are available athttps://www.nature.com/articles/s41586-021-03819-2#data-availability. This data was not collected by NVIDIA.** Data Collection Method by dataset Link: https://www.nature.com/articles/s41586-021-03819-2#data-availability • See the description athttps://www.nature.com/articles/s41586-021-03819-2#data-availability. https://www.nature.com/articles/s41586-021-03819-2#data-availability ** Labeling Method by dataset • See the description athttps://www.nature.com/articles/s41586-021-03819-2#data-availability. https://www.nature.com/articles/s41586-021-03819-2#data-availability Properties (Quantity, Dataset Descriptions, Sensor(s)):Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set. Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: =================== Link:See the description athttps://www.nature.com/articles/s41586-021-03819-2#Sec10.** Data Collection Method by dataset Link: https://www.nature.com/articles/s41586-021-03819-2#Sec10 • [Not Applicable] ** Labeling Method by dataset • [Not Applicable] Properties (Quantity, Dataset Descriptions, Sensor(s)):Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set. Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference: ========== Engine:PythonTest Hardware: Engine: Test Hardware: • NVIDIA A6000 • NVIDIA A100 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws."
  },
  {
    "name": "esm2-650m",
    "nvidia_url": "https://build.nvidia.com/meta/esm2-650m",
    "scraped_on": "2025-10-15T21:42:30.399465",
    "tags": [
      "biology",
      "bionemo",
      "drug discovery",
      "meta",
      "nim",
      "protein embedding"
    ],
    "model_card": "Model Overview ============== Description: ============ ESM2 is a state of the art language model for protein sequences. It outputs a numerical representation of input protein sequences that is suitable for downstream tasks. In particular, its output is used for protein folding in the ESMFold model. ESM2 comes in various sizes: 650M parameters, 3B parameters, and 15B parameters. ESM2 was developed and trained byMETA. The larger models tend to result in more accurate results on downstream tasks, however they also have a longer runtime. META (https://github.com/facebookresearch/esm) License ======= MIT License Copyright (c) Meta Platforms, Inc. and affiliates. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. This model is available for commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIA Model Card. References: =========== Model Architecture: =================== Architecture Type:BERT with rotational embeddingsNetwork Architecture:ESM2-650m Architecture Type: Network Architecture: Input: ====== Input Type(s):Protein SequenceInput Format(s):StringInput Parameters:1DOther Properties Related to Input:Protein Sequence matching the regular expression^[ARNDCQEGHILKMFPSTWYVXBOU]*$upto 1024 characters Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):EmbeddingsOutput Format:Float 16 ArrayOutput Parameters:1DOther Properties Related to Output:NA Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s): • [Not Applicable (N/A)- Name Platform If Multiple] Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • L40 [Preferred/Supported] Operating System(s): • Linux Model Version(s): ================= ESM2 Training & Evaluation: ====================== Training Dataset: ================= Link:UniRef50** Data Collection Method by dataset Link: UniRef50 (https://www.uniprot.org/help/uniref) • Not Applicable ** Labeling Method by dataset • Not Applicable Properties (Quantity, Dataset Descriptions, Sensor(s)):UniRef50, September 2021 version, is used for the training of ESM models. The training dataset was partitioned by randomly selecting 0.5% (≈ 250,000) sequences to form the validation set. The training set has sequences removed via the procedure described Properties (Quantity, Dataset Descriptions, Sensor(s)): UniRef50 (https://www.uniprot.org/help/uniref) Evaluation Dataset: =================== UniRef50** Data Collection Method by dataset UniRef50 (https://www.uniprot.org/help/uniref) • Not Applicable ** Labeling Method by dataset • Not Applicable Properties (Quantity, Dataset Descriptions, Sensor(s)):UniRef50, September 2021 version, is used for the training of ESM models. The training dataset was partitioned by randomly selecting 0.5% (≈ 250,000) sequences to form the validation set. The training set has sequences removed via the procedure described Properties (Quantity, Dataset Descriptions, Sensor(s)): UniRef50 (https://www.uniprot.org/help/uniref) Inference: ========== Engine:TritonTest Hardware: Engine: Test Hardware: • [Other (Not Listed)] Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) **If anything is meant for internal-purposes only (including this statement and pre-filled content recommendations, please alert Trustworthy AI Product Manager or designee before publishing)"
  },
  {
    "name": "ai-generated-image-detection",
    "nvidia_url": "https://build.nvidia.com/hive/ai-generated-image-detection",
    "scraped_on": "2025-10-15T21:42:27.230407",
    "tags": [
      "ai safety",
      "computer vision",
      "content moderation",
      "hive",
      "image classification"
    ],
    "model_card": "Model Overview ============== Description: ============ Hive’s AI-Generated Image Classification model is optimized for use with the kind of media generated by popular AI generative engines such as DALL-E, Midjourney, and Stable Diffusion. The model has two heads. The first head gives a binary classification for all images, identifying whether or not they were AI generated and the accompanying confidence score. The second head provides further details as to the image's source, with support for the most popular AI art generators currently in use. If the model cannot identify a source, it will return none under the source head. This model is ready for commercial and non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. License/Terms of Use: --------------------- https://thehive.ai/terms-of-use Model Architecture: =================== Architecture Type:Convolution Neural Network (CNN)]Network Architecture:EfficientNet-B4 Architecture Type: Network Architecture: Input: ====== Input Type(s):Ensemble Gradient Boosted (EGB) ImageInput Format(s):PNG, JPEG, or JPGInput Parameters:Two-Dimensional (2D) Input Type(s): Input Format(s): Input Parameters: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:One Dimensional (1D) Output Type(s): Output Format: Output Parameters: Software Integration: ===================== Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Jetson • NVIDIA Hopper • NVIDIA Lovelace • NVIDIA Volta [Preferred/Supported] Operating System(s):Linux [Preferred/Supported] Operating System(s): Model Version(s): ================= v1.0 Training Dataset: ================= Training Dataset: ================= ** Data Collection Method by dataset • Unknown ** Labeling Method by dataset • Unknown Properties (Quantity, Dataset Descriptions, Sensor(s)):Trained on a large dataset comprising millions of artificially generated images and human-created images such as photographs, digital and traditional art, and memes sourced from across the web. Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference: ========== Engine:TensorRTTest Hardware:L40 Engine: Test Hardware: Ethical Considerations (For NVIDIA Models Only): ================================================ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "vila",
    "nvidia_url": "https://build.nvidia.com/nvidia/vila",
    "scraped_on": "2025-10-15T21:42:26.719452",
    "tags": [
      "image caption",
      "image to text",
      "nvidia",
      "vision language model",
      "vlm"
    ],
    "model_card": "Vila Model Card =============== Description =========== NVIDIA Vila is a leading vision language model (VLMs) that enables the ability to query and summarize images and video from the physical or virtual world. Vila is deployable in the data center, cloud and at the edge, including Jetson Orin and laptop by AWQ 4bit quantization through TinyChat framework. We find: (1) image-text pairs are not enough, interleaved image-text is essential; (2) unfreezing LLM during interleaved image-text pre-training enables in-context learning; (3)re-blending text-only instruction data is crucial to boost both VLM and text-only performance. This model is ready for commercial use. It was trained on commercial images and videos for all three stages of training and supports single image and video inference. This version does not support interleaved and in-context learning capabilities. References ========== • VILA technical paper VILA technical paper (https://arxiv.org/abs/2312.07533) • VILA github repo VILA github repo (https://github.com/NVlabs/VILA) License ======= The license to use this model is covered by the Model EULA. By downloading the unpruned or pruned version of the model, you accept the terms and conditions of theselicenses licenses (https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/) Model Architecture ================== Architecture Type:Transformer-based Network Architecture Architecture Type: Network Architecture • Vision Encoder: SigLIP-400M • Language Encoder: Yi-34B Input ----- • Input Type(s):Image, Video, Text Input Type(s): • Input Format(s):Image (Red, Green, Blue (RGB)), Video (.mp4), and Text (String) Input Format(s): • Input Parameters:Image (2D), Video (3D), Text (1D) Input Parameters: Output ------ • Output Type(s):Text Output Type(s): • Output Formats:String Output Formats: • Output Parameters:1D Output Parameters: • Other Properties Related to Output:N/A Other Properties Related to Output: Software Integration ==================== • Runtime Engine(s):TensorRT-LLM Runtime Engine(s): • Supported Hardware Architecture(s):NVIDIA Hopper Supported Hardware Architecture(s): • Supported Operating System(s):Linux Supported Operating System(s): Model Versions ============== • Cosmos-Nemotron-SigLIP-Yi-34B Training Dataset ================ NV-Pretraining and NV-CosmosNemotron-SFT data were used. Additionally, the commercial subset of following datasets were used: • OASST1 OASST1 (https://huggingface.co/datasets/OpenAssistant/oasst1) • OASST2 OASST2 (https://huggingface.co/datasets/OpenAssistant/oasst2) • Localized Narratives Localized Narratives (https://google.github.io/localized-narratives/) • TextCaps TextCaps (https://textvqa.org/textcaps/dataset/) • TextVQA TextVQA (https://textvqa.org/dataset/) • RefCOCO RefCOCO (https://github.com/lichengunc/refer) • VQAv2 VQAv2 (https://visualqa.org/) • GQA GQA (https://cs.stanford.edu/people/dorarad/gqa/index.html) • SynthDoG-en SynthDoG-en (https://huggingface.co/datasets/naver-clova-ix/synthdog-en) • A-OKVQ A-OKVQ (https://github.com/allenai/aokvqa) • WIT WIT (https://github.com/google-research-datasets/wit) • CLEVR CLEVR (https://cs.stanford.edu/people/jcjohns/clevr/) • CLEVR-X CLEVR-X (https://github.com/ExplainableML/CLEVR-X) • CLEVR-Math CLEVR-Math (https://huggingface.co/datasets/dali-does/clevr-math) • ScreenQA ScreenQA (https://github.com/google-research-datasets/screen_qa) • WikiSQL WikiSQL (https://github.com/salesforce/WikiSQL) • WikiTablQuestions WikiTablQuestions (https://github.com/ppasupat/WikiTableQuestions/) • RenderedText RenderedText (https://github.com/GbotHQ/ocr-dataset-rendering/) • FinQA FinQA (https://github.com/czyssrs/FinQA) • TAT-QA TAT-QA (https://nextplusplus.github.io/TAT-QA/) • Dolly Dolly (https://huggingface.co/datasets/databricks/databricks-dolly-15k) • Websight Websight (https://huggingface.co/datasets/HuggingFaceM4/WebSight) • RAVEN RAVEN (https://github.com/WellyZhang/RAVEN) • VizWiz VizWiz (https://vizwiz.org/tasks-and-datasets/vqa/) • Inter-GPS Inter-GPS (https://github.com/lupantech/InterGPS) • YouCook2 YouCook2 (http://youcook2.eecs.umich.edu/) • ActivityNet Captions ActivityNet Captions (https://cs.stanford.edu/people/ranjaykrishna/densevid/) • Video Localized Narratives Video Localized Narratives (https://google.github.io/video-localized-narratives/) • CLEVRER CLEVRER (https://google.github.io/video-localized-narratives/) • Perception Test Perception Test (https://github.com/google-deepmind/perception_test) • Next-QA Next-QA (https://github.com/doc-doc/NExT-QA) • Kinetics-400 Kinetics-400 (https://paperswithcode.com/dataset/kinetics) Data Collection Method by dataset: • Hybrid: Human, Automatic/Sensors Labeling Method by dataset: • Hybrid: Human, Automatic/Sensors Properties: • NV-Pretraining data was collected from 5M subsampled NV-CLIP dataset. Stage 3 NV-SFT data has 2.8M images and 3.58M annotations on images that only have commercial license. Additionally, 355K videos with commercial license and 400K annotations on videos were used. Evaluation Data =============== Data Collection Method by dataset: • Hybrid: Human, Automatic/Sensors Labeling Method by dataset: • Hybrid: Human, Automatic/Sensors Properties: • A collection of different benchmarks, including academic VQA benchmarks and recent benchmarks specifically proposed for instruction-following LMMs. • VQAv2 VQAv2 (https://visualqa.org/) • GQA GQA (https://cs.stanford.edu/people/dorarad/gqa/about.html) • ScienceQA Image ScienceQA Image (https://scienceqa.github.io/) • Text VQA Text VQA (https://textvqa.org/) • POPE POPE (https://github.com/AoiDragon/POPE) • MME MME (https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) • SEED-Bench SEED-Bench (https://github.com/AILab-CVC/SEED-Bench) • MMMU MMMU (https://mmmu-benchmark.github.io/) • Video MME Video MME (https://video-mme.github.io/home_page.html) • Egoschema Egoschema (https://egoschema.github.io/) • Perception Test Perception Test (https://github.com/google-deepmind/perception_test) Methodology and KPI ------------------- Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Promise and the Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "alphafold2-multimer",
    "nvidia_url": "https://build.nvidia.com/deepmind/alphafold2-multimer",
    "scraped_on": "2025-10-15T21:42:23.545678",
    "tags": [
      "biology",
      "bionemo",
      "deepmind",
      "drug discovery",
      "nim",
      "protein folding"
    ],
    "model_card": "Model Overview ============== Description: ============ AlphaFold2 is a deep learning model for protein structure prediction developed by the research group at DeepMind, an artificial intelligence (AI) research lab owned by Google (jumper2021alphafold). AlphaFold2 builds on the success of its predecessor, AlphaFold, and represents a significant breakthrough in the field of protein structure prediction. This model is available for commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. License / Terms of Use ---------------------- The AlphaFold2 code is released under theApache 2.0 License. The model parameters are licensed under theCC BY 4.0 License. Apache 2.0 License (https://github.com/google-deepmind/alphafold/blob/main/LICENSE) CC BY 4.0 License (https://github.com/google-deepmind/alphafold?tab=readme-ov-file#model-parameters) References: =========== Model Architecture: =================== Architecture Type:Protein Structure PredictionNetwork Architecture:AlphaFold2 Architecture Type: Network Architecture: Input Type(s):Protein Sequence, Relax Prediction (Default True)Input Format(s):String (less than or equal to 4096 characters), booleanInput Parameters:1DOther Properties Related to Input:NA Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):Protein Structure(s) in PDB FormatOutput Format:PDB (text file)Output Parameters:1DOther Properties Related to Output:Pose (num_atm_ x 3) Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s): • Python Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere [Preferred/Supported] Operating System(s): • [Linux] Model Version(s): ================= AlphaFold2 2.3.2 Training & Evaluation: ====================== Training Dataset: ================= Link:A description of the training dataset and relevant download links are available athttps://www.nature.com/articles/s41586-021-03819-2#data-availability. This data was not collected by NVIDIA.** Data Collection Method by dataset Link: https://www.nature.com/articles/s41586-021-03819-2#data-availability • See the description athttps://www.nature.com/articles/s41586-021-03819-2#data-availability. https://www.nature.com/articles/s41586-021-03819-2#data-availability ** Labeling Method by dataset • See the description athttps://www.nature.com/articles/s41586-021-03819-2#data-availability. https://www.nature.com/articles/s41586-021-03819-2#data-availability Properties (Quantity, Dataset Descriptions, Sensor(s)):Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set. Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: =================== Link:See the description athttps://www.nature.com/articles/s41586-021-03819-2#Sec10.** Data Collection Method by dataset Link: https://www.nature.com/articles/s41586-021-03819-2#Sec10 • [Not Applicable] ** Labeling Method by dataset • [Not Applicable] Properties (Quantity, Dataset Descriptions, Sensor(s)):Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set. Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference: ========== Engine:PythonTest Hardware: Engine: Test Hardware: • NVIDIA A6000 • NVIDIA A100 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws."
  },
  {
    "name": "dracarys-llama-3.1-70b-instruct",
    "nvidia_url": "https://build.nvidia.com/abacusai/dracarys-llama-3_1-70b-instruct",
    "scraped_on": "2025-10-15T21:42:23.156580",
    "tags": [
      "abacusai",
      "chat",
      "code generation",
      "text-to-text"
    ],
    "model_card": "Dracarys-Llama-3.1-70B-Instruct =============================== Introduction ============ We introduce the latest in the Smaug series, the Dracarys family of finetunes targeting coding performance improvements across a variety of base models. This variant is a finetune ofmeta-llama/Meta-Llama-3.1-70B-Instructthat allows to generate code and answer questions about code. meta-llama/Meta-Llama-3.1-70B-Instruct (https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct) Compared to meta-llama/Meta-Llama-3.1-70B-Instruct, Dracarys has better LiveCodeBench scores (see evaluation results below). This model is ready for commercial and non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link toNon-NVIDIA Model Card. Non-NVIDIA Model Card (https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md) License ======= META LLAMA 3 COMMUNITY LICENSE META LLAMA 3 COMMUNITY LICENSE (https://llama.meta.com/llama3/license/) Model Details ============= • Developed by:Abacus.AI Developed by: Abacus.AI (https://abacus.ai) • Finetuned from model:meta-llama/Meta-Llama-3.1-70B-Instruct. Finetuned from model: meta-llama/Meta-Llama-3.1-70B-Instruct (https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct) Model Architecture ================== Architecture Type:Transformer Architecture Type: Input ===== Input Type(s):TextInput Format(s):StringInput Parameters:One Dimensional (1D) Input Type(s): Input Format(s): Input Parameters: Output ====== Output Type(s):TextOutput Format:StringOutput Parameters:1D Output Type(s): Output Format: Output Parameters: Supported Hardware Microarchitecture Compatibility: • NVIDIA Hopper • NVIDIA Lovelace Preferred Operating System(s): • Linux Evaluation Results ================== LiveCodeBench ------------- Dracarys-Llama-3.1-70B-Instruct Data Collection Method by dataset: • [Unknown] Labeling Method by dataset: • [Unknown] Inference ========= Engine:TensorRT-LLMTest Hardware: Engine: Test Hardware: • NVIDIA H100x2 Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "qwen2-7b-instruct",
    "nvidia_url": "https://build.nvidia.com/qwen/qwen2-7b-instruct",
    "scraped_on": "2025-10-15T21:42:20.001716",
    "tags": [
      "chat",
      "chinese language generation",
      "large language models",
      "qwen",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description =========== Qwen2 is the new series of Qwen large language models for language understanding, language generation, multilingual capability, coding, mathematics, and reasoning. For Qwen2, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters, including a Mixture-of-Experts model. This repo contains the 7B Qwen2 base language model. Compared with the state-of-the-art open source language models, including the previously released Qwen1.5, Qwen2 has generally surpassed most open source models and demonstrated competitiveness against proprietary models across a series of benchmarks targeting for language understanding, language generation, multilingual capability, coding, mathematics, and reasoning tasks, etc. This model is ready for commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Qwen's(Model Card). (Model Card) (https://huggingface.co/Qwen/Qwen2-7B-Instruct) License, Acceptable Use, and Research Privacy Policy ==================================================== By using this model, you are agreeing to the terms and conditions of theApache 2.0 Apache 2.0 (https://huggingface.co/Qwen/Qwen2-7B-Instruct/blob/main/LICENSE) Model Developer: QwenModel Update Date: August 8, 2024 Model Developer Model Update Date Model ArchitectureArchitecture Type: TransformerNetwork Architecture: Qwen Model Architecture InputInput Type: TextInput Format: StringInput Parameters: max_tokens, temperature, top_p, stop, frequency_penalty, presence_penalty, seed Input OutputOutput Type: TextOutput Format: String Output Software Integration ==================== [Preferred/Supported] Operating System(s): Linux Model Version(s): ================= The instruction-tuned 7B Qwen2 model, Qwen2-7B-Instruct Training Dataset: ================= Link:[Unknown] Link: Data Collection Method by dataset • [Unknown] Labeling Method by dataset • [Unknown] Properties (Quantity, Dataset Descriptions, Sensor(s)):Unknown Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: =================== Link:See Evaluation section of theHugging Face Qwen2-7B-Instruct Model Card Link: Hugging Face Qwen2-7B-Instruct Model Card (https://huggingface.co/Qwen/Qwen2-7B-Instruct#evaluation) InferenceEngine: TensorRT-LLMTest Hardware: L40 Inference Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.2-1b-instruct",
    "nvidia_url": "https://build.nvidia.com/meta/llama-3.2-1b-instruct",
    "scraped_on": "2025-10-15T21:42:19.654179",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "meta",
      "text-to-text"
    ],
    "model_card": "Model Information ================= The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pre-trained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks. Llama 3.2 models are ready for commercial use. Models are accelerated byTensorRT-LLM, a library for optimizing Large Language Model (LLM) inference on NVIDIA GPUs. TensorRT-LLM (https://github.com/NVIDIA/TensorRT-LLM) Models in this Collection: • Llama-3.2-1B • Llama-3.2-1B-Instruct • Llama-3.2-3B • Llama-3.2-3B-Instruct Model Developer:Meta Model Developer: Model Release Date:September 25, 2024 Model Release Date: Third-Party Community Consideration:This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIALlama 3.2 Model Card. Third-Party Community Consideration: Llama 3.2 Model Card (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md) License:Use of Llama 3.2 is governed by theLlama 3.2 Community License(a custom, commercial license agreement). License: Llama 3.2 Community License (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) Model Architecture:Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Model Architecture: Supported Languages:English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly. Supported Languages: Llama 3.2 Model Family:Token counts refer to pre-training data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability. Llama 3.2 Model Family: Status:This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety. Status: Feedback:Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the modelREADME. For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please gohere. Feedback: README (https://github.com/meta-llama/llama-models/tree/main/models/llama3_2) here (https://github.com/meta-llama/llama-recipes) Intended Use ============ Intended Use Cases:Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pre-trained models can be adapted for a variety of additional natural language generation tasks. Intended Use Cases: Out of Scope:Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card. Out of Scope: Hardware and Software ===================== Training Factors:We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pre-training. Fine-tuning, annotation, and evaluation were also performed on production infrastructure. Training Factors: Training Energy Use:Training utilized a cumulative of916kGPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. Training Energy Use: 916k Training Greenhouse Gas Emissions:Estimated total location-based greenhouse gas emissions were240tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq. Training Greenhouse Gas Emissions: 240 The methodology used to determine training energy use and greenhouse gas emissions can be foundhere. Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others. here (https://arxiv.org/pdf/2204.05149) Training Data ============= Data Collection Method:UnknownLabeling Method:Unknown Data Collection Method: Labeling Method: Overview:Llama 3.2 was pre-trained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pre-training stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO). Overview: Data Freshness:The pre-training data has a cutoff of December 2023. Data Freshness: Benchmarks - English Text ========================= In this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library. Base Pre-trained Models ----------------------- Instruction-Tuned Models ------------------------ Multilingual Benchmarks ----------------------- Inference ========= Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper • NVIDIA Lovelace • NVIDIA Jetson Supported Operating System(s): • Linux • Windows Responsibility & Safety ======================= As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks: • Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama • Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm • Provide protections for the community to help prevent the misuse of our models Responsible Deployment ---------------------- Approach:Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in ourCommunity Stories webpage. Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in ourResponsible Use Guide. Approach: Community Stories webpage (https://llama.meta.com/community-stories/) Responsible Use Guide (https://llama.meta.com/responsible-use-guide/) Llama 3.2 Instruct ------------------ Objective:Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3paper. Objective: paper (https://ai.meta.com/research/publications/the-llama-3-herd-of-models/) Fine-Tuning Data:We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. Fine-Tuning Data: Refusals and Tone:Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. Refusals and Tone: Llama 3.2 Systems ----------------- Safety as a System:Large language models, including Llama 3.2,are not designed to be deployed in isolationbut instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community withsafeguardsthat developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All ourreference implementationsdemos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. Safety as a System: are not designed to be deployed in isolation safeguards (https://llama.meta.com/trust-and-safety/) reference implementations (https://github.com/meta-llama/llama-agentic-system) New Capabilities and Use Cases ------------------------------ Technological Advancement:Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, seeLlama 3.1 Model Card, as the same considerations apply here as well. Technological Advancement: Llama 3.1 Model Card (https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) Constrained Environments:Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version. Constrained Environments: Evaluations ----------- Scaled Evaluations:We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Scaled Evaluations: Red Teaming:We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. Red Teaming: Critical Risks -------------- In addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas: 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models. 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons): 2. Child Safety:Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. 2. Child Safety: 3. Cyber Attacks:Our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed. Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. 3. Cyber Attacks: Community --------- Industry Partnerships:Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository. Industry Partnerships: Github repository (https://github.com/meta-llama/PurpleLlama) Grants:We also set up theLlama Impact Grantsprogram to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be foundhere. Grants: Llama Impact Grants (https://llama.meta.com/llama-impact-grants/) here (https://llama.meta.com/llama-impact-grants/#finalists) Reporting:Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community. Reporting: output reporting mechanism (https://developers.facebook.com/llama_output_feedback) bug bounty program (https://www.facebook.com/whitehat) Ethical Considerations and Limitations ====================================== Values:The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. Values: Testing:Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including ourResponsible Use Guide,Trust and Safetysolutions, and otherresourcesto learn more about responsible development. Testing: Responsible Use Guide (https://llama.meta.com/responsible-use-guide) Trust and Safety (https://llama.meta.com/trust-and-safety/) resources (https://llama.meta.com/docs/get-started/) NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.2-90b-vision-instruct",
    "nvidia_url": "https://build.nvidia.com/meta/llama-3.2-90b-vision-instruct",
    "scraped_on": "2025-10-15T21:42:16.513600",
    "tags": [
      "image captioning",
      "image-text retrieval",
      "image-to-text",
      "meta",
      "visual grounding",
      "visual qa"
    ],
    "model_card": "Model Information ================= The Meta Llama 3.2 Vision collection of multimodal large language models (LLMs) is a collection of pre-trained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2 Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks. Llama 3.2 Vision models are ready for commercial use. Models in this Collection: • Llama-3.2-11B-Vision • Llama-3.2-11B-Vision-Instruct • Llama-3.2-90B-Vision • Llama-3.2-90B-Vision-Instruct Model Developer: Meta Model Developer Model Release Date:September 25, 2024 Model Release Date: Third-Party Community Consideration:This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIALlama 3.2 Model Card. Third-Party Community Consideration: Llama 3.2 Model Card (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md) License:Use of Llama 3.2 is governed by theLlama 3.2 Community License(a custom, commercial license agreement). License: Llama 3.2 Community License (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) Model Architecture:Llama 3.2 Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2 Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM. Model Architecture: Supported Languages:For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported. Supported Languages: Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly. Llama 3.2 Model Family:Token counts refer to pre-training data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability. Llama 3.2 Model Family: Status:This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety. Status: Feedback:Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the modelREADME. For more technical information about generation parameters and recipes for how to use Llama 3.2 Vision in applications, please gohere. Feedback: README (https://github.com/meta-llama/llama-models/tree/main/models/llama3_2) here (https://github.com/meta-llama/llama-recipes) Intended Use ============ Intended Use Cases:Llama 3.2 Vision is intended for commercial and research use. Instruction-tuned models are intended for visual recognition, image reasoning, captioning, and assistant-like chat with images, whereas pre-trained models can be adapted for a variety of image reasoning tasks. Additionally, because of Llama 3.2 Vision’s ability to take images and text as inputs, additional use cases could include: Intended Use Cases: • Visual Question Answering (VQA) and Visual Reasoning: Imagine a machine that looks at a picture and understands your questions about it. • Document Visual Question Answering (DocVQA): Imagine a computer understanding both the text and layout of a document, like a map or contract, and then answering questions about it directly from the image. • Image Captioning: Image captioning bridges the gap between vision and language, extracting details, understanding the scene, and then crafting a sentence or two that tells the story. • Image-Text Retrieval: Image-text retrieval is like a matchmaker for images and their descriptions. Similar to a search engine but one that understands both pictures and words. • Visual Grounding: Visual grounding is like connecting the dots between what we see and say. It’s about understanding how language references specific parts of an image, allowing AI models to pinpoint objects or regions based on natural language descriptions. The Llama 3.2 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.2 Community License allows for these use cases. Out of Scope:Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card. Out of Scope: Hardware and Software ===================== Training Factors:We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pre-training. Fine-tuning, annotation, and evaluation were also performed on production infrastructure. Training Factors: Training Energy Use:Training utilized a cumulative of2.02MGPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. Training Energy Use: 2.02M Training Greenhouse Gas Emissions:Estimated total location-based greenhouse gas emissions were584tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq. Training Greenhouse Gas Emissions: 584 The methodology used to determine training energy use and greenhouse gas emissions can be foundhere. Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others. here (https://arxiv.org/pdf/2204.05149) Training Data ============= Data Collection Method:UnknownLabeling Method:Unknown Data Collection Method: Labeling Method: Overview:Llama 3.2 Vision was pre-trained on 6B image and text pairs. The instruction tuning data includes publicly available vision instruction datasets, as well as over 3M synthetically generated examples. Overview: Data Freshness:The pre-training data has a cutoff of December 2023. Data Freshness: Benchmarks - Image Reasoning ============================ In this section, we report the results for Llama 3.2 Vision models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library. Base Pre-trained Models ----------------------- Instruction-Tuned Models ------------------------ 73.0 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=E19) 86.0 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=H19) 51.9 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=E25) 68.0 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=H25) 32.8 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=E27) 46.7 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=H27) 68.9 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=E33) 86.9 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=H33) Inference ========= Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper • NVIDIA Lovelace Supported Operating System(s): • Linux Responsibility & Safety ======================= As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks: • Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. • Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm. • Provide protections for the community to help prevent the misuse of our models. Responsible Deployment ---------------------- Approach:Llama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in ourCommunity Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our Responsible Use Guide, you can refer to theResponsible Use Guideto learn more. Approach: Community Stories webpage (https://llama.meta.com/community-stories/) Responsible Use Guide (https://llama.meta.com/responsible-use-guide/) Llama 3.2 Instruct ------------------ Objective:Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3paper. Objective: paper (https://ai.meta.com/research/publications/the-llama-3-herd-of-models/) Fine-Tuning Data:We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. Fine-Tuning Data: Refusals and Tone:Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. Refusals and Tone: Llama 3.2 Systems ----------------- Safety as a System:Large language models, including Llama 3.2,are not designed to be deployed in isolationbut instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community withsafeguardsthat developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All ourreference implementationsdemos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. Safety as a System: are not designed to be deployed in isolation safeguards (https://llama.meta.com/trust-and-safety/) reference implementations (https://github.com/meta-llama/llama-agentic-system) New Capabilities and Use Cases ------------------------------ Technological Advancement:Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, seeLlama 3.1 Model Card, as the same considerations apply here as well., Technological Advancement: Llama 3.1 Model Card (https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) Image Reasoning:Llama 3.2 Vision models come with multimodal (text and image) input capabilities enabling image reasoning applications. As part of our responsible release process, we took dedicated measures including evaluations and mitigations to address the risk of the models uniquely identifying individuals in images. As with other LLM risks, models may not always be robust to adversarial prompts, and developers should evaluate identification and other applicable risks in the context of their applications as well as consider deploying Llama Guard 3-11B-Vision as part of their system or other mitigations as appropriate to detect and mitigate such risks. Image Reasoning: Evaluations ----------- Scaled Evaluations:We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Scaled Evaluations: Red teaming:We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. Red teaming: Critical Risks -------------- In addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas: 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):For Llama 3.1, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. For Llama 3.2 Vision models, we conducted additional targeted evaluations and found that it was unlikely Llama 3.2 presented an increase in scientific capabilities due to its added image understanding capability as compared to Llama 3.1. 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons): 2. Child Safety:Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. 2. Child Safety: 3. Cyber Attacks:Our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. 3. Cyber Attacks: Community --------- Industry Partnerships:Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository. Industry Partnerships: Github repository (https://github.com/meta-llama/PurpleLlama) Grants:We also set up theLlama Impact Grantsprogram to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be foundhere. Grants: Llama Impact Grants (https://llama.meta.com/llama-impact-grants/) here (https://llama.meta.com/llama-impact-grants/#finalists) Reporting:Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community. Reporting: output reporting mechanism (https://developers.facebook.com/llama_output_feedback) bug bounty program (https://www.facebook.com/whitehat) Ethical Considerations and Limitations ====================================== Values:The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. Values: Testing:But Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including ourResponsible Use Guide,Trust and Safetysolutions, and otherresourcesto learn more about responsible development. Testing: Responsible Use Guide (https://llama.meta.com/responsible-use-guide) Trust and Safety (https://llama.meta.com/trust-and-safety/) resources (https://llama.meta.com/docs/get-started/) NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.2-11b-vision-instruct",
    "nvidia_url": "https://build.nvidia.com/meta/llama-3.2-11b-vision-instruct",
    "scraped_on": "2025-10-15T21:42:13.313084",
    "tags": [
      "image captioning",
      "image-text retrieval",
      "image-to-text",
      "meta",
      "visual grounding",
      "visual qa"
    ],
    "model_card": "Model Information ================= The Meta Llama 3.2 Vision collection of multimodal large language models (LLMs) is a collection of pre-trained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2 Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks. Llama 3.2 Vision models are ready for commercial use. Models in this Collection: • Llama-3.2-11B-Vision • Llama-3.2-11B-Vision-Instruct • Llama-3.2-90B-Vision • Llama-3.2-90B-Vision-Instruct Model Developer: Meta Model Developer Model Release Date:September 25, 2024 Model Release Date: Third-Party Community Consideration:This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIALlama 3.2 Model Card. Third-Party Community Consideration: Llama 3.2 Model Card (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md) License:Use of Llama 3.2 is governed by theLlama 3.2 Community License(a custom, commercial license agreement). License: Llama 3.2 Community License (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) Model Architecture:Llama 3.2 Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2 Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM. Model Architecture: Supported Languages:For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported. Supported Languages: Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly. Llama 3.2 Model Family:Token counts refer to pre-training data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability. Llama 3.2 Model Family: Status:This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety. Status: Feedback:Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the modelREADME. For more technical information about generation parameters and recipes for how to use Llama 3.2 Vision in applications, please gohere. Feedback: README (https://github.com/meta-llama/llama-models/tree/main/models/llama3_2) here (https://github.com/meta-llama/llama-recipes) Intended Use ============ Intended Use Cases:Llama 3.2 Vision is intended for commercial and research use. Instruction-tuned models are intended for visual recognition, image reasoning, captioning, and assistant-like chat with images, whereas pre-trained models can be adapted for a variety of image reasoning tasks. Additionally, because of Llama 3.2 Vision’s ability to take images and text as inputs, additional use cases could include: Intended Use Cases: • Visual Question Answering (VQA) and Visual Reasoning: Imagine a machine that looks at a picture and understands your questions about it. • Document Visual Question Answering (DocVQA): Imagine a computer understanding both the text and layout of a document, like a map or contract, and then answering questions about it directly from the image. • Image Captioning: Image captioning bridges the gap between vision and language, extracting details, understanding the scene, and then crafting a sentence or two that tells the story. • Image-Text Retrieval: Image-text retrieval is like a matchmaker for images and their descriptions. Similar to a search engine but one that understands both pictures and words. • Visual Grounding: Visual grounding is like connecting the dots between what we see and say. It’s about understanding how language references specific parts of an image, allowing AI models to pinpoint objects or regions based on natural language descriptions. The Llama 3.2 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.2 Community License allows for these use cases. Out of Scope:Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card. Out of Scope: Hardware and Software ===================== Training Factors:We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pre-training. Fine-tuning, annotation, and evaluation were also performed on production infrastructure. Training Factors: Training Energy Use:Training utilized a cumulative of2.02MGPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. Training Energy Use: 2.02M Training Greenhouse Gas Emissions:Estimated total location-based greenhouse gas emissions were584tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq. Training Greenhouse Gas Emissions: 584 The methodology used to determine training energy use and greenhouse gas emissions can be foundhere. Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others. here (https://arxiv.org/pdf/2204.05149) Training Data ============= Data Collection Method:UnknownLabeling Method:Unknown Data Collection Method: Labeling Method: Overview:Llama 3.2 Vision was pre-trained on 6B image and text pairs. The instruction tuning data includes publicly available vision instruction datasets, as well as over 3M synthetically generated examples. Overview: Data Freshness:The pre-training data has a cutoff of December 2023. Data Freshness: Benchmarks - Image Reasoning ============================ In this section, we report the results for Llama 3.2 Vision models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library. Base Pre-trained Models ----------------------- Instruction-Tuned Models ------------------------ 73.0 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=E19) 86.0 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=H19) 51.9 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=E25) 68.0 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=H25) 32.8 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=E27) 46.7 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=H27) 68.9 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=E33) 86.9 (https://docs.google.com/spreadsheets/d/1b3IrobU5rTfbxtR-lfEMn7Fb41_YO3kfhrPmuP-g6ys/edit?gid=688970324#gid=688970324&range=H33) Inference ========= Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper • NVIDIA Lovelace Supported Operating System(s): • Linux Responsibility & Safety ======================= As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks: • Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. • Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm. • Provide protections for the community to help prevent the misuse of our models. Responsible Deployment ---------------------- Approach:Llama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in ourCommunity Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our Responsible Use Guide, you can refer to theResponsible Use Guideto learn more. Approach: Community Stories webpage (https://llama.meta.com/community-stories/) Responsible Use Guide (https://llama.meta.com/responsible-use-guide/) Llama 3.2 Instruct ------------------ Objective:Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3paper. Objective: paper (https://ai.meta.com/research/publications/the-llama-3-herd-of-models/) Fine-Tuning Data:We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. Fine-Tuning Data: Refusals and Tone:Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. Refusals and Tone: Llama 3.2 Systems ----------------- Safety as a System:Large language models, including Llama 3.2,are not designed to be deployed in isolationbut instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community withsafeguardsthat developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All ourreference implementationsdemos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. Safety as a System: are not designed to be deployed in isolation safeguards (https://llama.meta.com/trust-and-safety/) reference implementations (https://github.com/meta-llama/llama-agentic-system) New Capabilities and Use Cases ------------------------------ Technological Advancement:Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, seeLlama 3.1 Model Card, as the same considerations apply here as well., Technological Advancement: Llama 3.1 Model Card (https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) Image Reasoning:Llama 3.2 Vision models come with multimodal (text and image) input capabilities enabling image reasoning applications. As part of our responsible release process, we took dedicated measures including evaluations and mitigations to address the risk of the models uniquely identifying individuals in images. As with other LLM risks, models may not always be robust to adversarial prompts, and developers should evaluate identification and other applicable risks in the context of their applications as well as consider deploying Llama Guard 3-11B-Vision as part of their system or other mitigations as appropriate to detect and mitigate such risks. Image Reasoning: Evaluations ----------- Scaled Evaluations:We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Scaled Evaluations: Red teaming:We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. Red teaming: Critical Risks -------------- In addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas: 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):For Llama 3.1, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. For Llama 3.2 Vision models, we conducted additional targeted evaluations and found that it was unlikely Llama 3.2 presented an increase in scientific capabilities due to its added image understanding capability as compared to Llama 3.1. 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons): 2. Child Safety:Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. 2. Child Safety: 3. Cyber Attacks:Our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. 3. Cyber Attacks: Community --------- Industry Partnerships:Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository. Industry Partnerships: Github repository (https://github.com/meta-llama/PurpleLlama) Grants:We also set up theLlama Impact Grantsprogram to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be foundhere. Grants: Llama Impact Grants (https://llama.meta.com/llama-impact-grants/) here (https://llama.meta.com/llama-impact-grants/#finalists) Reporting:Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community. Reporting: output reporting mechanism (https://developers.facebook.com/llama_output_feedback) bug bounty program (https://www.facebook.com/whitehat) Ethical Considerations and Limitations ====================================== Values:The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. Values: Testing:But Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including ourResponsible Use Guide,Trust and Safetysolutions, and otherresourcesto learn more about responsible development. Testing: Responsible Use Guide (https://llama.meta.com/responsible-use-guide) Trust and Safety (https://llama.meta.com/trust-and-safety/) resources (https://llama.meta.com/docs/get-started/) NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.2-3b-instruct",
    "nvidia_url": "https://build.nvidia.com/meta/llama-3.2-3b-instruct",
    "scraped_on": "2025-10-15T21:42:10.202063",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "meta",
      "text-to-text"
    ],
    "model_card": "Model Information ================= The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pre-trained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks. Llama 3.2 models are ready for commercial use. Models are accelerated byTensorRT-LLM, a library for optimizing Large Language Model (LLM) inference on NVIDIA GPUs. TensorRT-LLM (https://github.com/NVIDIA/TensorRT-LLM) Models in this Collection: • Llama-3.2-1B • Llama-3.2-1B-Instruct • Llama-3.2-3B • Llama-3.2-3B-Instruct Model Developer:Meta Model Developer: Model Release Date:September 25, 2024 Model Release Date: Third-Party Community Consideration:This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIALlama 3.2 Model Card. Third-Party Community Consideration: Llama 3.2 Model Card (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md) License:Use of Llama 3.2 is governed by theLlama 3.2 Community License(a custom, commercial license agreement). License: Llama 3.2 Community License (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) Model Architecture:Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Model Architecture: Supported Languages:English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly. Supported Languages: Llama 3.2 Model Family:Token counts refer to pre-training data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability. Llama 3.2 Model Family: Status:This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety. Status: Feedback:Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the modelREADME. For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please gohere. Feedback: README (https://github.com/meta-llama/llama-models/tree/main/models/llama3_2) here (https://github.com/meta-llama/llama-recipes) Intended Use ============ Intended Use Cases:Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pre-trained models can be adapted for a variety of additional natural language generation tasks. Intended Use Cases: Out of Scope:Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card. Out of Scope: Hardware and Software ===================== Training Factors:We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pre-training. Fine-tuning, annotation, and evaluation were also performed on production infrastructure. Training Factors: Training Energy Use:Training utilized a cumulative of916kGPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. Training Energy Use: 916k Training Greenhouse Gas Emissions:Estimated total location-based greenhouse gas emissions were240tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq. Training Greenhouse Gas Emissions: 240 The methodology used to determine training energy use and greenhouse gas emissions can be foundhere. Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others. here (https://arxiv.org/pdf/2204.05149) Training Data ============= Data Collection Method:UnknownLabeling Method:Unknown Data Collection Method: Labeling Method: Overview:Llama 3.2 was pre-trained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pre-training stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO). Overview: Data Freshness:The pre-training data has a cutoff of December 2023. Data Freshness: Benchmarks - English Text ========================= In this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library. Base Pre-trained Models ----------------------- Instruction-Tuned Models ------------------------ Multilingual Benchmarks ----------------------- Inference ========= Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper • NVIDIA Lovelace • NVIDIA Jetson Supported Operating System(s): • Linux • Windows Responsibility & Safety ======================= As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks: • Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama • Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm • Provide protections for the community to help prevent the misuse of our models Responsible Deployment ---------------------- Approach:Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in ourCommunity Stories webpage. Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in ourResponsible Use Guide. Approach: Community Stories webpage (https://llama.meta.com/community-stories/) Responsible Use Guide (https://llama.meta.com/responsible-use-guide/) Llama 3.2 Instruct ------------------ Objective:Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3paper. Objective: paper (https://ai.meta.com/research/publications/the-llama-3-herd-of-models/) Fine-Tuning Data:We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. Fine-Tuning Data: Refusals and Tone:Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. Refusals and Tone: Llama 3.2 Systems ----------------- Safety as a System:Large language models, including Llama 3.2,are not designed to be deployed in isolationbut instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community withsafeguardsthat developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All ourreference implementationsdemos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. Safety as a System: are not designed to be deployed in isolation safeguards (https://llama.meta.com/trust-and-safety/) reference implementations (https://github.com/meta-llama/llama-agentic-system) New Capabilities and Use Cases ------------------------------ Technological Advancement:Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, seeLlama 3.1 Model Card, as the same considerations apply here as well. Technological Advancement: Llama 3.1 Model Card (https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) Constrained Environments:Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version. Constrained Environments: Evaluations ----------- Scaled Evaluations:We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Scaled Evaluations: Red Teaming:We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. Red Teaming: Critical Risks -------------- In addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas: 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models. 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons): 2. Child Safety:Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. 2. Child Safety: 3. Cyber Attacks:Our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed. Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. 3. Cyber Attacks: Community --------- Industry Partnerships:Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository. Industry Partnerships: Github repository (https://github.com/meta-llama/PurpleLlama) Grants:We also set up theLlama Impact Grantsprogram to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be foundhere. Grants: Llama Impact Grants (https://llama.meta.com/llama-impact-grants/) here (https://llama.meta.com/llama-impact-grants/#finalists) Reporting:Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community. Reporting: output reporting mechanism (https://developers.facebook.com/llama_output_feedback) bug bounty program (https://www.facebook.com/whitehat) Ethical Considerations and Limitations ====================================== Values:The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. Values: Testing:Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including ourResponsible Use Guide,Trust and Safetysolutions, and otherresourcesto learn more about responsible development. Testing: Responsible Use Guide (https://llama.meta.com/responsible-use-guide) Trust and Safety (https://llama.meta.com/trust-and-safety/) resources (https://llama.meta.com/docs/get-started/) NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.1-nemotron-70b-reward",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-reward",
    "scraped_on": "2025-10-15T21:42:09.755012",
    "tags": [
      "nvidia",
      "reward model",
      "rlhf",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description: ============ Llama-3.1-Nemotron-70B-Reward is a large language model customized by NVIDIA to predict the quality of LLM generated responses. This model is ready for commercial use. Terms of use ============ By accessing this model, you are agreeing to the LLama 3 terms and conditions of thelicense,acceptable use policyandMeta’s privacy policy license (https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE) acceptable use policy (https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/USE_POLICY.md) Meta’s privacy policy (https://www.facebook.com/privacy/policy/) References(s): ============== • SteerLM method SteerLM method (https://arxiv.org/abs/2310.05344) • HelpSteer HelpSteer (https://arxiv.org/abs/2311.09528) • HelpSteer2 HelpSteer2 (https://arxiv.org/abs/2406.08673) • Introducing Llama 3.1: Our most capable models to date Introducing Llama 3.1: Our most capable models to date (https://ai.meta.com/blog/meta-llama-3-1/) • Meta's Llama 3.1 Webpage Meta's Llama 3.1 Webpage (https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1) • Meta's Llama 3.1 Model Card Meta's Llama 3.1 Model Card (https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Llama 3.1 Architecture Type: Network Architecture: Input: ====== Input Type(s):TextInput Format:StringInput Parameters:NAOther Properties Related to Input:Provided text must be within 4096 tokens Input Type(s): Input Format: Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):FloatOutput Format:One Single FloatOutput Parameters:NAOther Properties Related to Output:NA Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: ===================== Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper • NVIDIA TuringSupported Operating System(s):Linux Supported Operating System(s): Model Version: ============== v1.0 Training & Evaluation: ====================== Datasets: ========= Data Collection Method by dataset • [Hybrid: Human, Synthetic] Labeling Method by dataset • [Human] Link: • HelpSteer2 HelpSteer2 (https://huggingface.co/datasets/nvidia/HelpSteer2) Properties (Quantity, Dataset Descriptions, Sensor(s)): • 37,120 prompt-responses built to make more models more aligned with human preference - specifically more helpful, factually-correct, coherent, and customizable based on complexity and verbosity. Inference: ========== Engine:TritonTest Hardware:H100, A100 80GB, A100 40GB Engine: Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "studiovoice",
    "nvidia_url": "https://build.nvidia.com/nvidia/studiovoice",
    "scraped_on": "2025-10-15T21:42:06.522461",
    "tags": [
      "digital human",
      "nvidia",
      "nvidia maxine",
      "run-on-rtx",
      "speech enhancement",
      "speech-to-speech"
    ],
    "model_card": "Model Overview ============== Description =========== Maxine Studio Voice enhances the input speech recorded through low quality microphones in noisy/reverberant environment to studio-recorded quality speech. Studio Voice is available under NVIDIA Maxine — a developer platform for deploying AI features that enhance audio, video, and creating new experiences in real-time audio-video communication. Maxine's state-of-the-art models create high-quality AI effects using standard microphones and cameras without additional special equipments. NVIDIA Maxine is exclusively part of NVIDIA AI Enterprise for production workflows — an extensive library of full-stack software, including AI solution workflows, frameworks, pre-trained models, and infrastructure optimization. Terms of use ============ The use of NVIDIA Maxine's Studio Voice is available as a demonstration of the input and output of the Studio Voice generative model. As such the user may upload an audio file or select one of the sample inputs and download the generated audio for evaluation under the terms of theNVIDIA MAXINE EVALUATION LICENSE AGREEMENT. NVIDIA MAXINE EVALUATION LICENSE AGREEMENT (https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf) References(s): ============== • NVIDIA Maxine NVIDIA Maxine (https://developer.nvidia.com/maxine) Model Architecture ================== Architecture Type:Convolution Neural Networks (CNNs), Transformers, Generative Adversarial Networks (GANs)Network Architecture:Encoder-DecoderModel Version:0.2 Architecture Type: Network Architecture: Model Version: Input: ====== Input Type(s):Ordered List (audio samples)Input Format(s):FP32 (-1.0 to 1.0)Other Properties Related to Input:Pulse Code Modulation (PCM) audio samples with no encoding or pre-processing; 16kHz or 48kHz sampling rate required. Input Type(s): Input Format(s): Other Properties Related to Input: Output: ======= Output Type(s):Ordered List (audio samples)Output Format:FP32 (-1.0 to 1.0)Other Properties Related to Output:PCM audio samples at input sampling rate with no encoding or post-processing. Output Type(s): Output Format: Other Properties Related to Output: Software Integration ==================== Supported Hardware Platform(s):Hopper, Ada, Ampere, Turing, VoltaTest Hardware:A10, L40, T10Supported Operating System(s):Linux, Windows Supported Hardware Platform(s): Test Hardware: Supported Operating System(s): Training & Evaluation ===================== Datasets ======== NVIDIA models are trained on a diverse set of public and proprietary datasets. The Studio Voice model is trained on a dataset that comprises of diverse English accents. Link:DAPSProperties (Quantity, Dataset Descriptions, Sensor(s)):The DAPS dataset has 15 versions of audio (3 professional versions and 12 consumer device/real-world environment combinations). Each version consists of about 4.5 hours of data (about 14 minutes from each of 20 speakers). Link: DAPS (https://ccrma.stanford.edu/~gautham/Site/daps.html) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:LibriTTSProperties (Quantity, Dataset Descriptions, Sensor(s)):LibriTTS is a multi-speaker English corpus of approximately 585 hours of read English speech, which is resampled at 16kHZ. Link: LibriTTS (https://www.openslr.org/60/) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:VCTKProperties (Quantity, Dataset Descriptions, Sensor(s)):This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. Link: VCTK (https://datashare.ed.ac.uk/handle/10283/3443) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:HiFi-TTSProperties (Quantity, Dataset Descriptions, Sensor(s)):A multi-speaker English dataset for training text-to-speech models. The HiFi-TTS dataset contains about 291.6 hours of speech from 10 speakers with at least 17 hours per speaker sampled at 44.1 kHz. Link: HiFi-TTS (https://www.openslr.org/109/) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:Device Recorded VCTK (DR-VCTK)Properties (Quantity, Dataset Descriptions, Sensor(s)):Device recorded version of VCTK dataset on common consumer devices (laptop, tablet and smartphone) in office environment. This dataset contains 109 English speakers with different accents. There are around 400 sentences available from each speaker. For this recording, 8 different microphones were used. This dataset contains around 250 Gb of re-recorded speech. Link: Device Recorded VCTK (DR-VCTK) (https://github.com/nii-yamagishilab/downloader-DR-VCTK-complete) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:Dataset of impulse responses from variable acoustics room Arni at Aalto Acoustic LabsProperties (Quantity, Dataset Descriptions, Sensor(s)):A dataset of impulse responses collected in the variable acoustics laboratory Arni at Acoustics Lab of Aalto University, Espoo, Finland. IRs of 5342 configurations of sound absorption in Arni are included in the dataset. Each of them were measured using an omnidirectional sound source and 5 sound receivers. For each configuration, 5 impulse reponses (IRs) were captured. The total number of measurements in the dataset is 132 037. Link: Dataset of impulse responses from variable acoustics room Arni at Aalto Acoustic Labs (https://zenodo.org/records/6985104#.Y7_vv3ZBy3C) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:Room Impulse Response and Noise DatabaseProperties (Quantity, Dataset Descriptions, Sensor(s)):A database of simulated and real room impulse responses, isotropic and point-source noises. The audio files in this data are all in 16KHz sampling rate and 16-bit precision. Link: Room Impulse Response and Noise Database (https://www.openslr.org/28/) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:DNS Challenge 5Properties (Quantity, Dataset Descriptions, Sensor(s)):Collated dataset of clean speech, noise and impulse response provided by Microsoft for the ICASSP 2023 Deep Noise Suppression Challenge. Link: DNS Challenge 5 (https://github.com/microsoft/DNS-Challenge/tree/2db96d5f75257df764a6ef66513b4b97bc707f30) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:AudioSetProperties (Quantity, Dataset Descriptions, Sensor(s)):AudioSet consists of an expanding ontology of 632 audio event classes and a collection of 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos. Link: AudioSet (https://research.google.com/audioset/download.html) Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference ========= Engine:TritonTest Hardware:A10, L40, T10 Engine: Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware: Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++Explainability,Bias,Safety & Security, andPrivacySubcards. Please report security vulnerabilities or NVIDIA AI Concernshere. Explainability (explainability) Bias (bias) Safety & Security (safety-and-security) Privacy (privacy) here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.1-swallow-8b-instruct-v0.1",
    "nvidia_url": "https://build.nvidia.com/institute-of-science-tokyo/llama-3-1-swallow-8b-instruct-v01",
    "scraped_on": "2025-10-15T21:42:03.315187",
    "tags": [
      "chat",
      "institute-of-science-tokyo",
      "large language model",
      "regional language generation",
      "sovereign ai"
    ],
    "model_card": "Llama 3.1 Swallow ================= Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on theMeta Llama 3.1models. Llama 3.1 Swallow enhanced the Japanese language capabilities of the original Llama 3.1 while retaining the English language capabilities. We use approximately 200 billion tokens that were sampled from a large Japanese web corpus (Swallow Corpus Version 2), Japanese and English Wikipedia articles, and mathematical and coding contents, etc (see the Training Datasets section) for continual pre-training. The instruction-tuned models (Instruct) were built by supervised fine-tuning (SFT) on the synthetic data specially built for Japanese. See the Swallow Model Index section to find other model variants. Meta Llama 3.1 (https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f) Release History =============== • October 08, 2024: ReleasedLlama-3.1-Swallow-8B-v0.1,Llama-3.1-Swallow-8B-Instruct-v0.1,Llama-3.1-Swallow-70B-v0.1, andLlama-3.1-Swallow-70B-Instruct-v0.1. October 08, 2024 Llama-3.1-Swallow-8B-v0.1 (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1) Llama-3.1-Swallow-8B-Instruct-v0.1 (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1) Llama-3.1-Swallow-70B-v0.1 (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1) Llama-3.1-Swallow-70B-Instruct-v0.1 (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1) Swallow Model Index =================== Link (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1) Link (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1) Link (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1) Link (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1) The websitehttps://swallow-llm.github.io/provides large language models developed by the Swallow team. https://swallow-llm.github.io/ Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link toNon-NVIDIA Model Card. Non-NVIDIA Model Card (https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md) License and Terms of Use ======================== GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theLlama 3.1 Community License Agreementand theGemma Terms of Use. Built with Llama. GOVERNING TERMS NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) Llama 3.1 Community License Agreement (https://www.llama.com/llama3_1/license/) Gemma Terms of Use (https://ai.google.dev/gemma/terms) Model Details ============= • Model type: Please refer toLlama 3.1 MODEL_CARDfor details on the model architecture. Model type Llama 3.1 MODEL_CARD (https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) • Language(s): Japanese English Language(s) • Library:Megatron-LM Library Megatron-LM (https://github.com/NVIDIA/Megatron-LM) • Tokenizer: Please refer toLlama 3.1 blogfor details on the tokenizer. Tokenizer Llama 3.1 blog (https://ai.meta.com/blog/meta-llama-3-1) • Contact: swallow[at]nlp.c.titech.ac.jp Contact Model Architecture: =================== Architecture Type:Transformer Architecture Type: Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:One Dimensional (1D) Input Type(s): Input Format(s): Input Parameters: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:1D Output Type(s): Output Format: Output Parameters: Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper • NVIDIA Lovelace Model Performance ================= Japanese tasks -------------- 0.5683 0.6240 0.6252 0.2178 0.9240 0.5874 0.5736 0.9170 0.2820 0.2282 0.5055 English tasks ------------- 0.6465 0.4280 0.7419 0.6360 0.7453 0.6994 0.3783 0.9037 0.6928 0.6321 MT-Bench JA =========== 0.5970 0.5111 0.6810 0.6134 0.7125 0.5438 0.7091 0.5500 0.7734 Evaluation Benchmarks ===================== Japanese evaluation benchmarks ------------------------------ We used llm-jp-eval(v1.3.0), JP Language Model Evaluation Harness(commit #9b42d41) and Code Generation LM Evaluation Harness(commit #0261c52). The details are as follows: • Multiple-choice question answering (JCommonsenseQA [Kurihara et al., 2022]) • Open-ended question answering (JEMHopQA [Ishii et al., 2024]) • Open-ended question answering (NIILC [関根, 2003]) • Machine reading comprehension (JSQuAD [Kurihara et al., 2022]) • Automatic summarization (XL-Sum [Hasan et al., 2021]) • Machine translation (WMT2020 ja-en [Barrault et al., 2020]) • Machine translation (WMT2020 en-ja [Barrault et al., 2020]) • Mathematical reasoning (MGSM [Shi et al., 2023]) • Academic exams (JMMLU [尹ら, 2024]) • Code generation (JHumanEval [佐藤ら, 2024]) English evaluation benchmarks ----------------------------- We used the Language Model Evaluation Harness(v.0.4.2) and Code Generation LM Evaluation Harness(commit #0261c52). The details are as follows: • Multiple-choice question answering (OpenBookQA [Mihaylov et al., 2018]) • Open-ended question answering (TriviaQA [Joshi et al., 2017]) • Machine reading comprehension (SQuAD2 [Rajpurkar et al., 2018]) • Commonsense reasoning (XWINO [Tikhonov and Ryabinin, 2021]) • Natural language inference (HellaSwag [Zellers et al., 2019]) • Mathematical reasoning (GSM8K [Cobbe et al., 2021]) • Reasoning (BBH (BIG-Bench-Hard) [Suzgun et al., 2023]) • Academic exams (MMLU [Hendrycks et al., 2021]) • Code generation (HumanEval [Chen et al., 2021]) MT-Bench JA ----------- We usedJapanese MT-Benchto assess the instruction-following capabilities of models. We utilized the following settings: Japanese MT-Bench (https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_question) • Implemantation: FastChat [Zheng+, 2023] (commit #e86e70d0) • Question:Nejumi LLM-Leaderboard NEO, mtbench_ja_question_v3 Nejumi LLM-Leaderboard NEO, mtbench_ja_question_v3 (https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_question/v3) • Reference Answer:Nejumi LLM-Leaderboard NEO, mtbench_ja_referenceanswer_v1 Nejumi LLM-Leaderboard NEO, mtbench_ja_referenceanswer_v1 (https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_referenceanswer/v1) • Prompt for Judge:Nejumi LLM-Lederboard NEO, mtbench_ja_prompt_v1 Nejumi LLM-Lederboard NEO, mtbench_ja_prompt_v1 (https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_prompt/v1) • Judge:gpt-4-1106-preview • Scoring: Absolute scale normalized to a 0-1 range, averaged over five runs. Inference: ========== Engine:TensorRT-LLMTest Hardware: Engine: Test Hardware: • NVIDIA H100x4 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Usage ===== Training Datasets ================= Instruction Tuning ------------------ The following instruction datasets were used for the instruction tuning. • Japaneselmsys-chat-1m-synth-ja-wo-pii-and-template-instructionsSingle-turn Japanese synthetic instruction dataset derived fromlmsys-chat-1mdataset[Zhang+, ICLR24]). The first-turn user instructions were translated into Japanese via DeepL machine translation, and the assistant responses were generated using theLlama-3.1-405B-Instructmodel. Rejection sampling (n=6) was applied, withLlama-3.1-70B-Instructserving as a judge.As implied by the dataset name, conversations that contain personally identifiable information (PII) or template-based user instructions have been removed. Duplicate instuctions have also been removed.filtered-magpie-ultra-jaA Japanese variant of thefiltered-magpie-ultra-endataset, machine-translated into Japanese using thegemma-2-27b-it.gemma-magpieA Japanese synthetic Q&A dataset from scratch, generated usinggemma-2-27b-it. User instructions were created with prompts specific to each topic, and the assistant responses were generated for these instructions. The conversations were then heuristically filtered for quality and length. • lmsys-chat-1m-synth-ja-wo-pii-and-template-instructionsSingle-turn Japanese synthetic instruction dataset derived fromlmsys-chat-1mdataset[Zhang+, ICLR24]). The first-turn user instructions were translated into Japanese via DeepL machine translation, and the assistant responses were generated using theLlama-3.1-405B-Instructmodel. Rejection sampling (n=6) was applied, withLlama-3.1-70B-Instructserving as a judge.As implied by the dataset name, conversations that contain personally identifiable information (PII) or template-based user instructions have been removed. Duplicate instuctions have also been removed. • Single-turn Japanese synthetic instruction dataset derived fromlmsys-chat-1mdataset[Zhang+, ICLR24]). The first-turn user instructions were translated into Japanese via DeepL machine translation, and the assistant responses were generated using theLlama-3.1-405B-Instructmodel. Rejection sampling (n=6) was applied, withLlama-3.1-70B-Instructserving as a judge. lmsys-chat-1m (https://huggingface.co/datasets/lmsys/lmsys-chat-1m) [Zhang+, ICLR24] (https://openreview.net/forum?id=BOfDKxfwt0) Llama-3.1-405B-Instruct (https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct) Llama-3.1-70B-Instruct (https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct) • As implied by the dataset name, conversations that contain personally identifiable information (PII) or template-based user instructions have been removed. Duplicate instuctions have also been removed. • filtered-magpie-ultra-jaA Japanese variant of thefiltered-magpie-ultra-endataset, machine-translated into Japanese using thegemma-2-27b-it. • A Japanese variant of thefiltered-magpie-ultra-endataset, machine-translated into Japanese using thegemma-2-27b-it. gemma-2-27b-it (https://huggingface.co/google/gemma-2-27b-it) • gemma-magpieA Japanese synthetic Q&A dataset from scratch, generated usinggemma-2-27b-it. User instructions were created with prompts specific to each topic, and the assistant responses were generated for these instructions. The conversations were then heuristically filtered for quality and length. • A Japanese synthetic Q&A dataset from scratch, generated usinggemma-2-27b-it. User instructions were created with prompts specific to each topic, and the assistant responses were generated for these instructions. The conversations were then heuristically filtered for quality and length. gemma-2-27b-it (https://huggingface.co/google/gemma-2-27b-it) • Englishlmsys-chat-1m-synth-en-wo-pii-and-template-instructionsSimilar to thelmsys-chat-1m-synth-ja-wo-pii-and-template-instructions, but this version uses the original English user instructions. The assistant responses were generated in English as well. Rejection sampling was not applied in this version.filtered-magpie-ultra-enA subset of themagpie-ultradataset, developed following the MAGPIE recipe[Xu+, arXiv24]usingLlama-3.1-405B-Instruct. This subset includes only samples rated as 'average,' 'good,' or 'excellent.' • lmsys-chat-1m-synth-en-wo-pii-and-template-instructionsSimilar to thelmsys-chat-1m-synth-ja-wo-pii-and-template-instructions, but this version uses the original English user instructions. The assistant responses were generated in English as well. Rejection sampling was not applied in this version. • Similar to thelmsys-chat-1m-synth-ja-wo-pii-and-template-instructions, but this version uses the original English user instructions. The assistant responses were generated in English as well. Rejection sampling was not applied in this version. • filtered-magpie-ultra-enA subset of themagpie-ultradataset, developed following the MAGPIE recipe[Xu+, arXiv24]usingLlama-3.1-405B-Instruct. This subset includes only samples rated as 'average,' 'good,' or 'excellent.' • A subset of themagpie-ultradataset, developed following the MAGPIE recipe[Xu+, arXiv24]usingLlama-3.1-405B-Instruct. This subset includes only samples rated as 'average,' 'good,' or 'excellent.' magpie-ultra (https://huggingface.co/datasets/argilla/magpie-ultra-v0.1) [Xu+, arXiv24] (https://arxiv.org/abs/2406.08464) Llama-3.1-405B-Instruct (https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct) Risks and Limitations ===================== The models released here are still in the early stages of our research and development and have not been tuned to ensure outputs align with human intent and safety considerations. Acknowledgements ================ We thank Meta Research for releasing Llama 3.1 under a generous open license. We received various supports including: • AIST project: “Research and Development of Foundation Models for Generative AI in the Physical Domain” • NEDO project: “Development of Artificial Intelligence Application Technology to Support Judgment in Design Risk Assessment Work Based on the Perspective of Skilled Persons\" (JPNP18002) of “Development of Integration Technology as the Core of Next Generation Artificial Intelligence and Robotics” • MEXT project: \"Formation of R&D center to ensure transparency and reliability of generative AI models\" • AIST program:Large Generative AI Development Support Program Large Generative AI Development Support Program (https://abci.ai/en/link/lfm_support_program.html) Authors ======= Here are the team members: • FromTokyo Institute of Technology Okazaki Laboratory, the following members:Naoaki OkazakiSakae MizukiYoumi MaKoki MaedaKakeru HattoriMasanari OhiTaihei ShiotaniKoshiro Saito Tokyo Institute of Technology Okazaki Laboratory (https://www.nlp.c.titech.ac.jp/index.en.html) • Naoaki Okazaki Naoaki Okazaki (https://www.chokkan.org/index.ja.html) • Sakae Mizuki Sakae Mizuki (https://s-mizuki-nlp.github.io/) • Youmi Ma Youmi Ma (https://www.nlp.c.titech.ac.jp/member/youmi.en.html) • Koki Maeda Koki Maeda (https://sites.google.com/view/silviase) • Kakeru Hattori Kakeru Hattori (https://aya-se.vercel.app/) • Masanari Ohi Masanari Ohi (https://sites.google.com/view/masanariohi) • Taihei Shiotani Taihei Shiotani (https://github.com/inatoihs) • Koshiro Saito Koshiro Saito (https://sites.google.com/view/koshiro-saito) • FromTokyo Institute of Technology YOKOTA Laboratory, the following members:Rio YokotaKazuki FujiiTaishi NakamuraTakumi OkamotoIshida Shigeki Tokyo Institute of Technology YOKOTA Laboratory (https://www.rio.gsic.titech.ac.jp/en/index.html) • Rio Yokota Rio Yokota (https://twitter.com/rioyokota) • Kazuki Fujii Kazuki Fujii (https://twitter.com/okoge_kaz) • Taishi Nakamura Taishi Nakamura (https://twitter.com/Setuna7777_2) • Takumi Okamoto Takumi Okamoto (https://www.linkedin.com/in/takumi-okamoto) • Ishida Shigeki Ishida Shigeki (https://www.wantedly.com/id/reborn27) • FromArtificial Intelligence Research Center, AIST, Japan, the following members:Hiroya Takamura Artificial Intelligence Research Center, AIST, Japan (https://www.airc.aist.go.jp/en/teams/) • Hiroya Takamura Hiroya Takamura (https://sites.google.com/view/hjtakamura) How to cite =========== If you find our work helpful, please feel free to cite us. References ----------"
  },
  {
    "name": "llama-3.1-swallow-70b-instruct-v0.1",
    "nvidia_url": "https://build.nvidia.com/institute-of-science-tokyo/llama-3-1-swallow-70b-instruct-v01",
    "scraped_on": "2025-10-15T21:42:00.157793",
    "tags": [
      "chat",
      "institute-of-science-tokyo",
      "large language model",
      "regional language generation",
      "sovereign ai"
    ],
    "model_card": "Llama 3.1 Swallow ================= Llama 3.1 Swallow is a series of large language models (8B, 70B) that were built by continual pre-training on theMeta Llama 3.1models. Llama 3.1 Swallow enhanced the Japanese language capabilities of the original Llama 3.1 while retaining the English language capabilities. We use approximately 200 billion tokens that were sampled from a large Japanese web corpus (Swallow Corpus Version 2), Japanese and English Wikipedia articles, and mathematical and coding contents, etc (see the Training Datasets section) for continual pre-training. The instruction-tuned models (Instruct) were built by supervised fine-tuning (SFT) on the synthetic data specially built for Japanese. See the Swallow Model Index section to find other model variants. Meta Llama 3.1 (https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f) Release History =============== • October 08, 2024: ReleasedLlama-3.1-Swallow-8B-v0.1,Llama-3.1-Swallow-8B-Instruct-v0.1,Llama-3.1-Swallow-70B-v0.1, andLlama-3.1-Swallow-70B-Instruct-v0.1. October 08, 2024 Llama-3.1-Swallow-8B-v0.1 (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1) Llama-3.1-Swallow-8B-Instruct-v0.1 (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1) Llama-3.1-Swallow-70B-v0.1 (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1) Llama-3.1-Swallow-70B-Instruct-v0.1 (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1) Swallow Model Index =================== Link (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1) Link (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1) Link (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1) Link (https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1) The websitehttps://swallow-llm.github.io/provides large language models developed by the Swallow team. https://swallow-llm.github.io/ Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link toNon-NVIDIA Model Card. Non-NVIDIA Model Card (https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md) License and Terms of Use ======================== GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theLlama 3.1 Community License Agreementand theGemma Terms of Use. Built with Llama. GOVERNING TERMS NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) Llama 3.1 Community License Agreement (https://www.llama.com/llama3_1/license/) Gemma Terms of Use (https://ai.google.dev/gemma/terms) Model Details ============= • Model type: Please refer toLlama 3.1 MODEL_CARDfor details on the model architecture. Model type Llama 3.1 MODEL_CARD (https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) • Language(s): Japanese English Language(s) • Library:Megatron-LM Library Megatron-LM (https://github.com/NVIDIA/Megatron-LM) • Tokenizer: Please refer toLlama 3.1 blogfor details on the tokenizer. Tokenizer Llama 3.1 blog (https://ai.meta.com/blog/meta-llama-3-1) • Contact: swallow[at]nlp.c.titech.ac.jp Contact Model Architecture: =================== Architecture Type:Transformer Architecture Type: Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:One Dimensional (1D) Input Type(s): Input Format(s): Input Parameters: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:1D Output Type(s): Output Format: Output Parameters: Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper • NVIDIA Lovelace Model Performance ================= Japanese tasks -------------- 0.9696 0.8360 0.7899 0.6933 0.6643 0.6817 0.2611 0.6150 0.9236 0.3123 0.2593 English tasks ------------- 0.4540 0.7064 0.8478 0.9113 0.8212 0.7915 0.7277 0.4085 0.9230 0.8486 MT-Bench JA =========== 0.7032 0.8223 0.6968 0.8301 0.7296 0.8540 0.8646 0.8184 0.7791 Evaluation Benchmarks ===================== Japanese evaluation benchmarks ------------------------------ We used llm-jp-eval(v1.3.0), JP Language Model Evaluation Harness(commit #9b42d41) and Code Generation LM Evaluation Harness(commit #0261c52). The details are as follows: • Multiple-choice question answering (JCommonsenseQA [Kurihara et al., 2022]) • Open-ended question answering (JEMHopQA [Ishii et al., 2024]) • Open-ended question answering (NIILC [関根, 2003]) • Machine reading comprehension (JSQuAD [Kurihara et al., 2022]) • Automatic summarization (XL-Sum [Hasan et al., 2021]) • Machine translation (WMT2020 ja-en [Barrault et al., 2020]) • Machine translation (WMT2020 en-ja [Barrault et al., 2020]) • Mathematical reasoning (MGSM [Shi et al., 2023]) • Academic exams (JMMLU [尹ら, 2024]) • Code generation (JHumanEval [佐藤ら, 2024]) English evaluation benchmarks ----------------------------- We used the Language Model Evaluation Harness(v.0.4.2) and Code Generation LM Evaluation Harness(commit #0261c52). The details are as follows: • Multiple-choice question answering (OpenBookQA [Mihaylov et al., 2018]) • Open-ended question answering (TriviaQA [Joshi et al., 2017]) • Machine reading comprehension (SQuAD2 [Rajpurkar et al., 2018]) • Commonsense reasoning (XWINO [Tikhonov and Ryabinin, 2021]) • Natural language inference (HellaSwag [Zellers et al., 2019]) • Mathematical reasoning (GSM8K [Cobbe et al., 2021]) • Reasoning (BBH (BIG-Bench-Hard) [Suzgun et al., 2023]) • Academic exams (MMLU [Hendrycks et al., 2021]) • Code generation (HumanEval [Chen et al., 2021]) MT-Bench JA ----------- We usedJapanese MT-Benchto assess the instruction-following capabilities of models. We utilized the following settings: Japanese MT-Bench (https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_question) • Implemantation: FastChat [Zheng+, 2023] (commit #e86e70d0) • Question:Nejumi LLM-Leaderboard NEO, mtbench_ja_question_v3 Nejumi LLM-Leaderboard NEO, mtbench_ja_question_v3 (https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_question/v3) • Reference Answer:Nejumi LLM-Leaderboard NEO, mtbench_ja_referenceanswer_v1 Nejumi LLM-Leaderboard NEO, mtbench_ja_referenceanswer_v1 (https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_referenceanswer/v1) • Prompt for Judge:Nejumi LLM-Lederboard NEO, mtbench_ja_prompt_v1 Nejumi LLM-Lederboard NEO, mtbench_ja_prompt_v1 (https://wandb.ai/wandb-japan/llm-leaderboard/artifacts/dataset/mtbench_ja_prompt/v1) • Judge:gpt-4-1106-preview • Scoring: Absolute scale normalized to a 0-1 range, averaged over five runs. Inference: ========== Engine:TensorRT-LLMTest Hardware: Engine: Test Hardware: • NVIDIA H100x4 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Usage ===== Training Datasets ================= Instruction Tuning ------------------ The following instruction datasets were used for the instruction tuning. • Japaneselmsys-chat-1m-synth-ja-wo-pii-and-template-instructionsSingle-turn Japanese synthetic instruction dataset derived fromlmsys-chat-1mdataset[Zhang+, ICLR24]). The first-turn user instructions were translated into Japanese via DeepL machine translation, and the assistant responses were generated using theLlama-3.1-405B-Instructmodel. Rejection sampling (n=6) was applied, withLlama-3.1-70B-Instructserving as a judge.As implied by the dataset name, conversations that contain personally identifiable information (PII) or template-based user instructions have been removed. Duplicate instuctions have also been removed.filtered-magpie-ultra-jaA Japanese variant of thefiltered-magpie-ultra-endataset, machine-translated into Japanese using thegemma-2-27b-it.gemma-magpieA Japanese synthetic Q&A dataset from scratch, generated usinggemma-2-27b-it. User instructions were created with prompts specific to each topic, and the assistant responses were generated for these instructions. The conversations were then heuristically filtered for quality and length. • lmsys-chat-1m-synth-ja-wo-pii-and-template-instructionsSingle-turn Japanese synthetic instruction dataset derived fromlmsys-chat-1mdataset[Zhang+, ICLR24]). The first-turn user instructions were translated into Japanese via DeepL machine translation, and the assistant responses were generated using theLlama-3.1-405B-Instructmodel. Rejection sampling (n=6) was applied, withLlama-3.1-70B-Instructserving as a judge.As implied by the dataset name, conversations that contain personally identifiable information (PII) or template-based user instructions have been removed. Duplicate instuctions have also been removed. • Single-turn Japanese synthetic instruction dataset derived fromlmsys-chat-1mdataset[Zhang+, ICLR24]). The first-turn user instructions were translated into Japanese via DeepL machine translation, and the assistant responses were generated using theLlama-3.1-405B-Instructmodel. Rejection sampling (n=6) was applied, withLlama-3.1-70B-Instructserving as a judge. lmsys-chat-1m (https://huggingface.co/datasets/lmsys/lmsys-chat-1m) [Zhang+, ICLR24] (https://openreview.net/forum?id=BOfDKxfwt0) Llama-3.1-405B-Instruct (https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct) Llama-3.1-70B-Instruct (https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct) • As implied by the dataset name, conversations that contain personally identifiable information (PII) or template-based user instructions have been removed. Duplicate instuctions have also been removed. • filtered-magpie-ultra-jaA Japanese variant of thefiltered-magpie-ultra-endataset, machine-translated into Japanese using thegemma-2-27b-it. • A Japanese variant of thefiltered-magpie-ultra-endataset, machine-translated into Japanese using thegemma-2-27b-it. gemma-2-27b-it (https://huggingface.co/google/gemma-2-27b-it) • gemma-magpieA Japanese synthetic Q&A dataset from scratch, generated usinggemma-2-27b-it. User instructions were created with prompts specific to each topic, and the assistant responses were generated for these instructions. The conversations were then heuristically filtered for quality and length. • A Japanese synthetic Q&A dataset from scratch, generated usinggemma-2-27b-it. User instructions were created with prompts specific to each topic, and the assistant responses were generated for these instructions. The conversations were then heuristically filtered for quality and length. gemma-2-27b-it (https://huggingface.co/google/gemma-2-27b-it) • Englishlmsys-chat-1m-synth-en-wo-pii-and-template-instructionsSimilar to thelmsys-chat-1m-synth-ja-wo-pii-and-template-instructions, but this version uses the original English user instructions. The assistant responses were generated in English as well. Rejection sampling was not applied in this version.filtered-magpie-ultra-enA subset of themagpie-ultradataset, developed following the MAGPIE recipe[Xu+, arXiv24]usingLlama-3.1-405B-Instruct. This subset includes only samples rated as 'average,' 'good,' or 'excellent.' • lmsys-chat-1m-synth-en-wo-pii-and-template-instructionsSimilar to thelmsys-chat-1m-synth-ja-wo-pii-and-template-instructions, but this version uses the original English user instructions. The assistant responses were generated in English as well. Rejection sampling was not applied in this version. • Similar to thelmsys-chat-1m-synth-ja-wo-pii-and-template-instructions, but this version uses the original English user instructions. The assistant responses were generated in English as well. Rejection sampling was not applied in this version. • filtered-magpie-ultra-enA subset of themagpie-ultradataset, developed following the MAGPIE recipe[Xu+, arXiv24]usingLlama-3.1-405B-Instruct. This subset includes only samples rated as 'average,' 'good,' or 'excellent.' • A subset of themagpie-ultradataset, developed following the MAGPIE recipe[Xu+, arXiv24]usingLlama-3.1-405B-Instruct. This subset includes only samples rated as 'average,' 'good,' or 'excellent.' magpie-ultra (https://huggingface.co/datasets/argilla/magpie-ultra-v0.1) [Xu+, arXiv24] (https://arxiv.org/abs/2406.08464) Llama-3.1-405B-Instruct (https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct) Risks and Limitations ===================== The models released here are still in the early stages of our research and development and have not been tuned to ensure outputs align with human intent and safety considerations. Acknowledgements ================ We thank Meta Research for releasing Llama 3.1 under a generous open license. We received various supports including: • AIST project: “Research and Development of Foundation Models for Generative AI in the Physical Domain” • NEDO project: “Development of Artificial Intelligence Application Technology to Support Judgment in Design Risk Assessment Work Based on the Perspective of Skilled Persons\" (JPNP18002) of “Development of Integration Technology as the Core of Next Generation Artificial Intelligence and Robotics” • MEXT project: \"Formation of R&D center to ensure transparency and reliability of generative AI models\" • AIST program:Large Generative AI Development Support Program Large Generative AI Development Support Program (https://abci.ai/en/link/lfm_support_program.html) Authors ======= Here are the team members: • FromTokyo Institute of Technology Okazaki Laboratory, the following members:Naoaki OkazakiSakae MizukiYoumi MaKoki MaedaKakeru HattoriMasanari OhiTaihei ShiotaniKoshiro Saito Tokyo Institute of Technology Okazaki Laboratory (https://www.nlp.c.titech.ac.jp/index.en.html) • Naoaki Okazaki Naoaki Okazaki (https://www.chokkan.org/index.ja.html) • Sakae Mizuki Sakae Mizuki (https://s-mizuki-nlp.github.io/) • Youmi Ma Youmi Ma (https://www.nlp.c.titech.ac.jp/member/youmi.en.html) • Koki Maeda Koki Maeda (https://sites.google.com/view/silviase) • Kakeru Hattori Kakeru Hattori (https://aya-se.vercel.app/) • Masanari Ohi Masanari Ohi (https://sites.google.com/view/masanariohi) • Taihei Shiotani Taihei Shiotani (https://github.com/inatoihs) • Koshiro Saito Koshiro Saito (https://sites.google.com/view/koshiro-saito) • FromTokyo Institute of Technology YOKOTA Laboratory, the following members:Rio YokotaKazuki FujiiTaishi NakamuraTakumi OkamotoIshida Shigeki Tokyo Institute of Technology YOKOTA Laboratory (https://www.rio.gsic.titech.ac.jp/en/index.html) • Rio Yokota Rio Yokota (https://twitter.com/rioyokota) • Kazuki Fujii Kazuki Fujii (https://twitter.com/okoge_kaz) • Taishi Nakamura Taishi Nakamura (https://twitter.com/Setuna7777_2) • Takumi Okamoto Takumi Okamoto (https://www.linkedin.com/in/takumi-okamoto) • Ishida Shigeki Ishida Shigeki (https://www.wantedly.com/id/reborn27) • FromArtificial Intelligence Research Center, AIST, Japan, the following members:Hiroya Takamura Artificial Intelligence Research Center, AIST, Japan (https://www.airc.aist.go.jp/en/teams/) • Hiroya Takamura Hiroya Takamura (https://sites.google.com/view/hjtakamura) How to cite =========== If you find our work helpful, please feel free to cite us. References ----------"
  },
  {
    "name": "granite-guardian-3.0-8b",
    "nvidia_url": "https://build.nvidia.com/ibm/granite-guardian-3_0-8b",
    "scraped_on": "2025-10-15T21:41:59.685495",
    "tags": [
      "guardrail",
      "ibm",
      "text-to-text"
    ],
    "model_card": "Granite Guardian 3.0 8B ======================= Model Summary ============= • Developer:IBM Research Developer: • GitHub Repository:ibm-granite/granite-guardian GitHub Repository: ibm-granite/granite-guardian (https://github.com/ibm-granite/granite-guardian) • Website:Granite Guardian Docs Website Granite Guardian Docs (https://www.ibm.com/granite/docs/) • Release Date: October 21st, 2024 Release Date Granite Guardian 3.0 8Bis a fine-tuned Granite 3.0 8B Instruct model designed to detect risks in prompts and responses. It can help with risk detection along many key dimensions catalogued in theIBM Risk Atlas. It is trained on unique data comprising human annotations and synthetic data informed by internal red-teaming. It outperforms other open-source models in the same space on standard benchmarks. Granite Guardian 3.0 8B IBM Risk Atlas (https://www.ibm.com/docs/en/watsonx/saas?topic=ai-risk-atlas) Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAGranite-3.0-8B-Basemodel card. Granite-3.0-8B-Base Granite-3.0-8B-Base (https://huggingface.co/ibm-granite/granite-3.0-8b-base) License/Terms of Use: --------------------- GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service; and the use of this model is governed by theNVIDIA AI Foundation Models Community License Agreement. ADDITIONAL INFORMATION:Apache 2.0 License. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA AI Foundation Models Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-ai-foundation-models-community-license-agreement/) Apache 2.0 License (https://choosealicense.com/licenses/apache-2.0/) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Other-Dense Architecture Type: Network Architecture: Intended Use ============ Granite Guardian is useful for risk detection use-cases which are applicable across a wide-range of enterprise applications - • Detecting harm-related risks within prompt text or model response (as guardrails). These present two fundamentally different use cases as the former assesses user supplied text while the latter evaluates model generated text. • RAG (retrieval-augmented generation) use-case where the guardian model assesses three key issues: context relevance (whether the retrieved context is relevant to the query), groundedness (whether the response is accurate and faithful to the provided context), and answer relevance (whether the response directly addresses the user's query). Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:One-Dimensional (1D)Other Properties Related to Input:None Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:1DOther Properties Related to Output:None Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Usage ===== Risk Definitions ---------------- The model is specifically designed to detect the following risks in user and assistant message • Harm: content considered harmful by common-sense. Harm • Social Bias: prejudice based on identity or characteristics. Social Bias • Jailbreaking: deliberate instances of manipulating AI to generate harmful, undesired, or inappropriate content. Jailbreaking • Violence: content promoting physical, mental, or sexual harm. Violence • Profanity: use of offensive language or insults. Profanity • Sexual Content: explicit or suggestive material of a sexual nature. Sexual Content • Unethical Behavior: actions that violate moral or legal standards. Unethical Behavior The model also finds a novel use in assessing hallucinations risks within a RAG pipeline. These include • Context Relevance: retrieved context is not pertinent to answering the user's question or addressing their needs. Context Relevance • Groundedness: assistant's response includes claims or facts not supported by or contradicted by the provided context. Groundedness • Answer Relevance: assistant's response fails to address or properly respond to the user's input. Answer Relevance Using Granite Guardian ---------------------- Cookbooksoffer an excellent starting point for working with Granite Guardian, providing a variety of examples that demonstrate how Granite Guardian can be configured for different risk detection scenarios. This includes the model use for inspecting assistant message (responses) or evaluating risks that may arise in RAG use cases. Cookbooks (https://github.com/ibm-granite-community) Quickstart Example ------------------ The following code describes how to use Granite-Guardian to obtain probability scores for a given user and assistant message and a pre-defined guardian configuration. Prompt Template --------------- Granite Guardian is designed to output a yes/no response for the following prompt template. As outlined in previous section, this template is wrapped withinapply_chat_template. The following snippet illustrates its basic structure in free-form forsocial_biasrisk. Scope of Use ------------ • Granite Guardian models mustonlybe used strictly for the prescribed scoring mode, which generates yes/no outputs based on the specified template. Any deviation from this intended use may lead to unexpected, potentially unsafe, or harmful outputs. The model may also be prone to such behaviour via adversarial attacks. • The model is targeted for risk definitions of general harm, social bias, profanity, violence, sexual content, unethical behavior, jailbreaking, or groundedness/relevance for retrieval-augmented generation. It is also applicable for use for custom risk definitions but these require testing. • The model is only trained and tested on English data. • Given their parameter size, the main Granite Guardian models are intended for use cases that require moderate cost, latency, and throughput such as model risk assessment, model observability and monitoring, and spot-checking inputs and outputs. Smaller models, like theGranite-Guardian-HAP-38mfor recognizing hate, abuse and profanity can be used for guardrailing with stricter cost, latency, or throughput requirements. Granite-Guardian-HAP-38m (https://huggingface.co/ibm-granite/granite-guardian-hap-38m) Training Data ============= Granite Guardian is trained on a combination of human annotated and synthetic data. Samples fromhh-rlhfdataset were used to obtain responses from Granite and Mixtral models. These prompt-response pairs were annotated for different risk dimensions by a group of people at DataForce. DataForce prioritizes the well-being of its data contributors by ensuring they are paid fairly and receive livable wages for all projects. Additional synthetic data was used to supplement the training set to improve performance for hallucination and jailbreak related risks. hh-rlhf (https://huggingface.co/datasets/Anthropic/hh-rlhf) Annotator Demographics ---------------------- Evaluations =========== Harm Benchmarks --------------- Following the general harm definition, Granite-Guardian-3.0-8B is evaluated across the standard benchmarks ofAeigis AI Content Safety Dataset,ToxicChat,HarmBench,SimpleSafetyTests. With the risk definition set tojailbreak, the model gives a recall of 1.0 for the jailbreak prompts within ToxicChat dataset. Aeigis AI Content Safety Dataset (https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-1.0) ToxicChat (https://huggingface.co/datasets/lmsys/toxic-chat) HarmBench (https://github.com/centerforaisafety/HarmBench/tree/main) SimpleSafetyTests (https://huggingface.co/datasets/Bertievidgen/SimpleSafetyTests) AUC RAG Hallucination Benchmarks ---------------------------- For risks in RAG use cases, the model is evaluated onTRUEbenchmarks. TRUE (https://github.com/google-research/true) AUC Model Version(s): ================= Granite Guardian 3.0 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nemotron-4-mini-hindi-4b-instruct",
    "nvidia_url": "https://build.nvidia.com/nvidia/nemotron-4-mini-hindi-4b-instruct",
    "scraped_on": "2025-10-15T21:41:59.349275",
    "tags": [
      "chat",
      "indic",
      "language generation",
      "nvidia",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description: ============ Nemotron-4-Mini-Hindi-4B-Instruct is a chat model for generating responses for chat application and retrieval augmented generation in Hindi. It is a aligned version of Nemotron-4-Mini-Hindi-4B-Base . It is a small language model (SLM) optimized through distillation, pruning, and quantization for speed and on-device deployment. VRAM usage has been minimized to approximately 2 GB, providing significantly faster time to first token compared to LLMs. This model is ready for commercial use. License/Terms of Use: --------------------- NVIDIA Open Model License NVIDIA Open Model License (https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf) References ========== Please refer to theUser Guideto use the model and use a suggested guideline for prompts. User Guide Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Decoder-only Architecture Type: Network Architecture: Limitations =========== The model was trained on data that contains toxic language and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive. The model may answer with I statements, exhibiting some anthropomorphizing. This issue could be exacerbated without the use of the recommended prompt template. Input: ====== Input Type(s):Text (Prompt)Input Format(s):StringInput Parameters:One Dimensional (1D)Other Properties Related to Input:The model has a maximum of 4096 input tokens. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):Text (Response)Output Format:StringOutput Parameters:1DOther Properties Related to Output:The model has a maximum of 4096 input tokens. Maximum output for both versions can be set apart from input. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Prompt Format: ============== We recommend using the following prompt template, which was used to fine-tune the model. The model may not perform optimally without it. Single Turn • Note that a newline character\\nshould be added at the end of the prompt. • We recommend using<extra_id_1>as a stop token. Evaluation Results ================== Software Integration: (On-Device) ================================= Runtime(s):AI Inference Manager (NVAIM) Version 1.0.0Toolkit:NVAIMSeethis documentfor details on how to integrate the model into NVAIM. Runtime(s): Toolkit: this document (https://developer.nvidia.com/rtx/ai-inference-manager) Supported Hardware Platform(s):GPU supporting DirectX 11/12 and Vulkan 1.2 or higher Supported Hardware Platform(s): [Preferred/Supported] Operating System(s): • Windows Software Integration: (Cloud) ============================= Toolkit:NVIDIA NIMSeethis documentfor details on how to integrate the model into NVAIM. Toolkit: this document (https://developer.nvidia.com/rtx/ai-inference-manager) [Preferred/Supported] Operating System(s): • Linux Model Version(s) ---------------- Nemotron-4-Mini-Hindi-4B-Instruct Training & Evaluation Datasets: =============================== Training Dataset: ================= ** Data Collection Method by dataset • Hybrid: Automated, Human ** Labeling Method by dataset • Hybrid: Automated, Human Properties (Quantity, Dataset Descriptions, Sensor(s)): Trained of general Supervised Fine-Tuning (SFT) data followed by DPO on general and translated corpus. Evaluation Dataset: =================== ** Data Collection Method by dataset • Hybrid: Automated, Human ** Labeling Method by dataset • Human Properties (Quantity, Dataset Descriptions, Sensor(s)): 12 benchmark datasets includingIndicXtremeand other translated English Benchmarks likeMMLUandHellaswag. IndicXtreme (https://arxiv.org/abs/2212.05409) MMLU (https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu) Hellaswag (https://paperswithcode.com/dataset/hellaswag) Inference: ========== Engine:TRT-LLMTest Hardware [Name the specific test hardware model]: Engine: Test Hardware [Name the specific test hardware model]: • A100 • A10g • H100 • L40s Supported Hardware Platform(s):A10g, A100, L40s, H100 Supported Hardware Platform(s): Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++Explainability,Bias,Safety & Security, andPrivacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. Explainability (./explainability.md) Bias (./bias.md) Safety & Security (./safety.md) Privacy Subcards (./privacy.md) here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "deepfake-image-detection",
    "nvidia_url": "https://build.nvidia.com/hive/deepfake-image-detection",
    "scraped_on": "2025-10-15T21:41:56.202731",
    "tags": [
      "ai safety",
      "computer vision",
      "content moderation",
      "deep fake detection",
      "hive"
    ],
    "model_card": "Model Overview ============== Description: ============ Hive's Deepfake Image Detection model identifies whether or not an image is a deepfake. The model locates faces in an image. For each detected face, this model outputs a bounding box for its location, a classification, and accompanying confidence score. Image classification model that classifies whether an image is a deepfake. This model is ready for commercial and non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. License/Terms of Use: --------------------- https://thehive.ai/terms-of-use References (Leave Blank If None): ================================= Model Architecture: =================== Architecture Type:Convolution Neural Network (CNN)]Network Architecture:EfficientNet-B4, YoloV8 Architecture Type: Network Architecture: Input: ====== Input Type(s):Ensemble Gradient Boosted (EGB) ImageInput Format(s):PNG, JPEG, or JPGInput Parameters:Two-Dimensional (2D) Input Type(s): Input Format(s): Input Parameters: Output: ======= Output Type(s):Image with bounding boxes, TextOutput Format:JPEG, StringOutput Parameters:Two-Dimensional (2D) Image with the same shape as input, One Dimensional (1D) Output Type(s): Output Format: Output Parameters: Software Integration: ===================== Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Jetson • NVIDIA Hopper • NVIDIA Lovelace • NVIDIA Volta [Preferred/Supported] Operating System(s):Linux [Preferred/Supported] Operating System(s): Model Version(s): ================= v1.0 Training Dataset: ================= Training Dataset: ================= ** Data Collection Method by dataset • Unknown ** Labeling Method by dataset • Unknown Properties (Quantity, Dataset Descriptions, Sensor(s)):Trained on a large dataset comprising millions of artificially generated images and human-created images such as photographs, digital and traditional art, and memes sourced from across the web. Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference: ========== Engine:TensorRTTest Hardware:L40 Engine: Test Hardware: Ethical Considerations (For NVIDIA Models Only): ================================================ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "fourcastnet",
    "nvidia_url": "https://build.nvidia.com/nvidia/fourcastnet",
    "scraped_on": "2025-10-15T21:41:53.095804",
    "tags": [
      "ai weather prediction",
      "climate science",
      "earth-2",
      "nvidia",
      "weather simulation"
    ],
    "model_card": "Model Overview ============== Description =========== FourCastNet2 uses Spherical Fourier Neural Operator (SFNO) to predict a collection of surface and atmospheric variables such as wind speed, temperature and pressure and is applied to forecasting global atmospheric dynamics. FourCastNet is a data-driven model that provides accurate short to medium-range global predictions at a time-step size of 6 hours with predictive stability for over a year of simulated time (1,460 steps), while retaining physically plausible dynamics. This model is ready for commercial use. Reference(s) ============ • Spherical Fourier Neural Operator Paper Spherical Fourier Neural Operator Paper (https://arxiv.org/abs/2306.03838) • FourCastNet Paper FourCastNet Paper (https://arxiv.org/abs/2202.11214) • Codebase Codebase (https://github.com/NVIDIA/modulus) • The ERA5 global reanalysis The ERA5 global reanalysis (https://doi.org/10.1002/qj.3803) Model Architecture ================== Architecture Type:Neural OperatorNetwork Architecture:FourCastNet SFNO Architecture Type: Network Architecture: Input ===== Input Type(s): • Tensor (73 Surface & Atmospheric Variables) • DateTime Input Format(s):NumPyInput Parameters: Input Format(s): Input Parameters: • Four Dimensional (4D) (batch, variable, latitude, longitude) • Input DateTime Other Properties Related to Input: • 0.25 degree latitude-longitude grid • Input resolution: [721, 1440] • Latitude Coordinates: [90, 89.75, 89.5, ..., -89.5, -89.75, -90] • Longitude Coordinates: [0, 0.25, 0.5, ..., 359.25, 359.5, 359.75] • Input weather variables: \"u10m\", \"v10m\", \"u100m\", \"v100m\", \"t2m\", \"sp\", \"msl\", \"tcwv\", \"u50\", \"u100\", \"u150\", \"u200\", \"u250\", \"u300\", \"u400\", \"u500\", \"u600\", \"u700\", \"u850\", \"u925\", \"u1000\", \"v50\", \"v100\", \"v150\", \"v200\", \"v250\", \"v300\", \"v400\", \"v500\", \"v600\", \"v700\", \"v850\", \"v925\", \"v1000\", \"z50\", \"z100\", \"z150\", \"z200\", \"z250\", \"z300\", \"z400\", \"z500\", \"z600\", \"z700\", \"z850\", \"z925\", \"z1000\", \"t50\", \"t100\", \"t150\", \"t200\", \"t250\", \"t300\", \"t400\", \"t500\", \"t600\", \"t700\", \"t850\", \"t925\", \"t1000\", \"q50\", \"q100\", \"q150\", \"q200\", \"q250\", \"q300\", \"q400\", \"q500\", \"q600\", \"q700\", \"q850\", \"q925\", \"q1000\" Output ====== Output Type(s): • Tensor (73 Surface & Atmospheric Variables) Output Format(s):NumPyOutput Parameters: Output Format(s): Output Parameters: • Four Dimensional (4D) (batch, variable, latitude, longitude) Other Properties Related to Output: • Time-delta of 6 hours from input array • 0.25 degree latitude-longitude grid • Output resolution: [721, 1440] • Latitude Coordinates: [90, 89.75, 89.5, ..., -89.5, -89.75, -90] • Longitude Coordinates: [0, 0.25, 0.5, ..., 359.25, 359.5, 359.75] • Output weather variables: \"u10m\", \"v10m\", \"u100m\", \"v100m\", \"t2m\", \"sp\", \"msl\", \"tcwv\", \"u50\", \"u100\", \"u150\", \"u200\", \"u250\", \"u300\", \"u400\", \"u500\", \"u600\", \"u700\", \"u850\", \"u925\", \"u1000\", \"v50\", \"v100\", \"v150\", \"v200\", \"v250\", \"v300\", \"v400\", \"v500\", \"v600\", \"v700\", \"v850\", \"v925\", \"v1000\", \"z50\", \"z100\", \"z150\", \"z200\", \"z250\", \"z300\", \"z400\", \"z500\", \"z600\", \"z700\", \"z850\", \"z925\", \"z1000\", \"t50\", \"t100\", \"t150\", \"t200\", \"t250\", \"t300\", \"t400\", \"t500\", \"t600\", \"t700\", \"t850\", \"t925\", \"t1000\", \"q50\", \"q100\", \"q150\", \"q200\", \"q250\", \"q300\", \"q400\", \"q500\", \"q600\", \"q700\", \"q850\", \"q925\", \"q1000\" Software Integration ==================== Runtime Engine(s):Not ApplicableSupported Hardware Microarchitecture Compatibility: Runtime Engine(s): Supported Hardware Microarchitecture Compatibility: • Ampere • Hopper • Turing Supported Operating System(s): • Linux Model Version(s) ================ Model version:v1 Model version: Training, Testing, and Evaluation Datasets: =========================================== Training Dataset ================ Link:ERA5 Link: ERA5 (https://cds.climate.copernicus.eu/) Data Collection Method by dataset • Automatic/Sensors Labeling Method by dataset • Automatic/Sensors Properties (Quantity, Dataset Descriptions, Sensor(s)):ERA5 data for the years of 1979-2017. ERA5 provides hourly estimates of various atmospheric, land, and oceanic climate variables. The data covers the Earth on a 30km grid and resolves the atmosphere at 137 levels. Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset ================== Link:ERA5 Link: ERA5 (https://cds.climate.copernicus.eu/) Data Collection Method by dataset • Automatic/Sensors Labeling Method by dataset • Automatic/Sensors Properties (Quantity, Dataset Descriptions, Sensor(s)):ERA5 data for the year of 2018. ERA5 provides hourly estimates of various atmospheric, land, and oceanic climate variables. The data covers the Earth on a 30km grid and resolves the atmosphere at 137 levels. Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference: ========== Engine:TritonTest Hardware: Engine: Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware: • A100 • H100 • L40S • RTX6000 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcardshere. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://ai.nvidia.com) here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) License ======= This model is licensed under theNVIDIA AI Product Agreement. By pulling and using this model, you accept the terms and conditions of this license. NVIDIA AI Product Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws."
  },
  {
    "name": "corrdiff",
    "nvidia_url": "https://build.nvidia.com/nvidia/corrdiff",
    "scraped_on": "2025-10-15T21:41:52.765747",
    "tags": [
      "ai weather prediction",
      "earth-2",
      "nvidia",
      "weather simulation"
    ],
    "model_card": "Model Overview ============== Description =========== Corrector Diffusion (CorrDiff) US GEFS-HRRR model down-scales several surface and atmospheric variables from 25-km resolution forecast data from the Global Ensemble Forecast System (GEFS) and predicts 3-km resolution High-Resolution Rapid Refresh (HRRR) data. CorrDiff US allows the prediction of high-fidelity stochastic weather phenomena over the CONUS from low-fidelity input data that would otherwise require expensive regional numerical simulations. CorrDiff is a generative downscaling model trained over the contiguous United States (CONUS). This model is ready for commercial use. Reference(s) ============ • CorrDiff Paper CorrDiff Paper (https://arxiv.org/pdf/2309.15214) • Codebase Codebase (https://github.com/NVIDIA/modulus) • Global Ensemble Forecast System Global Ensemble Forecast System (https://www.ncei.noaa.gov/products/weather-climate-models/global-ensemble-forecast) • The High-Resolution Rapid Refresh The High-Resolution Rapid Refresh (https://rapidrefresh.noaa.gov/hrrr/) Model Architecture ================== Architecture Type:DiffusionNetwork Architecture:Patch-Based Corrector Diffusion Architecture Type: Network Architecture: Input ===== Input Type(s): • Tensor (38 Surface & Atmospheric Variables + Forecast Lead Time) • Input data forecast lead time in hours Input Format(s):NumPyInput Parameters: Input Format(s): Input Parameters: • Four Dimensional (4D) (batch, variable, latitude, longitude) • Integer (Lead Time in Hours) Other Properties Related to Input: • 0.25 degree latitude-longitude grid bounded over CONUS • Input resolution: [129, 301] • Lattitude Coordinates: [53, 52.75, 52.5, ..., 21.5, 21.25, 21] • Longitude Coordinates: [225, 225.25, 225.5, ..., 299.5, 299.75, 300] • Input weather variables: \"u10m\", \"v10m\", \"t2m\", \"r2m\", \"sp\", \"msl\",\"tcwv\", \"u1000\", \"u925\", \"u850\", \"u700\", \"u500\", \"u250\", \"v1000\", \"v925\", \"v850\", \"v700\", \"v500\", \"v250\", \"z1000\", \"z925\", \"z850\", \"z700\", \"z500\", \"z200\", \"t1000\", \"t925\", \"t850\", \"t700\", \"t500\", \"t100\", \"r1000\", \"r925\", \"r850\", \"r700\", \"r500\", \"r100\" Output ====== Output Type(s):Tensor (8 Surface & Atmospheric Variables)Output Format:NumPyOutput Parameters:5D (batch, samples, variable, latitude, longitude)Other Properties Related to Output: Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: • 3-km lambert-conformal projection over CONUS of resolution • Output resolution: [1056, 1792] • Output weather variables: \"u10m\", \"v10m\", \"t2m\", \"tp\", \"csnow\", \"cicep\", \"cfrzr\", \"crain\" The output is on a cropped window of the grid used by HRRR. Refer to theHRRR documentationfor additional information on this grid. The output coordinates can be obtained from thecorrdiff_output_lat.npyandcorrdiff_output_lon.npyfiles in the model package. HRRR documentation (https://rapidrefresh.noaa.gov/hrrr/) Software Integration ==================== Runtime Engine(s):Not ApplicableSupported Hardware Microarchitecture Compatibility: Runtime Engine(s): Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper • NVIDIA Turing Supported Operating System(s): • Linux Model Version(s) ================ Model version:v1 Model version: Training, Testing, and Evaluation Datasets: =========================================== Training Dataset ================ Link:GEFS Link: GEFS (https://www.ncei.noaa.gov/products/weather-climate-models/global-ensemble-forecast) Data Collection Method by dataset • Automatic/Sensors Labeling Method by dataset • Automatic/Sensors Properties (Quantity, Dataset Descriptions, Sensor(s)):GEFS data for the date range of 2020/12/02 to 2023/12/31. The Global Ensemble Forecast System (GEFS) is a weather model created by the National Centers for Environmental Prediction (NCEP) that generates 21 separate forecasts (ensemble members) to address underlying uncertainties in the input data such limited coverage, instruments or observing systems biases, and the limitations of the model itself. Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:HRRR Link: HRRR (https://rapidrefresh.noaa.gov/hrrr/) Data Collection Method by dataset • Automatic/Sensors Labeling Method by dataset • Automatic/Sensors Properties (Quantity, Dataset Descriptions, Sensor(s)):HRRR data for the date range of 2020/12/02 to 2023/12/31. The HRRR is a NOAA real-time 3-km resolution, hourly updated, cloud-resolving, convection-allowing atmospheric model, initialized by 3km grids with 3km radar assimilation. Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset ================== Link:GEFS Link: GEFS (https://www.ncei.noaa.gov/products/weather-climate-models/global-ensemble-forecast) Data Collection Method by dataset • Automatic/Sensors Labeling Method by dataset • Automatic/Sensors Properties (Quantity, Dataset Descriptions, Sensor(s)):GEFS data for the date range of 2024/01/01 to 2024/07/31. The Global Ensemble Forecast System (GEFS) is a weather model created by the National Centers for Environmental Prediction (NCEP) that generates 21 separate forecasts (ensemble members) to address underlying uncertainties in the input data such limited coverage, instruments or observing systems biases, and the limitations of the model itself. Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:HRRR Link: HRRR (https://rapidrefresh.noaa.gov/hrrr/) Data Collection Method by dataset • Automatic/Sensors Labeling Method by dataset • Automatic/Sensors Properties (Quantity, Dataset Descriptions, Sensor(s)):HRRR data for the date range of 2024/01/01 to 2024/07/31. The HRRR is a NOAA real-time 3-km resolution, hourly updated, cloud-resolving, convection-allowing atmospheric model, initialized by 3km grids with 3km radar assimilation. Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference: ========== Engine:TritonTest Hardware: Engine: Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware: • A100 • H100 • L40S • RTX6000 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcardshere. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://ai.nvidia.com) here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "audio2face-3d",
    "nvidia_url": "https://build.nvidia.com/nvidia/audio2face-3d",
    "scraped_on": "2025-10-15T21:41:49.402797",
    "tags": [
      "audio-to-face",
      "digital humans",
      "nvidia",
      "nvidia nim",
      "speech-to-animation"
    ],
    "model_card": "Model Overview ============== Description =========== NVIDIA Audio2Face-3D is a microservice for animating 3D character's facial characteristics to match any audio track, whether for a game, film, or real-time digital assistant. This model is designed for commercial use. NVIDIA Audio2Emotion is embedded within Audio2Face, and it is designed to automatically recognize the emotions in human speech. These predictions are used to drive the Audio2Face avatar’s facial expressions to make it even more natural. Licenses ======== EULA information is availablehere. Customer will use the Software exclusively for authorized purposes, consistent with the Agreement’s terms and all applicable laws, regulations and the rights of others. here (https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/) Model Architecture ================== Architecture Type: • Audio2Face: CNN • Audio2Emotion: Transformer Network Architecture • Audio2Face: wav2vec2.0 • Audio2Emotion: wav2vec2.0 Input ===== Input Type(s):AudioInput Format:.wavInput Parameters: 2D:(Tuning Parameters and Audio)Other Properties Related to Input:Supported Sampling rates: 22.05KHz, 44.1KHz, 16KHz; All audio is resampled to 16KHz. There is no max audio length. Input Type(s): Input Format: Input Parameters: 2D: Other Properties Related to Input: Output: ======= Output Type(s): • Audio2Face: Blendshape Coefficients representing 3D facial animation throughout time • Audio2Emotion: Emotion Probability Coefficients representing 1D emotion values throughout time Output Format:Custom Protobuf FormatOutput Parameters: 2D:Custom Protobuf FormatOther Properties Related to Output:N/A Output Format: Output Parameters: 2D: Other Properties Related to Output: Software Integration ==================== Runtime Engine(s): • DeepStream-7.1 Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper • NVIDIA Lovelace • NVIDIA Pascal • NVIDIA Turing Preferred/Supported Operating System(s) ======================================= • Linux • WSL Model Versions: =============== Audio2Face: • Mark v2.3 • Claire v2.3 • James v2.3 Audio2Emotion: • v1.0 Training and Evaluation Dataset =============================== Data Collection Method by dataset: • Audio2Face: Human • Audio2Emotion: Automated Labeling Method by dataset: • Human Properties (Quantity, Dataset Descriptions, Sensor(s)): • Audio2Face: Multi-speaker English audio from microphone resampled at 16kHz across multiple audio types and frequency ranges. • Audio2Emotion: Multi-speaker English audio from microphone resampled at 16kHz across multiple audio types and frequency ranges. The datasets consist of multiple datasets, including RAVDESS, CREMA-D, JL, and Lindy & Rodney. Total quantity: ~18000 samples. Inference ========= Engine:TensorRTTest Hardware:A100 Engine: Test Hardware: Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. You may not use the Software or any of its components for the purpose of emotion recognition. Any technology included in the Software may only be used as fully integrated in the Software and consistent with all applicable documentation. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcardshere. here (https://build.nvidia.com/nvidia/audio2face-3d/modelcard/) Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Disclaimer ========== AI models generate responses and outputs based on complex algorithms and machine learning techniques, and those responses or outputs may be inaccurate or indecent. By testing this model, you assume the risk of any harm caused by any response or output of the model. Please do not upload any confidential information or personal data. Your use is logged for security."
  },
  {
    "name": "paddleocr",
    "nvidia_url": "https://build.nvidia.com/baidu/paddleocr",
    "scraped_on": "2025-10-15T21:41:46.097181",
    "tags": [
      "baidu",
      "data ingestion",
      "extraction",
      "nemo retriever",
      "optical character detection",
      "optical character recognition",
      "run-on-rtx",
      "table extraction"
    ],
    "model_card": "Model Overview ============== Description ----------- PaddleOCR is an ultra-lightweight Optical Character Recognition (OCR) system developed by Baidu. It supports a variety of cutting-edge OCR algorithms and provides value at every stage of the AI pipeline, including data generation, model training, and inference. This model is ready for commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAPaddleOCR Toolkit. PaddleOCR Toolkit (https://github.com/PaddlePaddle/PaddleOCR) Terms of use ------------ PaddleOCR is licensed underApache-2.You are responsible for ensuring that your use of models complies with all applicable laws. Apache-2 (https://www.apache.org/licenses/LICENSE-2.0) You are responsible for ensuring that your use of models complies with all applicable laws. References ---------- GithubArxiv Github (https://github.com/PaddlePaddle/PaddleOCR/blob/main/README_en.md) Arxiv (https://arxiv.org/abs/2206.03001) Model Architecture ================== Architecture Type for Text Detector:CNNNetwork Architecture for Text Detector:LK-PAN Architecture Type for Text Detector: Network Architecture for Text Detector: Architecture Type for Text Recognition:Hybrid Transformer CNNNetwork Architecture for Text Recognition:SVTR-LCNet (NRTR Head and CTCLoss head) Architecture Type for Text Recognition: Network Architecture for Text Recognition: Input ===== Input Type(s):ImageInput Format(s):Red, Green, Blue (RGB)Input Parameters:Two Dimensional (2D)Other Properties Related to Input:nd array, or batch of nd arrays are passed in with shape [Batch, Channel, Width, Height]. PaddleOCR does some internal thresholding, but none was implemented from our side. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output ====== Output Type(s):TextOutput Format:StringOutput Parameters:1DOther Properties Related to Output:Batch of text strings. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Supported Hardware Microarchitecture Compatibility:NVIDIA Ampere, NVIDIA Hopper, NVIDIA Lovelace Supported Hardware Microarchitecture Compatibility: Supported Operating System(s): ============================== • Linux Model Version(s): ================= • baidu/paddleocr Training Dataset: ================= Link: Text detection datasets include LSVT (Sun et al. 2019), RCTW-17 (Shiet al. 2017), MTWI 2018 (He and Yang 2018), CASIA-10K (He et al. 2018), SROIE (Huang et al. 2019), MLT 2019 (Nayef et al. 2019), BDI (Karatzas et al. 2011), MSRATD500 (Yao et al. 2012) and CCPD 2019 (Xu et al. 2018). These are two of the datasets (among others) which are used for text recognition:OpenImagesInvoiceDatasets OpenImages (https://github.com/openimages/dataset) InvoiceDatasets (https://github.com/FuxiJia/InvoiceDatasets) Data Collection Method by dataset:UnknownLabeling Method by datasetUnknown Data Collection Method by dataset: Labeling Method by dataset Text Detection: 127k training images (68K real scene images from Baidu image search and public datasets and 59K synthetic images) Text Recognition: 18.5M training images (7M real scene images from Baidu image search and public datasets and 11.5M synthetic images) Inference: ========== Engine:Tensor(RT)Test Hardware:Tested on all supported hardware listed in compatibility section Engine: Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nv-yolox-page-elements-v1",
    "nvidia_url": "https://build.nvidia.com/nvidia/nv-yolox-page-elements-v1",
    "scraped_on": "2025-10-15T21:41:42.790646",
    "tags": [
      "chart detection",
      "data ingestion",
      "extraction",
      "nemo retriever",
      "nvidia",
      "object detection",
      "run-on-rtx",
      "table detection"
    ],
    "model_card": "Model Overview ============== Description ----------- YOLOX is an anchor-free version of YOLO (You Only Look Once) one-shot object detection model series, with a simpler design, better performance and less restrictive license. It’s from Megvii Technology. This model is a YOLOX-L version fine-tuned on 26,000 images fromDigital Corpora dataset, with annotations from Azure AI Document Intelligence. The model is trained to detect tables, charts and titles in documents. Digital Corpora dataset (https://digitalcorpora.org/) This model is ready for commercial use. License/Terms of use -------------------- Use of this model is governed by theNVIDIA AI Foundation Models Community License Agreementand theApache 2.0 License. NVIDIA AI Foundation Models Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-ai-foundation-models-community-license-agreement/#:~:text=This%20license%20agreement%20(%E2%80%9CAgreement%E2%80%9D,algorithms%2C%20parameters%2C%20configuration%20files%2C)) Apache 2.0 License (https://github.com/Megvii-BaseDetection/YOLOX/blob/main/LICENSE) Model Architecture ------------------ Architecture Type:YoloxNetwork Architecture:DarkNet53 Backbone + FPN Decoupled head (one 1x1 convolution + 2 parallel 3x3 convolutions (one for the classification and one for the bounding box prediction) Architecture Type: Network Architecture: YOLOX is a single-stage object detector that improves on Yolo-v3. The model is fine-tuned to detect three classes of objects in documents: table, chart, title. Chart is defined as a bar chart, line chart or pie chart. Titles can be page titles, section titles, or table/chart titles. Model Version(s) ---------------- Short name: YOLOX Document Structure Detection Intended use ------------ YOLOX Model is suitable for users who want to extract tables, text titles and charts from documents. It can be used for document analysis, document understanding, and document processing. The goal is to extract structural elements (tables and charts) from the page to allow vision models to be applied for information extraction. Technical Details ================= Input ----- Input Type(s):ImageInput Format(s):Red, Green, Blue (RGB)Input Parameters:Two Dimensional (2D)Other Properties Related to Input:Image size resized to (1024, 1024) Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output ------ Output Type(s):ArrayOutput Format:dict of dictionaries containing np.ndarray. Outer dictionary contains each sample (page). Inner dictionary contains list of dictionaries with bboxes, type and confidence for that pageOutput Parameters:n/aOther Properties Related to Output:Output contains Bounding box, detection confidence and object class (chart, table, title). Thresholds used for nms - conf_thresh = 0.01; iou_thresh = 0.5; max_per_img = 100; min_per_img = 0 Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration -------------------- Runtime:NeMo Retriever YOLOX Structured Images NIMSupported Hardware Microarchitecture Compatibility:NVIDIA Ampere, NVIDIA Hopper, NVIDIA Lovelace Runtime: Supported Hardware Microarchitecture Compatibility: Supported Operating System(s): ============================== • Linux Model Version(s): ================= • nvidia/nv-yolox-structured-images-v1 Training Dataset & Evaluation ============================= Training Dataset ---------------- Data Collection Method by dataset:AutomatedLabeling Method by dataset:Automated Data Collection Method by dataset: Labeling Method by dataset: Pretraining:COCO train2017 COCO train2017 (https://cocodataset.org/#download) Finetuning (by NVIDIA): 25,832 images fromDigital Corpora dataset, with annotations fromAzure AI Document Intelligence. Digital Corpora dataset (https://digitalcorpora.org/) Azure AI Document Intelligence (https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence) Number of bounding boxes per class: 30,099 tables, 34,369 titles and 8,363 charts. The layout model of Document Intelligence was used with2024-02-29-previewapi version. Evaluation Results ------------------ The primary evaluation set is a cut of the azure labels and digital corpora images. Number of bounding boxes per class: 1704 tables, 1906 titles and 367 charts. mAP was used as an evaluation metric. Data Collection Method by dataset:AutomatedLabeling Method by dataset:Automated, HumanProperties (Quantity, Dataset Descriptions, Sensor(s)):We evaluated with azure labels from held out pages, as well as manual inspection on public PDFs and powerpoint slides. Data Collection Method by dataset: Labeling Method by dataset: Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference: ========== Engine:TensorRTTest Hardware:See Support Matrix from NIM documentation. Engine: Test Hardware: Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ tab for the Explainability, Bias, Safety & Security, and Privacy subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Model Card++ ============ Bias ---- Field Response protected classes (https://www.senate.ca.gov/content/protected-classes) Explainability -------------- NVIDIA AI Foundation Models Community License Agreement (https://developer.nvidia.com/downloads/nv-ai-foundation-models-license) MIT License (MIT) (https://github.com/microsoft/unilm/blob/master/LICENSE) Privacy ------- Safety and Security ------------------- NVIDIA AI Foundation Models Community License Agreement (https://developer.nvidia.com/downloads/nv-ai-foundation-models-license)"
  },
  {
    "name": "cached",
    "nvidia_url": "https://build.nvidia.com/university-at-buffalo/cached",
    "scraped_on": "2025-10-15T21:41:39.597139",
    "tags": [
      "chart element detection",
      "image-to-text",
      "nemo retriever",
      "university-at-buffalo"
    ],
    "model_card": "Model Overview ============== Description ----------- CACHED (Context-Aware Chart Element Detection) is a state-of-the-art chart element detection model from University at Buffalo. It was published in Document Analysis and Recognition - ICDAR 2023 conference. The code is based on the MMDetection Framework. CACHED is associated with PaddleOCR to perform Optical Character Recognition (OCR). PaddleOCR is an ultra lightweight OCR system by Baidu. This model is ready for commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see links to Non-NVIDIAContext-Aware Chart Element Detection GitHubandPaddleOCR Toolkit. Context-Aware Chart Element Detection GitHub (https://github.com/pengyu965/ChartDete) PaddleOCR Toolkit (https://github.com/PaddlePaddle/PaddleOCR) Terms of use ------------ CACHED is licensed underMIT. PaddleOCR is licensed underApache-2. MIT (https://opensource.org/license/mit) Apache-2 (https://www.apache.org/licenses/LICENSE-2.0) You are responsible for ensuring that your use of models complies with all applicable laws. References ---------- • CACHEDGithubArxiv • Github Github (https://github.com/pengyu965/ChartDete) • Arxiv Arxiv (https://arxiv.org/abs/2305.04151) • PaddleOCRGithubArxiv • Github Github (https://github.com/PaddlePaddle/PaddleOCR/blob/main/README_en.md) • Arxiv Arxiv (https://arxiv.org/abs/2206.03001) Model Architecture ================== • CACHEDArchitecture Type:Region-Based Convolutional Neural Network (R-CNN)Network Architecture:Cascade with local-global context fusion module • Architecture Type:Region-Based Convolutional Neural Network (R-CNN) Architecture Type: • Network Architecture:Cascade with local-global context fusion module Network Architecture: • PaddleOCRArchitecture Type for Text Detector:CNNNetwork Architecture for Text Detector:LK-PANArchitecture Type for Text Recognition:Hybrid Transformer CNNNetwork Architecture for Text Recognition:SVTR-LCNet (NRTR Head and CTCLoss head) • Architecture Type for Text Detector:CNN Architecture Type for Text Detector: • Network Architecture for Text Detector:LK-PAN Network Architecture for Text Detector: • Architecture Type for Text Recognition:Hybrid Transformer CNN Architecture Type for Text Recognition: • Network Architecture for Text Recognition:SVTR-LCNet (NRTR Head and CTCLoss head) Network Architecture for Text Recognition: Input ===== Input Type(s):ImageInput Format(s):Red, Green, Blue (RGB)Input Parameters:Two Dimensional (2D)Other Properties Related to Input:Expected input is a nd array image of shape[Channel, Width, Height], or a nd array batch of image of shape[Batch, Channel, Width, Height]. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output ====== Output Type(s):Text associated to each of the following classes :\"chart_title\", \"x_title\", \"y_title\", \"xlabel\", \"ylabel\", \"other\", \"legend_label\", \"legend_title\", \"mark_label\", \"value_label\"Output Format:Dict of StringOutput Parameters:1DOther Properties Related to Output:None Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Supported Hardware Microarchitecture Compatibility:NVIDIA Ampere, NVIDIA Hopper, NVIDIA Lovelace Supported Hardware Microarchitecture Compatibility: Supported Operating System(s): ============================== • Linux Model Version(s): ================= • university-at-buffalo/cached Training Dataset: ================= None of the models were trained by NVIDIA. • PubMed Central (PMC) Chart DatasetLink:https://chartinfo.github.io/index_2022.html. No nSpect ID.Data Collection Method:Automated, HumanLabeling Method: HumanDescription:A real-world dataset collected from PubMed Central Documents and manually annotated, released in the ICPR 2022 CHART-Infographic competition. There are 5614 images for chart element detection, 4293 images for final plot detection and data extraction, and 22924 images for chart classification. PubMed Central (PMC) Chart Dataset • Link:https://chartinfo.github.io/index_2022.html. No nSpect ID. Link: https://chartinfo.github.io/index_2022.html • Data Collection Method:Automated, Human Data Collection Method: • Labeling Method: Human Labeling Method • Description:A real-world dataset collected from PubMed Central Documents and manually annotated, released in the ICPR 2022 CHART-Infographic competition. There are 5614 images for chart element detection, 4293 images for final plot detection and data extraction, and 22924 images for chart classification. Description: • Text detection and recognition datasetsLink:Refer tohttps://github.com/PaddlePaddle/PaddleOCR. No nSpect ID.Data Collection Method:Human, Synthetic, UnknownLabeling Method: Human, Synthetic, UnknownDescription:PaddleOCR is trained on a collection of OCR datasets which includes: LSVT (Sun et al. 2019), RCTW-17 (Shiet al. 2017), MTWI 2018 (He and Yang 2018), CASIA-10K (He et al. 2018), SROIE (Huang et al. 2019), MLT 2019 (Nayef et al. 2019), BDI (Karatzas et al. 2011), MSRATD500 (Yao et al. 2012) and CCPD 2019 (Xu et al. 2018). Text detection and recognition datasets • Link:Refer tohttps://github.com/PaddlePaddle/PaddleOCR. No nSpect ID. Link: https://github.com/PaddlePaddle/PaddleOCR • Data Collection Method:Human, Synthetic, Unknown Data Collection Method: • Labeling Method: Human, Synthetic, Unknown Labeling Method • Description:PaddleOCR is trained on a collection of OCR datasets which includes: LSVT (Sun et al. 2019), RCTW-17 (Shiet al. 2017), MTWI 2018 (He and Yang 2018), CASIA-10K (He et al. 2018), SROIE (Huang et al. 2019), MLT 2019 (Nayef et al. 2019), BDI (Karatzas et al. 2011), MSRATD500 (Yao et al. 2012) and CCPD 2019 (Xu et al. 2018). Description: Inference: ========== Engine:Tensor(RT)Test Hardware:Tested on all supported hardware listed in compatibility section Engine: Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.3-70b-instruct",
    "nvidia_url": "https://build.nvidia.com/meta/llama-3_3-70b-instruct",
    "scraped_on": "2025-10-15T21:41:36.311180",
    "tags": [
      "chat",
      "code generation",
      "instruction following",
      "math",
      "meta",
      "reasoning",
      "text-to-text"
    ],
    "model_card": "Model Information ================= The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction-tuned text-only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. Models are accelerated byTensorRT-LLM, a library for optimizing Large Language Model (LLM) inference on NVIDIA GPUs. TensorRT-LLM (https://github.com/NVIDIA/TensorRT-LLM) This model is ready for commercial use. Model Developer: Meta Model Developer Model Architecture:Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Model Architecture: Supported languages:English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai. Supported languages: Llama 3.3 model. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability. Llama 3.3 model Model Release Date: • 70B Instruct: December 6, 2024 70B Instruct: December 6, 2024 Status:This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback. Status: LicenseA custom commercial license, the Llama 3.3 Community License Agreement, is available at:https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE License https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE Intended Use ============ Intended Use CasesLlama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases. Intended Use Cases Out-of-scopeUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card**. Out-of-scope **Note: Llama 3.3 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner. Hardware and Software ===================== Training FactorsWe used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure. Training Factors Training Energy UseTraining utilized a cumulative of39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. Training Energy Use 39.3 Training Greenhouse Gas EmissionsEstimated total location-based greenhouse gas emissions were11,390tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq. Training Greenhouse Gas Emissions 11,390 The methodology used to determine training energy use and greenhouse gas emissions can be foundhere. Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others. here (https://arxiv.org/pdf/2204.05149) Training Data ============= Overview:Llama 3.3 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. Overview: Data Freshness:The pretraining data has a cutoff of December 2023. Data Freshness: Benchmarks - English Text ========================= In this section, we report the results for Llama 3.3 relative to our previous models. Instruction Tuned models ------------------------ Responsibility & Safety ======================= As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks: • Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. • Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm. • Provide protections for the community to help prevent the misuse of our models. Responsible deployment ---------------------- Llama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in ourCommunity Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.3 was developed following the best practices outlined in our Responsible Use Guide, you can refer to theResponsible Use Guideto learn more. Community Stories webpage (https://llama.meta.com/community-stories/) Responsible Use Guide (https://llama.meta.com/responsible-use-guide/) Llama 3.3 Instruct ------------------ Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. Fine-tuning dataWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. Fine-tuning data Refusals and ToneBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. Refusals and Tone Llama 3.3 Systems ----------------- Large language models, including Llama 3.3, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. Large language models, including Llama 3.3, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. As part of our responsible release approach, we provide the community withsafeguardsthat developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All ourreference implementationsdemos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. safeguards (https://llama.meta.com/trust-and-safety/) reference implementations (https://github.com/meta-llama/llama-agentic-system) Capability specific considerations ---------------------------------- Tool-use: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. Tool-use Multilinguality: Llama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. Multilinguality Evaluations ----------- We evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application. Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization. Red TeamingFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. Red Teaming We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. . Critical and Other Risks ------------------------ We specifically focused our efforts on mitigating the following critical risk areas: ------------------------------------------------------------------------------------ 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulnessTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of the Llama 3.3 model could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness 2. Child Safety Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. 3. Cyber Attack EnablementOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed. 3. Cyber Attack Enablement Our attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Community --------- Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository. Github repository (https://github.com/meta-llama/PurpleLlama) We also set up theLlama Impact Grantsprogram to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be foundhere. Llama Impact Grants (https://llama.meta.com/llama-impact-grants/) here (https://llama.meta.com/llama-impact-grants/#finalists) Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community. output reporting mechanism (https://developers.facebook.com/llama_output_feedback) bug bounty program (https://www.facebook.com/whitehat) Ethical Considerations and Limitations ====================================== The core values of Llama 3.3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. But Llama 3.3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.3 model, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including ourResponsible Use Guide,Trust and Safetysolutions, and otherresourcesto learn more about responsible development. Responsible Use Guide (https://llama.meta.com/responsible-use-guide) Trust and Safety (https://llama.meta.com/trust-and-safety/) resources (https://llama.meta.com/docs/get-started/) Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "usdcode",
    "nvidia_url": "https://build.nvidia.com/nvidia/usdcode",
    "scraped_on": "2025-10-15T21:41:33.110579",
    "tags": [
      "chat",
      "code generation",
      "digital twin",
      "nvidia",
      "nvidia nim",
      "openusd",
      "synthetic data generation"
    ],
    "model_card": "Model Overview ============== Description: ============ USD Code is an OpenUSD Python code generation and knowledge answering model that helps developers to write OpenUSD code and answer OpenUSD knowledge questions. The following NIM are used by USD Code: • Llama-3.1-70b-instruct Llama-3.1-70b-instruct (https://build.nvidia.com/meta/llama-3_1-70b-instruct) • nv-embedqa-e5-v5 nv-embedqa-e5-v5 (https://build.nvidia.com/nvidia/nv-embedqa-e5-v5/) Llama-3.1-70b-instruct is used to drive the code generation and the agentic workflow, while NVIDIA Retrieval QA E5 Embedding is used for Retrieval Augmented Generation (RAG) to answer OpenUSD knowledge questions, USD code generation, and high-level Helper Function-based code generation. Helper Functions provide high-level abstractions leveraging the USD API, enabling developers to efficiently manage complex tasks such as creating, modifying, and querying USD scene. This model is ready for commercial use. Licenses: ========= If you download the software and materials as available from the NVIDIA AI product portfolio, use is governed by theNVIDIA Software License Agreementand theProduct-Specific Terms for NVIDIA AI Products; except for the model which is governed by theNVIDIA AI Foundation Models Community License Agreement, and the RAG dataset which is governed by the terms of theNVIDIA Asset License. NVIDIA Software License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) Product-Specific Terms for NVIDIA AI Products (https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/) NVIDIA AI Foundation Models Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-ai-foundation-models-community-license-agreement/) NVIDIA Asset License (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/omniverse-usdcode/resources/usdcode_asset_license/files) ADDITIONAL INFORMATION: For Llama model, Llama 3.1 Community License Agreement, Built with Llama; for NV-EmbedQA-E5-v5: MIT license; for NV-EmbedQA-Mistral7B-v2: Apache 2.0 license, and Snowflake arctic-embed-l: Apache 2.0 license. If you download the software and materials as available from the NVIDIA Omniverse portfolio, use is governed by theNVIDIA Software License Agreementand theProduct-Specific Terms for NVIDIA Omniverse; except for the model which is governed by theNVIDIA AI Foundation Models Community License Agreement, and the RAG dataset which is governed by the terms of theNVIDIA Asset License. NVIDIA Software License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) Product-Specific Terms for NVIDIA Omniverse (https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-omniverse/) NVIDIA AI Foundation Models Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-ai-foundation-models-community-license-agreement/) NVIDIA Asset License (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/omniverse-usdcode/resources/usdcode_asset_license/files) ADDITIONAL INFORMATION: For Llama model, Llama 3.1 Community License Agreement, Built with Llama; for NV-EmbedQA-E5-v5: MIT license; for NV-EmbedQA-Mistral7B-v2: Apache 2.0 license, and Snowflake arctic-embed-l: Apache 2.0 license. References: =========== • Llama-3.1 -https://ai.meta.com/blog/meta-llama-3-1/ https://ai.meta.com/blog/meta-llama-3-1/ • NVIDIA Retrieval QA E5 Embedding -https://build.nvidia.com/nvidia/nv-embedqa-e5-v5/ https://build.nvidia.com/nvidia/nv-embedqa-e5-v5/ • OpenUSD -https://www.openusd.org/ https://www.openusd.org/ Model Architecture: =================== • Llama-3.1-70b-instructArchitecture Type:TransformerNetwork Architecture:Llama-3.1 • Architecture Type:Transformer Architecture Type: • Network Architecture:Llama-3.1 Network Architecture: • NVIDIA Retrieval QA E5 EmbeddingArchitecture Type:TransformerNetwork Architecture:Fine-tuned E5-Large-Unsupervised retriever • Architecture Type:Transformer Architecture Type: • Network Architecture:Fine-tuned E5-Large-Unsupervised retriever Network Architecture: Input ===== Input Type(s):TextInput Format(s):StringInput Parameter(s):One Dimentional (1D)Other Properties Related to Input:Max context length of 128k tokens Input Type(s): Input Format(s): Input Parameter(s): Other Properties Related to Input: Output ====== Output Type(s):Text (Code, Python)Output Format:StringOutput Parameter(s):One Dimentional (1D)Other Properties Related to Output:Max output length of 128k tokens Output Type(s): Output Format: Output Parameter(s): Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s): • TensorRT Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper [Preferred/Supported] Operating System(s): • Linux Model Version(s): ================= • Llama-3.1llama-3.1-70b-instruct:1.3.0 • llama-3.1-70b-instruct:1.3.0 • NVIDIA Retrieval QA E5 Embeddingnv-embedqa-e5-v5:1.1.0 • nv-embedqa-e5-v5:1.1.0 Training Dataset: ================= • Llama-3.1-70b-instructLlama-3.1-70b-instruct were not trained by NVIDIA.Link:https://build.nvidia.com/meta/llama-3_1-70b-instruct/modelcardData Collection Method:Hybrid: Human, SyntheticLabeling Method:UnknownDescription:The 70B Llama 3.1 model is trained on a new mix of publicly available online data, supports multilingual text input and output (including code), has a 128k context length, 15T+ tokens, GQA enabled, and a knowledge cutoff of December 2023. Llama-3.1-70b-instruct • Llama-3.1-70b-instruct were not trained by NVIDIA. • Link:https://build.nvidia.com/meta/llama-3_1-70b-instruct/modelcard Link https://build.nvidia.com/meta/llama-3_1-70b-instruct/modelcard • Data Collection Method:Hybrid: Human, Synthetic Data Collection Method: • Labeling Method:Unknown Labeling Method: • Description:The 70B Llama 3.1 model is trained on a new mix of publicly available online data, supports multilingual text input and output (including code), has a 128k context length, 15T+ tokens, GQA enabled, and a knowledge cutoff of December 2023. Description: • NVIDIA Retrieval QA E5 EmbeddingLink:https://build.nvidia.com/nvidia/nv-embedqa-e5-v5/modelcardData Collection Method:UnknownLabeling Method:UnknownDescription:The model is trained on 400k samples from public datasets licensed for commercial use, focused on English (US) for information retrieval and question answering over text documents, with data collection and labeling methods unspecified. NVIDIA Retrieval QA E5 Embedding • Link:https://build.nvidia.com/nvidia/nv-embedqa-e5-v5/modelcard Link https://build.nvidia.com/nvidia/nv-embedqa-e5-v5/modelcard • Data Collection Method:Unknown Data Collection Method: • Labeling Method:Unknown Labeling Method: • Description:The model is trained on 400k samples from public datasets licensed for commercial use, focused on English (US) for information retrieval and question answering over text documents, with data collection and labeling methods unspecified. Description: Inference: ========== Engine: • TensorRT Test Hardware: • A100 • H100 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.2-nv-rerankqa-1b-v2",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_2-nv-rerankqa-1b-v2",
    "scraped_on": "2025-10-15T21:41:29.910863",
    "tags": [
      "nemo retriever",
      "nvidia",
      "reranking",
      "retrieval augmented generation",
      "run-on-rtx"
    ],
    "model_card": "Model Overview ============== Model Overview Description ----------- Description The NVIDIA NeMo Retriever Llama3.2 reranking model is optimized for providing a logit score that represents how relevant a document(s) is to a given query. The model was fine-tuned formultilingual, cross-lingualtext question-answering retrieval, with support forlong documents (up to 8192 tokens). This model was evaluated on 26 languages: English, Arabic, Bengali, Chinese, Czech, Danish, Dutch, Finnish, French, German, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, and Turkish. multilingual, cross-lingual long documents (up to 8192 tokens) The reranking model is a component in a text retrieval system to improve the overall accuracy. A text retrieval system often uses an embedding model (dense) or lexical search (sparse) index to return relevant text passages given the input. A reranking model can be used to rerank the potential candidate into a final order. The reranking model has the question-passage pairs as an input and therefore, can process cross attention between the words. It’s not feasible to apply a Ranking model on all documents in the knowledge base, therefore, ranking models are often deployed in combination with embedding models. This model is ready for commercial use. The Llama 3.2 1B reranking model is a part of the NeMo Retriever collection of NIM, which provide state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can also readily customize them for their domain-specific use cases, such as information technology, juman resource help assistants, and research & development research assistants. License/Terms of use -------------------- License/Terms of use The use of this model is governed by theNVIDIA AI Foundation Models Community License Agreementand Llama 3.2 is licensed under theLlama 3.2 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved. NVIDIA AI Foundation Models Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Llama 3.2 Community License (https://www.llama.com/llama3_2/license/) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. Intended use ------------ Intended use The NeMo Retriever Llama 3.2 reranking model is most suitable for users who want to improve their multilingual retrieval tasks by reranking a set of candidates for a given question. Model Architecture: Llama-3.2 1B Ranker --------------------------------------- Model Architecture: Llama-3.2 1B Ranker Architecture Type:TransformerNetwork Architecture:Fine-tuned meta-llama/Llama-3.2-1B Architecture Type: Network Architecture: The NeMo Retriever Llama 3.2 reranking model is a transformer encoder fine-tuned for contrastive learning. We employ bi-directional attention when fine-tuning for higher accuracy. The last embedding output by the decoder model is used with a mean pooling strategy, and a binary classification head is fine-tuned for the ranking task. Ranking models for text ranking are typically trained as a cross-encoder for sentence classification. This involves predicting the relevancy of a sentence pair (for example, question and chunked passages). The CrossEntropy loss is used to maximize the likelihood of passages containing information to answer the question and minimize the likelihood for (negative) passages that do not contain information to answer the question. We train the model on public datasets described in the Dataset and Training section. Input ----- Input Input Type:Pair of TextsInput Format:List of text pairsInput Parameters:1DOther Properties Related to Input:The model was trained on question and answering over text documents from multiple languages. It was evaluated to work successfully with up to a sequence length of 8192 tokens. Longer texts are recommended to be either chunked or truncated. Input Type: Input Format: Input Parameters: Other Properties Related to Input: OutputOutput Type:FloatsOutput Format:List of floatsOutput Parameters:1DOther Properties Related to Output:Each the probability score (or raw logits). Users can decide to implement a Sigmoid activation function applied to the logits in their usage of the model. Output Output Type: Output Format: Output Parameters: Other Properties Related to Output: Software Integration -------------------- Software Integration Runtime:NeMo Retriever Llama 3.2 reranking NIMSupported Hardware Microarchitecture Compatibility: NVIDIA Ampere, NVIDIA Hopper, NVIDIA LovelaceSupported Operating System(s):Linux Runtime: Supported Hardware Microarchitecture Compatibility Supported Operating System(s): Model Version(s) ---------------- Model Version(s) NVIDIA NeMo Retriever Llama 3.2 reranking Short Name: llama-3.2-nv-rerankqa-1b-v2 Training Dataset & Evaluation ============================= Training Dataset & Evaluation Training Dataset ---------------- Training Dataset The development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset namedMSMARCOrestricts ‌commercial licensing, limiting the use of these models in commercial settings. To address this, NVIDIA created its own training dataset blend based on public QA datasets, which each have a license for commercial applications. MSMARCO (https://microsoft.github.io/msmarco/) Data Collection Method by dataset: Automated, Unknown Data Collection Method by dataset Labeling Method by dataset:Automated, Unknown Labeling Method by dataset: Properties:This model was trained on 800k samples from public datasets. Properties: Evaluation Results ------------------ Evaluation Results We evaluate the pipelines on a set of evaluation benchmarks. We applied the ranking model to the candidates retrieved from a retrieval embedding model. Overall, the pipeline llama-3.2-nv-embedqa-1b-v2 + llama-3.2-nv-rerankqa-1b-v2 provides high BEIR+TechQA accuracy with multilingual and crosslingual support. The llama-3.2-nv-rerankqa-1B-v2 ranking model is 3.5x smaller than the nv-rerankqa-mistral-4b-v3 model. We evaluated the NVIDIA Retrieval QA Embedding Model in comparison to literature open & commercial retriever models on academic benchmarks for question-answering -NQ,HotpotQAandFiQA (Finance Q&A)from BeIR benchmark and TechQA dataset. In this benchmark, the metric used was Recall@5. As described, we need to apply the ranking model on the output of an embedding model. NQ (https://huggingface.co/datasets/BeIR/nq) HotpotQA (https://huggingface.co/datasets/hotpot_qa) FiQA (Finance Q&A) (https://huggingface.co/datasets/BeIR/fiqa) We evaluated the model’s multilingual capabilities on theMIRACLacademic benchmark - a multilingual retrieval dataset, across 15 languages, and on an additional 11 languages that were translated from the English and Spanish versions of MIRACL. The reported scores are based on a custom subsampled version by selecting hard negatives for each query to reduce the corpus size. MIRACL (https://github.com/project-miracl/miracl) We evaluated the cross-lingual capabilities on the academic benchmarkMLQAbased on 7 languages (Arabic, Chinese, English, German, Hindi, Spanish, Vietnamese). We consider only evaluation datasets when the query and documents are in different languages. We calculate the average Recall@5 across the 42 different language pairs. MLQA (https://github.com/facebookresearch/MLQA/) We evaluated the support of long documents on the academic benchmarkMultilingual Long-Document Retrieval (MLDR)built on Wikipedia and mC4, covering 12 typologically diverse languages . The English version has a median length of 2399 tokens and 90th percentile of 7483 tokens using the llama 3.2 tokenizer. Multilingual Long-Document Retrieval (MLDR) (https://huggingface.co/datasets/Shitao/MLDR) Data Collection Method by dataset: Unknown Data Collection Method by dataset Labeling Method by dataset:Unknown Labeling Method by dataset: PropertiesThe evaluation datasets are based on threeMTEB/BEIRTextQA datasets, the TechQA dataset, and MIRACL multilingual retrieval datasets, which are all public datasets. The sizes range between 10,000s up to 5M depending on the dataset. Properties MTEB/BEIR (https://github.com/beir-cellar/beir) InferenceEngine:TensorRTTest Hardware:H100 PCIe/SXM, A100 PCIe/SXM, L40s, L4, and A10G Inference Engine: Test Hardware: Ethical Considerations ====================== Ethical Considerations NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ tab for the Explainability, Bias, Safety & Security, and Privacy subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.2-nv-embedqa-1b-v2",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_2-nv-embedqa-1b-v2",
    "scraped_on": "2025-10-15T21:41:26.684233",
    "tags": [
      "embedding",
      "nemo retriever",
      "nvidia",
      "retrieval augmented generation",
      "run-on-rtx",
      "text-to-embedding"
    ],
    "model_card": "Model Overview ============== Model Overview Description ----------- Description The NVIDIA NeMo Retriever Llama3.2 embedding model is optimized formultilingual and cross-lingualtext question-answering retrieval withsupport for long documents (up to 8192 tokens) and dynamic embedding size (Matryoshka Embeddings). This model was evaluated on 26 languages: English, Arabic, Bengali, Chinese, Czech, Danish, Dutch, Finnish, French, German, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, and Turkish. multilingual and cross-lingual support for long documents (up to 8192 tokens) and dynamic embedding size (Matryoshka Embeddings) In addition to enabling multilingual and cross-lingual question-answering retrieval, this model reduces the data storage footprint by 35x through dynamic embedding sizing and support for longer token length, making it feasible to handle large-scale datasets efficiently. An embedding model is a crucial component of a text retrieval system, as it transforms textual information into dense vector representations. They are typically transformer encoders that process tokens of input text (for example: question, passage) to output an embedding. This model is ready for commercial use. The Llama 3.2 1b embedding model is a part of the NVIDIA NeMo Retriever collection of NIM, which provide state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can also readily customize them for domain-specific use cases, such as information technology, human resource help assistants, and research & development research assistants. Intended use ------------ Intended use The NeMo Retriever Llama3.2 embedding model is most suitable for users who want to build a multilingual question-and-answer application over a large text corpus, leveraging the latest dense retrieval technologies. License/Terms of use -------------------- License/Terms of use The use of this model is governed by theNVIDIA AI Foundation Models Community License Agreementand Llama 3.2 is licensed under theLlama 3.2 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved. NVIDIA AI Foundation Models Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Llama 3.2 Community License (https://www.llama.com/llama3_2/license/) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. Model Architecture ------------------ Model Architecture Architecture Type:TransformerNetwork Architecture:Fine-tuned Llama3.2 1b retriever Architecture Type: Network Architecture: This NeMo Retriever embedding model is a transformer encoder - a fine-tuned version of Llama3.2 1b, with 16 layers and an embedding size of 2048, which is trained on public datasets. The AdamW optimizer is employed incorporating 100 warm up steps and 5e-6 learning rate with WarmupDecayLR scheduler. Embedding models for text retrieval are typically trained using a bi-encoder architecture. This involves encoding a pair of sentences (for example, query and chunked passages) independently using the embedding model. Contrastive learning is used to maximize the similarity between the query and the passage that contains the answer, while minimizing the similarity between the query and sampled negative passages not useful to answer the question. Input ----- Input Input Type:TextInput Format:List of stringsInput Parameter:1DOther Properties Related to Input:The model's maximum context length is 8192 tokens. Texts longer than maximum length must either be chunked or truncated. Input Type: Input Format: Input Parameter: Other Properties Related to Input: Output ------ Output Output Type:FloatsOutput Format:List of float arraysOutput:Model outputs embedding vectors of maximum dimension 2048 for each text string (can be configured based on 384, 512, 768, 1024, or 2048).Other Properties Related to Output:N/A Output Type: Output Format: Output: Other Properties Related to Output: Software Integration -------------------- Software Integration Runtime Engine:NeMo Retriever embedding NIMSupported Hardware Microarchitecture Compatibility: NVIDIA Ampere, NVIDIA Hopper, NVIDIA LovelaceSupported Operating System(s):Linux Runtime Engine: Supported Hardware Microarchitecture Compatibility Supported Operating System(s): Model Version(s) ---------------- Model Version(s) NVIDIA NeMo Retriever Llama 3.2 embedding Short Name: llama-3.2-nv-embedqa-1b-v2 Training Dataset & Evaluation ============================= Training Dataset & Evaluation Training Dataset ---------------- Training Dataset The development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset named MS MARCO restricts ‌commercial licensing, limiting the use of these models in commercial settings. To address this, NVIDIA created its own training dataset blend based on public QA datasets, which each have a license for commercial applications. Data Collection Method by dataset: Automated, Unknown Data Collection Method by dataset Labeling Method by dataset: Automated, Unknown Labeling Method by dataset Properties:Semi-supervised pre-training on 12M samples from public datasets and fine-tuning on 1M samples from public datasets. Properties: Evaluation Results ------------------ Evaluation Results Properties: We evaluated the NeMo Rtriever embdding model in comparison to literature open & commercial retriever models on academic benchmarks for question-answering -NQ,HotpotQAandFiQA (Finance Q&A)from BeIR benchmark and TechQA dataset. Note that the model was evaluated offline on A100 GPUs using the model's PyTorch checkpoint. In this benchmark, the metric used was Recall@5. NQ (https://huggingface.co/datasets/BeIR/nq) HotpotQA (https://huggingface.co/datasets/hotpot_qa) FiQA (Finance Q&A) (https://huggingface.co/datasets/BeIR/fiqa) We evaluated the multilingual capabilities on the academic benchmarkMIRACLacross 15 languages and translated the English and Spanish version of MIRACL into additional 11 languages. The reported scores are based on an internal version of MIRACL by selecting hard negatives for each query to reduce the corpus size. MIRACL (https://github.com/project-miracl/miracl) We evaluated the cross-lingual capabilities on the academic benchmarkMLQAbased on 7 languages (Arabic, Chinese, English, German, Hindi, Spanish, Vietnamese). We consider only evaluation datasets when the query and documents are in different languages. We calculate the average Recall@5 across the 42 different language pairs. MLQA (https://github.com/facebookresearch/MLQA/) We evaluated the support of long documents on the academic benchmarkMultilingual Long-Document Retrieval (MLDR)built on Wikipedia and mC4, covering 12 typologically diverse languages. The English version has a median length of 2399 tokens and 90th percentile of 7483 tokens using the llama 3.2 tokenizer. The MLDR dataset is based on synthetic generated questions with a LLM, which has the tendency to create questions with similar keywords than the positive document, but might not be representative for real user queries. This characteristic of the dataset benefits sparse embeddings like BM25. Multilingual Long-Document Retrieval (MLDR) (https://huggingface.co/datasets/Shitao/MLDR) Data Collection Method by dataset: Unknown Data Collection Method by dataset Labeling Method by dataset:Unknown Labeling Method by dataset: Properties:The evaluation datasets are based onMTEB/BEIR, TextQA, TechQA,MIRACL,MLQA, andMLDR. The size ranges between 10,000s up to 5M depending on the dataset. Properties: MTEB/BEIR (https://github.com/beir-cellar/beir) MIRACL (https://github.com/project-miracl/miracl) MLQA (https://github.com/facebookresearch/MLQA) MLDR (https://huggingface.co/datasets/Shitao/MLDR) InferenceEngine:TensorRTTest Hardware:H100 PCIe/SXM, A100 PCIe/SXM, L40s, L4, and A10G Inference Engine: Test Hardware: Ethical Considerations ====================== Ethical Considerations NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ tab for the Explainability, Bias, Safety & Security, and Privacy subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "qwen2.5-coder-7b-instruct",
    "nvidia_url": "https://build.nvidia.com/qwen/qwen2_5-coder-7b-instruct",
    "scraped_on": "2025-10-15T21:41:26.196718",
    "tags": [
      "chat",
      "code completion",
      "code generation",
      "qwen",
      "text-to-code"
    ],
    "model_card": "Model Overview ============== Description: ============ Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5: • Significant improvements in code generation, code reasoning and code fixing. Increased training tokens to 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o. • A more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies. • Long-context support up to 32K tokens. This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAQwen2.5-Coder-7B-Instruct Model Card. Qwen2.5-Coder-7B-Instruct Model Card (https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct) License/Terms of Use ==================== Qwen/Qwen2.5-Coder-7B-Instruct is licensed under theApache 2.0 License Apache 2.0 License (https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE) References: =========== Blog,Github,Technical Report Blog (https://qwenlm.github.io/blog/qwen2.5-coder-family/) Github (https://github.com/QwenLM/Qwen2.5) Technical Report (https://arxiv.org/abs/2409.12186) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Qwen2.5-Coder-7B-Instruct Architecture Type: Network Architecture: Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:1D Input Type(s): Input Format(s): Input Parameters: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:1D Output Type(s): Output Format: Output Parameters: Model Version(s): ================= Qwen2.5-Coder-7B-Instruct Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= Link:UnknownData Collection Method by dataset:Hybrid: Automated, HumanLabeling Method by dataset:Hybrid: Automated, SyntheticProperties:The training dataset contains over 5.5 trillion tokens total across 92 programming languages with a mixture ratio of 70% Code, 20% Text, 10% Math, sourced from GitHub repositories, Pull Requests, Commits, Jupyter Notebooks, and Kaggle datasets. Link: Data Collection Method by dataset: Labeling Method by dataset: Properties: Testing Dataset: ================ Link:UnknownData Collection Method by dataset:UnknownLabeling Method by dataset:UnknownProperties:Unknown Link: Data Collection Method by dataset: Labeling Method by dataset: Properties: Evaluation Dataset: =================== Link:See evaluation section of theHugging Face Qwen2.5-Coder-7B-Instruct Model CardData Collection Method by dataset:Hybrid: Human, AutomatedLabeling Method by dataset:Hybrid: Automated, HumanProperties:The evaluation datasets consist of multiple benchmarks including HumanEval with 164 Python programming tasks, MBPP with 974 programming problems, LiveCodeBench with over 600 coding problems, and additional benchmarks covering code generation, completion, reasoning and debugging capabilities. Link: Hugging Face Qwen2.5-Coder-7B-Instruct Model Card (https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct#evaluation--performance) Data Collection Method by dataset: Labeling Method by dataset: Properties: Inference: ========== Engine:TensorRT-LLMTest Hardware:NVIDIA L40S Engine: Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "qwen2.5-coder-32b-instruct",
    "nvidia_url": "https://build.nvidia.com/qwen/qwen2_5-coder-32b-instruct",
    "scraped_on": "2025-10-15T21:41:25.843435",
    "tags": [
      "chat",
      "code completion",
      "code generation",
      "qwen",
      "text-to-code"
    ],
    "model_card": "Model Overview ============== Description: ============ Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5: • Significant improvements in code generation, code reasoning and code fixing. Increased training tokens to 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o. • A more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies. • Long-context support up to 32K tokens. This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAQwen2.5-Coder-32B-Instruct Model Card. Qwen2.5-Coder-32B-Instruct Model Card (https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct) License/Terms of Use ==================== Qwen/Qwen2.5-Coder-32B-Instruct is licensed under theApache 2.0 License Apache 2.0 License (https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct/blob/main/LICENSE) References: =========== Blog,Github,Technical Report Blog (https://qwenlm.github.io/blog/qwen2.5-coder-family/) Github (https://github.com/QwenLM/Qwen2.5) Technical Report (https://arxiv.org/abs/2409.12186) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Qwen2.5-Coder-32B-Instruct Architecture Type: Network Architecture: Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:1D Input Type(s): Input Format(s): Input Parameters: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:1D Output Type(s): Output Format: Output Parameters: Model Version(s): ================= Qwen2.5-Coder-32B-Instruct Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= Link:UnknownData Collection Method by dataset:Hybrid: Automated, HumanLabeling Method by dataset:Hybrid: Automated, SyntheticProperties:The training dataset contains over 5.5 trillion tokens total across 92 programming languages with a mixture ratio of 70% Code, 20% Text, 10% Math, sourced from GitHub repositories, Pull Requests, Commits, Jupyter Notebooks, and Kaggle datasets. Link: Data Collection Method by dataset: Labeling Method by dataset: Properties: Testing Dataset: ================ Link:UnknownData Collection Method by dataset:UnknownLabeling Method by dataset:UnknownProperties:Unknown Link: Data Collection Method by dataset: Labeling Method by dataset: Properties: Evaluation Dataset: =================== Link:See evaluation section of theHugging Face Qwen2.5-Coder-32B-Instruct Model CardData Collection Method by dataset:Hybrid: Human, AutomatedLabeling Method by dataset:Hybrid: Automated, HumanProperties:The evaluation datasets consist of multiple benchmarks including HumanEval with 164 Python programming tasks, MBPP with 974 programming problems, LiveCodeBench with over 600 coding problems, and additional benchmarks covering code generation, completion, reasoning and debugging capabilities. Link: Hugging Face Qwen2.5-Coder-32B-Instruct Model Card (https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct#evaluation--performance) Data Collection Method by dataset: Labeling Method by dataset: Properties: Inference: ========== Engine:TensorRT-LLMTest Hardware:NVIDIA L40S Engine: Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "cosmos-nemotron-34b",
    "nvidia_url": "https://build.nvidia.com/nvidia/cosmos-nemotron-34b",
    "scraped_on": "2025-10-15T21:41:25.514941",
    "tags": [
      "image caption",
      "image to text",
      "nvidia",
      "vision language model",
      "vlm"
    ],
    "model_card": "Cosmos Nemotron 34B Model Card ============================== Description =========== NVIDIA Cosmos Nemotron 34B is a leading vision language model (VLMs) that enables the ability to query and summarize images and video from the physical or virtual world. Cosmos Nemotron 34B is deployable in the data center, cloud and at the edge, including Jetson Orin and laptop by AWQ 4bit quantization through TinyChat framework. We find: (1) image-text pairs are not enough, interleaved image-text is essential; (2) unfreezing LLM during interleaved image-text pre-training enables in-context learning; (3)re-blending text-only instruction data is crucial to boost both VLM and text-only performance. This model is ready for commercial use. It was trained on commercial images and videos for all three stages of training and supports single image and video inference. This version does not support interleaved and in-context learning capabilities. References ========== • VILA technical paper VILA technical paper (https://arxiv.org/abs/2312.07533) • VILA github repo VILA github repo (https://github.com/NVlabs/VILA) License ======= The license to use this model is covered by the Model EULA. By downloading the unpruned or pruned version of the model, you accept the terms and conditions of theselicenses licenses (https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/) Model Architecture ================== Architecture Type:Transformer-based Network Architecture Architecture Type: Network Architecture • Vision Encoder: SigLIP-400M • Language Encoder: Yi-34B Input ----- • Input Type(s):Image, Video, Text Input Type(s): • Input Format(s):Image (Red, Green, Blue (RGB)), Video (.mp4), and Text (String) Input Format(s): • Input Parameters:Image (2D), Video (3D), Text (1D) Input Parameters: Output ------ • Output Type(s):Text Output Type(s): • Output Formats:String Output Formats: • Output Parameters:1D Output Parameters: • Other Properties Related to Output:N/A Other Properties Related to Output: Software Integration ==================== • Runtime Engine(s):TensorRT-LLM Runtime Engine(s): • Supported Hardware Architecture(s):NVIDIA Hopper Supported Hardware Architecture(s): • Supported Operating System(s):Linux Supported Operating System(s): Model Versions ============== • Cosmos-Nemotron-SigLIP-Yi-34B Training Dataset ================ NV-Pretraining and NV-CosmosNemotron-SFT data were used. Additionally, the commercial subset of following datasets were used: • OASST1 OASST1 (https://huggingface.co/datasets/OpenAssistant/oasst1) • OASST2 OASST2 (https://huggingface.co/datasets/OpenAssistant/oasst2) • Localized Narratives Localized Narratives (https://google.github.io/localized-narratives/) • TextCaps TextCaps (https://textvqa.org/textcaps/dataset/) • TextVQA TextVQA (https://textvqa.org/dataset/) • RefCOCO RefCOCO (https://github.com/lichengunc/refer) • VQAv2 VQAv2 (https://visualqa.org/) • GQA GQA (https://cs.stanford.edu/people/dorarad/gqa/index.html) • SynthDoG-en SynthDoG-en (https://huggingface.co/datasets/naver-clova-ix/synthdog-en) • A-OKVQ A-OKVQ (https://github.com/allenai/aokvqa) • WIT WIT (https://github.com/google-research-datasets/wit) • CLEVR CLEVR (https://cs.stanford.edu/people/jcjohns/clevr/) • CLEVR-X CLEVR-X (https://github.com/ExplainableML/CLEVR-X) • CLEVR-Math CLEVR-Math (https://huggingface.co/datasets/dali-does/clevr-math) • ScreenQA ScreenQA (https://github.com/google-research-datasets/screen_qa) • WikiSQL WikiSQL (https://github.com/salesforce/WikiSQL) • WikiTablQuestions WikiTablQuestions (https://github.com/ppasupat/WikiTableQuestions/) • RenderedText RenderedText (https://github.com/GbotHQ/ocr-dataset-rendering/) • FinQA FinQA (https://github.com/czyssrs/FinQA) • TAT-QA TAT-QA (https://nextplusplus.github.io/TAT-QA/) • Dolly Dolly (https://huggingface.co/datasets/databricks/databricks-dolly-15k) • Websight Websight (https://huggingface.co/datasets/HuggingFaceM4/WebSight) • RAVEN RAVEN (https://github.com/WellyZhang/RAVEN) • VizWiz VizWiz (https://vizwiz.org/tasks-and-datasets/vqa/) • Inter-GPS Inter-GPS (https://github.com/lupantech/InterGPS) • YouCook2 YouCook2 (http://youcook2.eecs.umich.edu/) • ActivityNet Captions ActivityNet Captions (https://cs.stanford.edu/people/ranjaykrishna/densevid/) • Video Localized Narratives Video Localized Narratives (https://google.github.io/video-localized-narratives/) • CLEVRER CLEVRER (https://google.github.io/video-localized-narratives/) • Perception Test Perception Test (https://github.com/google-deepmind/perception_test) • Next-QA Next-QA (https://github.com/doc-doc/NExT-QA) • Kinetics-400 Kinetics-400 (https://paperswithcode.com/dataset/kinetics) Data Collection Method by dataset: • Hybrid: Human, Automatic/Sensors Labeling Method by dataset: • Hybrid: Human, Automatic/Sensors Properties: • NV-Pretraining data was collected from 5M subsampled NV-CLIP dataset. Stage 3 NV-SFT data has 2.8M images and 3.58M annotations on images that only have commercial license. Additionally, 355K videos with commercial license and 400K annotations on videos were used. Evaluation Data =============== Data Collection Method by dataset: • Hybrid: Human, Automatic/Sensors Labeling Method by dataset: • Hybrid: Human, Automatic/Sensors Properties: • A collection of different benchmarks, including academic VQA benchmarks and recent benchmarks specifically proposed for instruction-following LMMs. • VQAv2 VQAv2 (https://visualqa.org/) • GQA GQA (https://cs.stanford.edu/people/dorarad/gqa/about.html) • ScienceQA Image ScienceQA Image (https://scienceqa.github.io/) • Text VQA Text VQA (https://textvqa.org/) • POPE POPE (https://github.com/AoiDragon/POPE) • MME MME (https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) • SEED-Bench SEED-Bench (https://github.com/AILab-CVC/SEED-Bench) • MMMU MMMU (https://mmmu-benchmark.github.io/) • Video MME Video MME (https://video-mme.github.io/home_page.html) • Egoschema Egoschema (https://egoschema.github.io/) • Perception Test Perception Test (https://github.com/google-deepmind/perception_test) Methodology and KPI ------------------- Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Promise and the Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "genmol",
    "nvidia_url": "https://build.nvidia.com/nvidia/genmol-generate",
    "scraped_on": "2025-10-15T21:41:22.386723",
    "tags": [
      "bionemo",
      "chemistry",
      "drug discovery",
      "molecule generation",
      "nim",
      "nvidia"
    ],
    "model_card": "Model Overview ============== Description: ============ GenMol is a masked diffusion model1trained on molecular Sequential Attachment-based Fragment Embedding (SAFE) representations2for fragment-based molecule generation, which can serve as a generalist model for various drug discovery tasks, including De Novo generation​, linker design​, motif extension​, scaffold decoration/morphing​, hit generation​, and lead optimization. SAFE (https://arxiv.org/abs/2310.10773) This model is ready for commercial use. License/Terms of Use: ===================== The NIM container is governed by theNVIDIA Software License Agreement, theProduct-Specific Terms for NVIDIA AI Products; and the use of this model is governed by theAI Foundation Models Community License Agreement. NVIDIA Software License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) Product-Specific Terms for NVIDIA AI Products (https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/) AI Foundation Models Community License Agreement (https://docs.nvidia.com/ai-foundation-models-community-license.pdf) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. References: =========== Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:BERT Architecture Type: Network Architecture: Input: ====== Input Type(s):Text (Molecular Sequence), Number (Molecules to generate, SoftMax temperature scaling factor, randomness factor, diffusion step-size), Enumeration (Scoring method), Binary (Showing unique molecules only)Input Format(s):Text: String (Sequential Attachment-based Fragment Embedding (SAFE)); Number: Integer, FP32; Enumeration: String (QED, LogP); Binary: BooleanInput Parameters:1DOther Properties Related to Input:Maximum input length is 512 tokens. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):Text (List of molecule sequences), Number (List of scores)Output Format:Text: Array of string (Sequential Attachment-based Fragment Embedding (SAFE)); Number: Array of FP32 (Scores)Output Parameters:2DOther Properties Related to Output:Maximum output length is 512 tokens. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s):PyTorch >= 2.5.1 Runtime Engine(s): Supported Hardware Microarchitecture Compatibility:NVIDIA AmpereNVIDIA Ada LovelaceNVIDIA HopperNVIDIA Grace Hopper Supported Hardware Microarchitecture Compatibility: [Preferred/Supported] Operating System(s):Linux [Preferred/Supported] Operating System(s): Model Version(s): ================= GenMol v1.0 Training & Evaluation Dataset: ============================== Training and Testing Dataset: ============================= Link:SAFE-GPTGitHub,HuggingFace,Data Collection Method by dataset:AutomatedLabeling Method by dataset:AutomatedProperties:1.1B SAFE strings consist of various molecule types (drug-like compounds, peptides, multi-fragment molecules, polymers, reagents and non-small molecules). Link: GitHub (https://github.com/datamol-io/safe) HuggingFace (https://huggingface.co/datasets/datamol-io/safe-gpt) Data Collection Method by dataset: Labeling Method by dataset: Properties: Evaluation Dataset: =================== Link:SAFE-DRUGSGitHub,HuggingFaceData Collection Method by dataset:Not ApplicableLabeling Method by dataset:Not ApplicableProperties:SAFE-DRUGS consists of 26 known therapeutic drugs. Link: GitHub (https://github.com/datamol-io/safe) HuggingFace (https://huggingface.co/datasets/datamol-io/safe-drugs) Data Collection Method by dataset: Labeling Method by dataset: Properties: Inference: ========== Engine:PyTorchTest Hardware:A6000, A100, L40, L40S, H100 Engine: Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Users are responsible for ensuring the physical properties of model-generated molecules are appropriately evaluated and comply with applicable safety regulations and ethical standards. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "qwen2.5-7b-instruct",
    "nvidia_url": "https://build.nvidia.com/qwen/qwen2_5-7b-instruct",
    "scraped_on": "2025-10-15T21:41:19.166986",
    "tags": [
      "chat",
      "chinese language generation",
      "large language models",
      "qwen",
      "text-to-text"
    ],
    "model_card": "Model Overview ============== Description: ============ Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2: • Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains. • Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. • Long-context Support up to 128K tokens and can generate up to 8K tokens. • Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAQwen2.5-7B-Instruct Model Card. Qwen2.5-7B-Instruct Model Card (https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) License/Terms of Use ==================== Qwen/Qwen2.5-7B-Instruct is licensed under theApache 2.0 License Apache 2.0 License (https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE) References: =========== Blog,Github,Documentation,Technical Report Blog (https://qwenlm.github.io/blog/qwen2.5/) Github (https://github.com/QwenLM/Qwen2.5) Documentation (https://qwen.readthedocs.io/en/latest/) Technical Report (https://arxiv.org/abs/2412.15115) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Qwen2.5-7B-Instruct Architecture Type: Network Architecture: Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:1D Input Type(s): Input Format(s): Input Parameters: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:1D Output Type(s): Output Format: Output Parameters: Model Version(s): ================= Qwen2.5-7B-Instruct Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= Link:UnknownData Collection Method by dataset:UnknownLabeling Method by dataset:UnknownProperties:The size of the pre-training dataset is expanded from 7 trillion tokens used in Qwen2 to a maximum of 18 trillion tokens. Link: Data Collection Method by dataset: Labeling Method by dataset: Properties: Testing Dataset: ================ Link:UnknownData Collection Method by dataset:UnknownLabeling Method by dataset:UnknownProperties:Unknown Link: Data Collection Method by dataset: Labeling Method by dataset: Properties: Evaluation Dataset: =================== Link:See evaluation section of theHugging Face Qwen2.5-7B-Instruct Model CardData Collection Method by dataset:UnknownLabeling Method by dataset:UnknownProperties:Unknown Link: Hugging Face Qwen2.5-7B-Instruct Model Card (https://huggingface.co/Qwen/Qwen2.5-7B-Instruct#evaluation--performance) Data Collection Method by dataset: Labeling Method by dataset: Properties: Inference: ========== Engine:TensorRT-LLMTest Hardware:NVIDIA L40S Engine: Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "italia_10b_instruct_16k",
    "nvidia_url": "https://build.nvidia.com/igenius/italia_10b_instruct_16k",
    "scraped_on": "2025-10-15T21:41:15.936857",
    "tags": [
      "chat",
      "financial services",
      "government",
      "heavy industry",
      "highly regulated use case support",
      "igenius"
    ],
    "model_card": "Model Overview ============== Description: ============ Italia-10B is a large language model (LLM) that can be used for use-cases in regulated industries—such as financial services, government and heavy industry. It supports multi-lingual single-turn and multi-turn chat formats, with a context length of 16,384 tokens. The base model was pre-trained on a corpus of approximately 9 trillion tokens drawn from diverse English texts, more than 50 natural languages, and a wide range of coding languages. Next, it went through alignment steps including: • Supervised Fine-tuning (SFT) • Direct Preference Optimization (DPO) The pre-training dataset primarily consists of web documents and open-source repositories such as ArXiv, PubMed Central, GitHub and similar sources. The model supports over 50 languages, with a strong focus on European languages such as German, French, Italian, Spanish, Portuguese, Russian, Romanian, and Polish. Additionally, the model incorporates specialized sources from domains such as finance and reasoning, drawing from high-quality datasets to enhance its performance in these areas. This model is for research and development only. For commercial use please follow theTerms of Use Terms of Use (https://secure.igenius.ai/Website/Media/License+LLM+Italia+10B+ENG.pdf) Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case License/Terms of Use: --------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theLicense agreement for Italia NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) License agreement for Italia (http://secure.igenius.ai/legal/iGenius+License+Italia+10B.pdf) Model Architecture: =================== Architecture Type:Transformer Decoder (auto-regressive language model)Network Architecture:Italia-10B Architecture Type: Network Architecture: Input: ====== Input Type(s):TextInput Format:StringInput Parameter(s):1D Input Type(s): Input Format: Input Parameter(s): Output: ======= Output Type(s):TextOutput Format:StringOutput Parameter(s):1D Output Type(s): Output Format: Output Parameter(s): Model Version(s): ================= Italia-10B v1.0 Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper Supported Operating System(s): • Linux • Windows Inference: ========== Engine:[TensorRT_LLM, Triton, vLLM]BF16 - single GPU - A100, H100, A10 FP8 - single GPU - H100, L40s Engine: Prompt Format ------------- Single Turn ----------- Multi-Turn or Few-shot ---------------------- Evaluation Results ================== MT-Bench (GPT-4-Turbo)Evaluated using MT-Bench judging by GPT-4-0125-Preview as described in Appendix H in theHelpSteer2 Dataset Paper MT-Bench (GPT-4-Turbo) HelpSteer2 Dataset Paper (https://arxiv.org/abs/2406.08673) 6.46 IFEvalEvaluated using the Instruction Following Eval (IFEval) introduced in Instruction-Following Evaluation for Large Language Models. IFEval Prompt-Strict Acc: 57,31 Instruction-Strict Acc: 68,23 MMLUEvaluated using the Multi-task Language Understanding benchmarks as introduced in Measuring Massive Multitask Language Understanding. MMLU 5-shot: 63,5 ARC-CThe AI2’s Reasoning Challenge (ARC-C) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. ARC-C 5-shot: 84,1 Usage ===== Deployment and inference with Italia-10B can be done in many ways - Deployment of the TRTLLM engines with Triton with TRTLLM backend (Single GPU) Deployment as NIM - Nvidia Inference Microservices Deployment with PyTriton Limitations =========== The model was trained on data collected from the internet, which may contain language that is biased or inappropriate. As a result, the model might occasionally reflect these biases or generate responses that are inaccurate, omit key information, or include irrelevant or redundant text. There is also the possibility that it could produce content that is socially unacceptable or undesirable, even if the prompt does not include any offensive material. Ethical Considerations: ======================= We believe that developing trustworthy AI is a shared responsibility and have established policies and practices to support the development of a wide array of AI applications. When using this model in accordance with our terms of service, developers are encouraged to work with their internal teams to ensure the model meets the requirements of their specific industry and use case, and to address any potential misuse. If you have any concerns or wish to report security vulnerabilities, please contact ushere here (https://www.igenius.ai/contact) Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "falcon3-7b-instruct",
    "nvidia_url": "https://build.nvidia.com/tiiuae/falcon3-7b-instruct",
    "scraped_on": "2025-10-15T21:41:12.615654",
    "tags": [
      "chat",
      "code generation",
      "coding",
      "improved reasoning",
      "language generation",
      "math",
      "scientific knowledge",
      "tiiuae"
    ],
    "model_card": "Model Overview Description: ============ TheFalcon3-7B-Instructis an open foundation model designed for state-of-the-art performance in reasoning, language understanding, instruction following, code generation, and mathematics. It supports long-context tasks with a token limit of up to32Kand multilingual capabilities inEnglish, French, Spanish, and Portuguese.This model is forresearch and development purposes only. Falcon3-7B-Instruct 32K English, French, Spanish, and Portuguese research and development purposes only Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIATII Model Card. TII Model Card (https://huggingface.co/tiiuae/Falcon3-7B-Instruct) License/Terms of Use -------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Falcon 3 TII Falcon License. The NVIDIA-optimized Falcon3-7B-Instruct is built using artificial intelligence technology from the Technology Innovation Institute. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Falcon 3 TII Falcon License (https://falconllm.tii.ae/falcon-terms-and-conditions.html) References: =========== • Falcon3-7B-Instruct Model Card Falcon3-7B-Instruct Model Card (https://huggingface.co/tiiuae/Falcon3-7B-Instruct) • TII Website TII Website (https://www.tii.ae/) • Falcon 3 Family Release Blog Falcon 3 Family Release Blog (https://huggingface.co/blog/falcon3) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Transformer Decoder Only ArchitectureModel Details: Architecture Type: Network Architecture: Model Details: • Transformer-based causal decoder-only design. • Composed of 28 decoder blocks, leveraging Grouped Query Attention (GQA) for faster inference with 12 query heads and 4 key-value heads. • Wider head dimension of 256 for enhanced performance. • High RoPE (Rotary Positional Embedding) value of 1000042, enabling extended context understanding up to 32,000 tokens. • Incorporates SwiGLU activation and RMSNorm for improved training stability and efficiency. Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:(1D)Other Properties Related to Input:Supports multilingual input (EN, FR, ES, PT) and Context length up to 32,000 tokens. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:(1D)Other Properties Related to Output:Generates outputs in supported languages with capabilities across reasoning, code, and instructional tasks. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s):Not specified; supports standard machine learning pipelines such as PyTorch and Hugging Face Runtime Engine(s): Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper [Preferred/Supported] Operating System(s):Linux [Preferred/Supported] Operating System(s): Model Version(s): ================= • Falcon3-7B-Instruct v1.0 • Initial version released in December 2024 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= Link:Not publicly availableData Collection Method by dataset:Hybrid (Automated, Human)Labeling Method by dataset:Hybrid (Automated, Human)Properties: Link: Data Collection Method by dataset: Labeling Method by dataset: Properties: • Pretrained on 14 teratokens of web, code, STEM, multilingual, and high-quality datasets • Post-trained on 1.2 million samples of STEM, conversations, code, safety, and function call data Testing Dataset: ================ Link:Not publicly availableData Collection Method by dataset:Hybrid (Automated, Human)Labeling Method by dataset:Hybrid (Automated, Human)Properties:NA Link: Data Collection Method by dataset: Labeling Method by dataset: Properties: Evaluation Dataset: =================== Link:Not publicly availableData Collection Method by dataset:Hybrid (Automated, Human)Labeling Method by dataset:UnknownProperties:Benchmark scores for various models, including Falcon3-7B-Instruct, Qwen2.5-7B-Instruct, and Llama-3.1-8B-Instruct Link: Data Collection Method by dataset: Labeling Method by dataset: Properties: Inference: ========== Engine:TensorRT-LLMTest Hardware:NVIDIA Ampere Engine: Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "colosseum_355b_instruct_16k",
    "nvidia_url": "https://build.nvidia.com/igenius/colosseum_355b_instruct_16k",
    "scraped_on": "2025-10-15T21:41:09.424891",
    "tags": [
      "chat",
      "financial services",
      "government",
      "heavy industry",
      "highly regulated use case support",
      "igenius"
    ],
    "model_card": "Model Overview ============== Description: ============ Colosseum-355B is a large language model (LLM) that is designed for use cases in regulated industries—such as financial services, government, and heavy industry. It supports multi-lingual single-turn and multi-turn chat formats, with a context length of up to 16,384 tokens. The baseline model was pre-trained on a corpus of 9 trillion tokens drawn from a diverse assortment of English texts, more than 50 natural languages, and a wide range of coding languages. During Continual Pretraining (CPT), the model was trained on an additional 2 trillion tokens, bringing the total to 11 trillion. This phase expanded the model’s parameters and context length, implemented continual pretraining in FP8, and aligned the model's capabilities with specific domain expertise. Finally, the Colosseum-355B model went through alignment steps, including: • Supervised Fine-tuning (SFT) • Direct Preference Optimization (DPO) Because training datasets primarily consist of free web documents and open-source repositories, it is essential to build a CPT dataset that preserves the original data distribution and remains consistent with the composition of the original dataset. The model supports over 50 languages, with a strong focus on European languages such as German, French, Italian, Spanish, Portuguese, Russian, Romanian, and Polish. Its training dataset also includes substantial representations of non-European languages, including Japanese, Chinese, Arabic, Vietnamese, and Korean. Additionally, the model integrates specialized sources from domains such as finance and reasoning, drawing from high-quality datasets to enhance performance in these areas. This model is for research and development only. For commercial use please follow theTerms of Use Terms of Use (https://secure.igenius.ai/legal/iGenius+License+Colosseum+355B.pdf) Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case License/Terms of Use: --------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theLicense agreement for Colosseum NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) License agreement for Colosseum (https://secure.igenius.ai/legal/iGenius+License+Colosseum+355B.pdf) Intended use ============ Colosseum-355B is an instruct model intended for use for multi-lingual use-cases. It is designed for highly regulated industries and general purpose use cases to enable developers and enterprises to build and customize their own LLMs and LLM applications. The instruct model can be further customized using theNeMo Framework, which offers a suite of customization tools including Parameter-Efficient Fine-Tuning (P-tuning, Adapters, LoRA, and more), and Model Alignment (SFT, SteerLM, RLHF, and more) usingNeMo-Aligner. For examples and detailed instructions, refer to thedocumentation. NeMo Framework (https://docs.nvidia.com/nemo-framework/index.html) NeMo-Aligner (https://github.com/NVIDIA/NeMo-Aligner) documentation (https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/nemotron/index.html) Model Architecture: =================== Architecture Type:Transformer Decoder (auto-regressive language model)Network Architecture:Colosseum-355B Architecture Type: Network Architecture: Input: ====== Input Type(s):TextInput Format(s):StringInput Parameter(s):1D Input Type(s): Input Format(s): Input Parameter(s): Output: ======= Output Type(s):TextOutput Format:StringOutput Parameter(s):1D Output Type(s): Output Format: Output Parameter(s): Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper Supported Operating System(s): • Linux • Windows Model Version(s): ================= Colosseum-355B v1.0 Inference: ========== Engine:[TensorRT_LLM, Triton, vLLM]BF16 Inference: 8x H200 (1x H200 node) 16x H100 (2x H100 nodes) 16x A100 80GB (2x A100 80GB nodes) Engine: BF16 Inference FP8 Inference: 8x H200 (1x H200 node) 8x H100 (1x H100 node) FP8 Inference Prompt Format ------------- Single Turn ----------- Multi-Turn or Few-shot ---------------------- Evaluation Results ================== MT-Bench (GPT-4-Turbo)Evaluated using MT-Bench judging by GPT-4-0125-Preview as described in Appendix H in theHelpSteer2 Dataset Paper MT-Bench (GPT-4-Turbo) HelpSteer2 Dataset Paper (https://arxiv.org/abs/2406.08673) 8.11 IFEvalEvaluated using the Instruction Following Eval (IFEval) introduced in Instruction-Following Evaluation for Large Language Models. IFEval Prompt-Strict Acc: 78,34 Instruction-Strict Acc: 80,02 MMLUEvaluated using the Multi-task Language Understanding benchmarks as introduced in Measuring Massive Multitask Language Understanding. MMLU 5-shot: 82,04 MMLU (CoT)84,5 MMLU (CoT) ARC-CThe AI2’s Reasoning Challenge (ARC-C) dataset is a multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. ARC-C 5-shot: 94,81 Usage ===== Deployment and inference with Colosseum-355B can be done in many ways - Deployment of the TRTLLM engines with Triton with TRTLLM backend (multi-node in BF16 / Single node in FP8) Deployment as NIM - Nvidia Inference Microservices Deployment with PyTriton in multi-node BF16 Limitations =========== The model was trained on data collected from the internet, which may contain language that is biased or inappropriate. As a result, the model might occasionally reflect these biases or generate responses that are inaccurate, omit key information, or include irrelevant or redundant text. There is also the possibility that it could produce content that is socially unacceptable or undesirable, even if the prompt does not include any offensive material. Ethical Considerations: ======================= We believe that developing trustworthy AI is a shared responsibility and have established policies and practices to support the development of a wide array of AI applications. When using this model in accordance with our terms of service, developers are encouraged to work with their internal teams to ensure the model meets the requirements of their specific industry and use case, and to address any potential misuse. If you have any concerns or wish to report security vulnerabilities, please contact ushere here (https://www.igenius.ai/contact) Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.1-nemoguard-8b-content-safety",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_1-nemoguard-8b-content-safety",
    "scraped_on": "2025-10-15T21:41:06.073972",
    "tags": [
      "content moderation",
      "content safety",
      "guard model",
      "llm safety",
      "nvidia"
    ],
    "model_card": "Model Overview ============== Description =========== Llama 3.1 NemoGuard 8B ContentSafetyis a content safety model that moderates human-LLM interaction content and classifies user prompts and LLM responses as safe or unsafe. If the content is unsafe, the model additionally returns a response with a list of categories that the content violates. The base large language model (LLM) is the multilingualLlama-3.1-8B-Instructmodel from Meta. NVIDIA’s optimized release is LoRa-tuned on approved datasets and better conforms NVIDIA’s content safety risk taxonomy and other safety risks in human-LLM interactions. Llama 3.1 NemoGuard 8B ContentSafety Llama-3.1-8B-Instruct (https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) The model can be prompted using an instruction and a taxonomy of unsafe risks to be categorized. The instruction format for prompt moderation is shown below under input and output examples. License/Terms of Use: --------------------- Governing NVIDIA Model Hosting Terms (Cloud API) GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community License Agreement. Additional Information:Llama 3.1 Community License Agreement. Built with Llama. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Llama 3.1 Community License Agreement (https://www.llama.com/llama3_1/license/) Governing NVIDIA Download Terms & Third-Party Component Attribution Terms (Hugging Face LORA weights) GOVERNING TERMS: Use of this model is governed by theNVIDIA Open Model License Agreement. Additional Information:Llama 3.1 Community License Agreement. Built with Llama. NVIDIA Open Model License Agreement (https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf) Llama 3.1 Community License Agreement (https://www.llama.com/llama3_1/license/) GOVERNING TERMS: Use of the NIM container is governed by theNVIDIA Software License Agreementand theProduct-Specific Terms for NVIDIA AI Products; use of this model is governed by theNVIDIA Community Model License. Additional Information: Llama 3.1 Community License Agreement. Built with Llama. NVIDIA Software License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) Product-Specific Terms for NVIDIA AI Products (https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Model Architecture ================== Architecture Type:Transfomer Architecture Type: Network Architecture:The base model architecture is based on the Llama-3.1-8B-Instruct model from Meta (Model Card). We perform Parameter Efficient FineTuning (PEFT) over the above base model using the following Network Architecture parameters: Network Architecture: Model Card (https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/) • Rank: 16 • Alpha: 32 • Targeted low rank adaptation modules: 'k_proj', 'q_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj', 'gate_proj', 'lm_head'. Training Method: The training method forLlama 3.1 NemoGuard 8B ContentSafetyinvolves the following concepts: Llama 3.1 NemoGuard 8B ContentSafety • A system prompt, including the Aegis2.0 safety taxonomy, which is a safety policy that contains a list of unsafe categories. • Novel safety risk categories and policies can be provided in the instruction for the model to predict categories of violation if unsafe • The safety taxonomy and policy used to train the model contains 23 critically unsafe risk categories, a safe category and a \"needs caution\" category. • An internally annotated dataset, called Aegis-AI-Content-Safety-Dataset-2.0, of approximately 30,000 prompts and responses are used to instruction-tune the model. • The model is instruction-tuned to follow either safety or topic-following system prompts, with the LLM behaving as a classifier in both settings. • The model can return labels for both user and bot messages together in one inference call, if they both exist in the payload. This is unlike previous models in this space, where the system prompt needs to instruct the LLM to moderate either the user or an LLM turn. See the section on output format for more information. Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:One-Dimensional (1D)System Prompt with safety taxonomy and instruction, user message, LLM responseOther Properties Related to Input:Safety categories list, instruction asking to classify prompt or response Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: OR Output: ======= Output Type(s):Text JsonOutput Format:StringOutput Parameters:One-Dimensional (1D)Other Properties Related to Output:Text in JSON format with 3 fields: \"User Safety\", \"Response Safety\" and \"Safety Categories\". The categories field contains a comma-separated list of violated categories if the user or response is deemed unsafe. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: The violated category names are not fixed and are dependent on the categories mentioned in the instruction as present in the system prompt. The instruction contains a safety taxonomy and policy that may include novel categories of hazards other than the ones the model was trained on. Prompt Format: ============== The prompt template contains 3 sections,instruction,user prompt, oruser prompt and llm response (single or multiple turns). Example Model Input/Output for prompt safety moderation: -------------------------------------------------------- Input Output (Model Response) Example Model Input/Output for LLM response safety moderation: -------------------------------------------------------------- Input Output (Model Response) Model Version(s) ---------------- Llama 3.1 NemoGuard 8B ContentSafety Software Integration ==================== Runtime(s):PyTorch Runtime(s): Libraries:Meta'sllama-recipes, HuggingFacetransformerslibrary, HuggingFacepeftlibrary Libraries: llama-recipes (https://github.com/meta-llama/llama-recipes) transformers (https://github.com/huggingface/transformers) peft (https://github.com/huggingface/peft) Supported Hardware Platform(s):NVIDIA Ampere (A100 80GB, A100 40GB) Supported Hardware Platform(s): Preferred/Supported Operating System(s):Ubuntu Preferred/Supported Operating System(s): Training & Evaluation ===================== Training Dataset ================ Our curated training dataset consists of a mix collected or generated using the following data sources. • Human Prompts from Anthropic RLHF harmless datasetAnthropic RLHF,Do-Anything-Now DAN,AI-assisted Red-Teamingdatasets. Anthropic RLHF (https://huggingface.co/datasets/Anthropic/hh-rlhf) Do-Anything-Now DAN (https://github.com/verazuo/jailbreak_llms) AI-assisted Red-Teaming (https://www.kaggle.com/datasets/mpwolke/aart-ai-safety) • LLM response generated fromMistral-7B-v0.1 Mistral-7B-v0.1 (https://huggingface.co/mistralai/Mistral-7B-v0.1) • Response Safety Labels generated using ensemble of 3 LLMs:Mixtral-8x22B-v0.1,Mistral-NeMo-12B-Instruct, andGemma-2-27B-it. Mixtral-8x22B-v0.1 (https://huggingface.co/mistralai/Mixtral-8x22B-v0.1) Mistral-NeMo-12B-Instruct (https://huggingface.co/nvidia/Mistral-NeMo-12B-Instruct) Gemma-2-27B-it (https://huggingface.co/google/gemma-2-27b-it) • We additionally mix in topic-following data generated usingMixtral-8x7B-v0.1to improve the robustness and adaptability of the model. Mixtral-8x7B-v0.1 (https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) • Refusal data generated usingGemma-2-27B. Gemma-2-27B (https://build.nvidia.com/google/gemma-2-27b-it/modelcard) Data Collection Method • Hybrid: Human, Automated Labeling Method • Human - Overall content safety labels • LLM - Response safety labels Properties:Trained on approximately 25,007 user prompts, user prompts and LLM response single turn, user prompts and LLM response muliple turns. The validation set has 1245 samples and test set has 1964 samples from the above distribution. Properties: Additionally, the training dataset also contains 5k refusals data, that is synthetically generated. The validation set had 200 refusals and test set did not have any refusal data. The topic following data is approximately ~8k samples. Evaluation Dataset(s) ===================== • Test Split of the Aegis-AI-Content-Safety-Dataset-2.0Data Collection Method:Hybrid (Automated, Human)Labeling Method:Hybrid (Human, Automated) Data Collection Method: Labeling Method: • Toxic Chat DatasetData Collection Method:Hybrid (Automated, Human)Labeling Method:Human Toxic Chat Dataset (https://huggingface.co/datasets/lmsys/toxic-chat) Data Collection Method: Labeling Method: • OpenAI Moderation DatasetData Collection Method:Hybrid (Automated, Human)Labeling Method:Human OpenAI Moderation Dataset (https://huggingface.co/datasets/mmathys/openai-moderation-api-evaluation/tree/main) Data Collection Method: Labeling Method: • Wildguard PromptsData Collection Method:Hybrid (Automated, Human)Labeling Method:Hybrid (Automated, Human) Wildguard Prompts (https://huggingface.co/datasets/allenai/wildguardmix) Data Collection Method: Labeling Method: • Wildguard ResponsesData Collection Method:Hybrid (Automated, Human)Labeling Method:Hybrid (Automated, Human) Wildguard Responses (https://huggingface.co/datasets/allenai/wildguardmix) Data Collection Method: Labeling Method: • BeaverTails-30kData Collection Method:Hybrid (Automated, Human)Labeling Method:Hybrid (Automated, Human) BeaverTails-30k (https://huggingface.co/datasets/PKU-Alignment/BeaverTails) Data Collection Method: Labeling Method: • XSTest-ResponsesData Collection Method:Hybrid (Automated, Human)Labeling Method:Hybrid (Automated, Human) XSTest-Responses (https://huggingface.co/datasets/allenai/xstest-response) Data Collection Method: Labeling Method: Properties: ----------- • 1,984 prompt-response pair samples from the Aegis-AI-Content-Safety-Dataset-2.0 (test split) for evaluating on ourin-domainsafety risk taxonomy. in-domain • 5,083 prompt onlysamples from the ToxicChat dataset (test split) for evaluating generalization of performance on an out-of-domain safety risk taxonomy. • 1,680 prompt onlysamples from Open AI moderation dataset for evaluating generalization of performance on an out-of-domain safety risk taxonomy. • 1,729 prompt only samples from the WildguardMix (test split) for evaluating generalization of performance on a novel out-of-domain safety risk taxonomy. • 1,729 prompt-response pair samples from the WildguardMix (test split) for evaluating generalization of performance on a novel out-of-domain safety risk taxonomy. • 3,021 prompt-response pair samples from the BeaverTails-30k dataset (test split) for evaluating generalization of performance on a novel out-of-domain safety risk taxonomy. • 446 prompt-response pair samples from XSTest-Responses dataset for evaluating performance on adversarial refusal prompts. Inference ========= Engine:TensorRT-LLM available viaNVIDIA NIMsavailable on NGC. Engine: NVIDIA NIMs (https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/llama-3.1-nemoguard-8b-content-safety/) Test Hardware:Supported GPUs: A100, H100, L40S, A6000. Test Hardware: Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with the terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.Due to the serious nature of this project, annotators were asked to join on a volunteer basis based on their skill level, availability, and willingness to expose themselves to potentially unsafe content. Before work on this project began, all participants were asked to sign an Adult Content Acknowledgement that coincides with the organization's existing AntiHarassment Policy and Code of Conduct. This was to ensure that all annotators be made aware of the nature of this work, as well as the resources available to them should it affect their mental well-being. Regular 1:1 meetings were held between the leads assigned to this project and each annotator to make sure they are still comfortable with the material and are capable of continuing on with this type of work. Throughout the six months time span of the Content Moderation Guardrails project, we averaged twelve annotators at any given time. Of these twelve, four annotators come from Engineering backgrounds specializing in data analysis and collection, gaming, and robotics. Eight annotators have a background in Creative Writing, with specialization in linguistics, research and development, and other creative arts such as photography and film. All annotators have been extensively trained in working with Large Language Models (LLM), as well as other variations of Generative AI such as image retrieval or evaluations of multi-turn conversations. All are capable of generating creative text-based output and categorization work. Each of these twelve annotators resides in the United States, all from various ethnic and religious backgrounds that allow for representation across race, age, and social status. The process in which the Aegis-AI-Content-Safety-Dataset-2.0 creation abides by ethical data categorization work is based within the tooling ofLabel Studio, an open source data labeling tool often used for the organization's internal projects. This tooling technology allows for large sets of data to be analyzed by individual annotators without seeing the work of their peers. This is essential in preventing bias between annotators, as well as delivering prompts to each individual with variability so that no one annotator is completing similar tasks based on how the data was initially arranged. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. Label Studio (https://labelstud.io/) here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nemoguard-jailbreak-detect",
    "nvidia_url": "https://build.nvidia.com/nvidia/nemoguard-jailbreak-detect",
    "scraped_on": "2025-10-15T21:41:02.977509",
    "tags": [
      "jailbreak detection",
      "llm security",
      "nvidia",
      "nvidia nim",
      "prompt injection"
    ],
    "model_card": "Model Overview ============== Description: ============ NemoGuard JailbreakDetect was developed to detect attempts to jailbreak large language models. This model is ready for commercial use. License/Terms of Use: --------------------- NVIDIA Open Model License NVIDIA Open Model License (https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf) Reference(s): ============= Improved Large Language Model Jailbreak Detection via Pretrained Embeddings Improved Large Language Model Jailbreak Detection via Pretrained Embeddings (https://arxiv.org/abs/2412.01547) Model Architecture: =================== Architecture Type:Random ForestNetwork Architecture:N/A Architecture Type: Network Architecture: Input: ====== Input Type(s):Text EmbeddingInput Parameters:768 dimensional vectorInput Format(s):VectorOther Properties Related to Input:Must be an output from the corresponding embedding model. Eithernv-embedqa-e5-v5orsnowflake-arctic-m-long. Input Type(s): Input Parameters: Input Format(s): Other Properties Related to Input: nv-embedqa-e5-v5 (https://build.nvidia.com/nvidia/nv-embedqa-e5-v5) snowflake-arctic-m-long (https://huggingface.co/Snowflake/snowflake-arctic-embed-m-long) Output: ======= Output Type(s):Classification, ProbabilityOutput Format:Bool, FloatOutput Parameters:1DOther Properties Related to Output:N/A Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s): • Not Applicable (N/A) Supported Hardware Microarchitecture Compatibility: • x86 • x64 [Preferred/Supported] Operating System(s): • Windows • MacOS • Linux Model Version(s): ================= NemoGuard-JailbreakDetect-v1.0: Jailbreak detection model using Snowflake-arctic-embed-m embeddings Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= A combination of three open datasets, mixed together, de-duplicated, and reviewed for data quality. Jailbreak data was augmented with the use ofgarak. The datasets used are outlined below: garak (https://github.com/NVIDIA/garak) Advbench -------- Link:https://github.com/thunlp/Advbench** Data Collection Method by dataset Link: https://github.com/thunlp/Advbench • [Automated] ** Labeling Method by dataset • [Automated] Properties:520 entries, all comprised of jailbreak attempts. Properties: Wildjailbreak ------------- Link:https://huggingface.co/datasets/allenai/wildjailbreak** Data Collection Method by dataset Link: https://huggingface.co/datasets/allenai/wildjailbreak • Hybrid: Automated, Synthetic ** Labeling Method by dataset • [Automated] Properties:6387 total entries: 5721 benign prompts, 666 jailbreak attempts Properties: jackhao/jailbreak-classification -------------------------------- Link:https://huggingface.co/datasets/jackhhao/jailbreak-classification** Data Collection Method by dataset Link: https://huggingface.co/datasets/jackhhao/jailbreak-classification • [Automated] ** Labeling Method by dataset • [Automated] Properties:1306 total entries: 640 benign prompts, 666 jailbreak attempts Properties: Testing Dataset: ================ A stratified subset (20%) of the aggregate dataset was used for testing. Evaluation Dataset: =================== Evaluated onJailbreakHub. JailbreakHub (https://huggingface.co/datasets/walledai/JailbreakHub) Inference: ========== Engine:N/ATest Hardware: Engine: Test Hardware: • RTX A6000 • A100 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility, and we have established policies and practices to enable development for a wide array of AI applications.When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.1-nemoguard-8b-topic-control",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_1-nemoguard-8b-topic-control",
    "scraped_on": "2025-10-15T21:41:02.580120",
    "tags": [
      "content safety",
      "dialogue safety",
      "guard model",
      "llm safety",
      "nvidia"
    ],
    "model_card": "Model Overview ============== Description: ============ Llama 3.1 NemoGuard 8B TopicControlcan be used for topical and dialogue moderation of user prompts in human-assistant interactions being designed for task-oriented dialogue agents and custom policy-based moderation. Given a system instruction (also called topical instruction, i.e. specifying which topics are allowed and disallowed) and a conversation history ending with the last user prompt, the model returns a binary response that flags if the user message respects the system instruction, (i.e. message is on-topic or a distractor/off-topic). The base large language model (LLM) is the multilingualLlama-3.1-8B-Instructmodel from Meta. Llama 3.1 NemoGuard 8B TopicControl is LoRa-tuned on a topic-following dataset generated synthetically withMixtral-8x7B-Instruct-v0.1. This model is ready for commercial use. Llama 3.1 NemoGuard 8B TopicControl Llama-3.1-8B-Instruct (https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) Mixtral-8x7B-Instruct-v0.1 (https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) License/Terms of Use: --------------------- Governing NVIDIA Model Hosting Terms (Cloud API) GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community License Agreement. Additional Information:Llama 3.1 Community License Agreement. Built with Llama. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Llama 3.1 Community License Agreement (https://www.llama.com/llama3_1/license/) Reference(s): ============= Related paper: Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:The base model architecture is based on the Llama-3.1-8B-Instruct model from Meta (Model Card). We perform Parameter Efficient FineTuning (PEFT) over the base model using the following network architecture parameters: Architecture Type: Network Architecture: Model Card (https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/) • Rank: 8 • Alpha: 32 • Targeted low rank adaptation modules: 'k_proj', 'q_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj', 'gate_proj'. Training Method: The training method forLlama 3.1 NemoGuard 8B TopicControlinvolves the following concepts: Llama 3.1 NemoGuard 8B TopicControl • A system instruction which acts like a topical instruction with the rules that define the context of the user-assistant interaction, i.e. topics allowed or disallowed by the current task-oriented scenario, conversation style and tone, conversation flows. • Any user message in the conversation that respects the topical instruction is considered on-topic, while a user message that contradicts at least one of the rules is a distractor or off-topic. • A synthetic generated dataset, called CantTalkAboutThis-Mixtral-1.0, of approximately 1,000 multi-turn conversations is used to instruction-tune the base model. Each conversation has a specific topical instruction from various broad domains (i.e. customer support, travel, legal) and contains an entire conversation which is on-topic, together with several distractor user messages replacing some of the on-topic ones at specific key points in the conversation. • The model is instruction-tuned to detect whether a user message is either on-topic or a distractor given the topical instruction for the current conversation, with the LLM behaving as a classifier. Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:1D (One-Dimensional) List: System prompt with topical instructions, followed by a conversation structured as a list of user and assistant messages.Other Properties Related to Input:The conversation should end with a user message that is considered for topical moderation given the topical instruction and the context of the entire conversation (previous user and assistant turns). The input format for the system prompt and the conversation respects theOpenAI Chat specificationwidely adopted in the industry including byNVIDIA AI API. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: OpenAI Chat specification (https://platform.openai.com/docs/guides/text-generation) NVIDIA AI API (https://build.nvidia.com/) Sample input: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:1D (One-Dimensional)Other Properties Related to Output:The response is a binary string label determining if the last user turn in the input conversation respects the topical instruction. The label options are either\"on-topic\"or\"off-topic\". Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: \"on-topic\" \"off-topic\" Example Model Input/Output: --------------------------- Input Output (Model Response) Software Integration: ===================== Runtime Engine(s):PyTorchLibraries:Meta'sllama-recipes, HuggingFacetransformerslibrary, HuggingFacepeftlibrarySupported Hardware Platform(s):NVIDIA Ampere (A100 80GB, A100 40GB)Preferred/Supported Operating System(s):Linux (Ubuntu) Runtime Engine(s): Libraries: llama-recipes (https://github.com/meta-llama/llama-recipes) transformers (https://github.com/huggingface/transformers) peft (https://github.com/huggingface/peft) Supported Hardware Platform(s): Preferred/Supported Operating System(s): Model Version(s): ================= Llama 3.1 NemoGuard 8B TopicControl Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= Link:CantTalkABoutThisdatasetData Collection Method by dataset: SyntheticLabeling Method by dataset: SyntheticProperties:CantTalkABoutThis topic-following datasetcontains 1080 multi-turn conversations that are on-topic using 540 different topical instructions from various domains. For each on-topic conversation, we also generate off-topic/distractor turns at specific points in the conversation (about 4 distractors per conversation). Link: CantTalkABoutThis (https://github.com/makeshn/topic_following) Data Collection Method by dataset Labeling Method by dataset Properties: CantTalkABoutThis topic-following dataset (https://github.com/makeshn/topic_following) Testing Dataset: ================ The performance of the model is tested on a smaller, human-annotated subset of theCantTalkABoutThis topic-following datasetsynthetically created test set. The test set contains conversations on a different domain (banking) that does not appear in training or evaluation sets. While on-topic conversations are samples similar to the training dataset, the distractors are human annotated by expert annotators. CantTalkABoutThis topic-following dataset (https://github.com/makeshn/topic_following) banking Link:CantTalkABoutThis topic-following datasetData Collection Method by dataset: Hybrid: Synthetic, HumanLabeling Method by dataset: Hybrid: Synthetic, HumanProperties:We select 20 random dialogues from the synthetic test domain and manually ask two experts in dialogue systems to create five distractors per conversation. Thus, we also provide a small human annotated test set that is both more challenging and reflective of realistic scenarios. The test set contains 100 human annotated distractors and the reminder of the on-topic turns, having 11% of turns as distractors/off-topic. Link: CantTalkABoutThis topic-following dataset (https://github.com/makeshn/topic_following) Data Collection Method by dataset Labeling Method by dataset Properties: Evaluation Dataset: =================== The evaluation set is similar to the training dataset, synthetically generated on-topic conversations and distractors, but in thetraveldomain (not part of training set).Link:CantTalkABoutThisevaluation setData Collection Method by dataset: SyntheticLabeling Method by dataset: SyntheticProperties:We generate 20 multi-turn conversations on 10 different scenarios in the travel domain, each conversation having about 20 turns. travel Link: CantTalkABoutThis (https://github.com/makeshn/topic_following) Data Collection Method by dataset Labeling Method by dataset Properties: Inference: ========== Engine:TensorRT-LLM available viaNVIDIA NIMsavailable on NGC.Test Hardware:A100 80GB Engine: NVIDIA NIMs (https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/llama-3.1-nemoguard-8b-topic-control) Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards.Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "deepseek-r1",
    "nvidia_url": "https://build.nvidia.com/deepseek-ai/deepseek-r1",
    "scraped_on": "2025-10-15T21:41:02.216242",
    "tags": [
      "advanced reasoning",
      "chat",
      "deepseek-ai",
      "math"
    ],
    "model_card": "Model Overview Description: ============ DeepSeek-R1 is a first-generation reasoning model trained using large-scale reinforcement learning (RL) to solve complex reasoning tasks across domains such as math, code, and language. The model leverages RL to develop reasoning capabilities, which are further enhanced through supervised fine-tuning (SFT) to improve readability and coherence. DeepSeek-R1 achieves state-of-the-art results in various benchmarks and offers both its base models and distilled versions for community use. This model is ready for both research and commercial use. For more details, visit theDeepSeek website. DeepSeek website (https://www.deepseek.com/) Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. It is a community-driven model created by DeepSeek AI. See the officialDeepSeek-R1 Model Cardon Hugging Face for further details. DeepSeek-R1 Model Card (https://huggingface.co/deepseek-ai/DeepSeek-R1) License/Terms of Use: ===================== GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:MIT License. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) MIT License (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md) References: =========== • DeepSeek GitHub Repository DeepSeek GitHub Repository (https://github.com/deepseek-ai/DeepSeek-R1) • DeepSeek-R1 Paper DeepSeek-R1 Paper (https://arxiv.org/abs/2501.12948) Model Architecture: =================== Architecture Type:Mixture of Experts (MoE)Network Architecture: Architecture Type: Network Architecture: • Base Model: DeepSeek-V3-Base • Activated Parameters: 37 billion • Total Parameters: 671 billion • Distilled Models: Smaller, fine-tuned versions based on Qwen and Llama architectures. • Context Length: 128K tokens Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:(1D)Other Properties Related to Input:DeepSeek recommends adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance: Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: • Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs. • Avoid adding a system prompt; all instructions should be contained within the user prompt. Avoid adding a system prompt; all instructions should be contained within the user prompt. • For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\" • When evaluating model performance, it is recommended to conduct multiple tests and average the results. Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:(1D) Output Type(s): Output Format: Output Parameters: Software Integration: ===================== Runtime Engine(s):vLLM and SGLangSupported Hardware Microarchitecture Compatibility:NVIDIA Ampere, NVIDIA Blackwell, NVIDIA Jetson, NVIDIA Hopper, NVIDIA Lovelace, NVIDIA Pascal, NVIDIA Turing, and NVIDIA Volta architectures[Preferred/Supported] Operating System(s):Linux Runtime Engine(s): Supported Hardware Microarchitecture Compatibility: [Preferred/Supported] Operating System(s): Model Version(s): ================= DeepSeek-R1 V1.0 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ----------------- Data Collection Method by dataset:Hybrid: Human, AutomatedLabeling Method by dataset:Hybrid: Human, Automated Data Collection Method by dataset: Labeling Method by dataset: Testing Dataset: ---------------- Data Collection Method by dataset:Hybrid: Human, AutomatedLabeling Method by dataset:Hybrid: Human, Automated Data Collection Method by dataset: Labeling Method by dataset: Evaluation Dataset: ------------------- Data Collection Method by dataset:Hybrid: Human, AutomatedLabeling Method by dataset:Hybrid: Human, Automated Data Collection Method by dataset: Labeling Method by dataset: Inference: ========== Engine:SGLangTest Hardware:NVIDIA Hopper Engine: Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Model Limitations: ================== The base model was trained on data that contains toxic language and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive."
  },
  {
    "name": "mistral-small-24b-instruct",
    "nvidia_url": "https://build.nvidia.com/mistralai/mistral-small-24b-instruct",
    "scraped_on": "2025-10-15T21:41:01.836800",
    "tags": [
      "agent-centric",
      "chat",
      "code",
      "mistralai",
      "multilingual",
      "reasoning"
    ],
    "model_card": "Model Overview ============== Description: ============ Mistral Small 3 ( 2501 ) sets a new benchmark in the \"small\" Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models! Perfect for: • Fast response conversational agents. • Low latency function calling. • Subject matter experts via fine-tuning. • Local inference for hobbyists and organizations handling sensitive data. This model is ready for commercial use. Key features ------------ • Multilingual:Supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish. Multilingual: • Agent-Centric:Offers best-in-class agentic capabilities with native function calling and JSON outputting. Agent-Centric: • Advanced Reasoning:State-of-the-art conversational and reasoning capabilities. Advanced Reasoning: • Apache 2.0 License:Open license allowing usage and modification for both commercial and non-commercial purposes. Apache 2.0 License: • Context Window:A 32k context window. Context Window: • System Prompt:Maintains strong adherence and support for system prompts. System Prompt: • Tokenizer:Utilizes a Tekken tokenizer with a 131k vocabulary size. Tokenizer: Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAMistral Small 3 release announcement. Mistral Small 3 release announcement (https://mistral.ai/news/mistral-small-3/) License & Terms of use ====================== GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Apache 2.0. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) References(s): ============== Mistral Small 3BlogpostMistral-Small-24B-Instruct [HuggingFace] (https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501) Blogpost (https://mistral.ai/news/mistral-small-3/) https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501 Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:MistralModel Version:Small 3 ( 2501 ) Architecture Type: Network Architecture: Model Version: This transformer model has the following characteristics: • Layers: 40 • Dim: 5,120 • Head dim: 128 • Hidden dim: 32,768 • Activation Function: SwiGLU • Number of heads: 32 • Number of kv-heads: 8 (GQA) • Rotary embeddings (theta = 1M) • Vocabulary size: 32,768 • Context length: 32,768 Input • Input Type: Text • Input Format: String • Input Parameters: 1D • Other Properties: max_tokens, temperature, top_p, stop, frequency_penalty, presence_penalty, seed Output • Output Type: Text • Output Format: String • Output Parameters: 1D Benchmarks ---------- Publicly accesible benchmarks ----------------------------- Reasoning & Knowledge Math & Coding Instruction following Note: Note • Performance accuracy on all benchmarks were obtained through the same internal evaluation pipeline - as such, numbers may vary slightly from previously reported performance (Qwen2.5-32B-Instruct,Llama-3.3-70B-Instruct,Gemma-2-27B-IT). Qwen2.5-32B-Instruct (https://qwenlm.github.io/blog/qwen2.5/) Llama-3.3-70B-Instruct (https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) Gemma-2-27B-IT (https://huggingface.co/google/gemma-2-27b-it) • Judge based evals such as Wildbench, Arena hard and MTBench were based on gpt-4o-2024-05-13. Software Integration: ===================== Runtime Engine(s):TensorRT-LLMSupported Hardware Microarchitecture Compatibility:NVIDIA Ampere, NVIDIA Blackwell, NVIDIA Jetson, NVIDIA Hopper, NVIDIA Lovelace, NVIDIA Pascal, NVIDIA Turing, and NVIDIA Volta architecture[Preferred/Supported] Operating System(s):Linux Runtime Engine(s): Supported Hardware Microarchitecture Compatibility: [Preferred/Supported] Operating System(s): Model Version(s): ================= mistral-small-24b-instruct v1.0 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= Data Collection Method by dataset:Human, UnknownLabeling Method by dataset:Human, Unknown Data Collection Method by dataset: Labeling Method by dataset: Testing Dataset: ================ Data Collection Method by dataset:Human, UnknownLabeling Method by dataset:Human, Unknown Data Collection Method by dataset: Labeling Method by dataset: Evaluation Dataset: =================== Data Collection Method by dataset:Human, UnknownLabeling Method by dataset:Human, Unknown Data Collection Method by dataset: Labeling Method by dataset: Inference: ========== Engine:TensorRT-LLMTest Hardware:L40S Engine: Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "whisper-large-v3",
    "nvidia_url": "https://build.nvidia.com/openai/whisper-large-v3",
    "scraped_on": "2025-10-15T21:40:58.510568",
    "tags": [
      "asr",
      "ast",
      "batch",
      "multilingual",
      "nvidia nim",
      "nvidia riva",
      "openai",
      "speech-to-text",
      "whisper"
    ],
    "model_card": "Model Overview ============== Description: ============ This model is used to transcribe short-form audio files and is designed to be compatible withOpenAI's sequential long-form transcription algorithm. Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labeled data, Whisper models demonstrate a strong ability to generalize to many datasets and domains without the need for fine-tuning. Whisper-large-v3 is one of the 5 configurations of the model with 1550M parameters.This model version is optimized to run with NVIDIA TensorRT-LLM. This model is ready for commercial use. OpenAI's sequential long-form transcription algorithm Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see the Whisper Model Card on GitHub.(https://github.com/openai/whisper/blob/main/model-card.md). https://github.com/openai/whisper/blob/main/model-card.md License/Terms of Use: --------------------- GOVERNING TERMS: Use of this model is governed by theNVIDIA Community Model License Agreement. ADDITIONAL INFORMATION:MIT License. NVIDIA Community Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) MIT License (https://github.com/openai/whisper/blob/main/LICENSE) References: =========== WhisperwebsiteWhisper paper: website (https://openai.com/index/whisper/) Model Architecture: =================== Architecture Type:Transformer (Encoder-Decoder)Network Architecture:Whisper Architecture Type: Network Architecture: Input: ====== Input Type(s):Audio, Text-PromptInput Format(s):Linear PCM 16-bit 1 channel (Audio), String (Text Prompt)Input Parameters:One-Dimensional (1D) Input Type(s): Input Format(s): Input Parameters: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:1D Output Type(s): Output Format: Output Parameters: Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Blackwell Supported Operating System(s): • Linux Model Version(s): ================= Large-v3:Whisper large-v3 has the same architecture as the previous large and large-v2 models, except for the following minor differences: Large-v3: • The spectrogram input uses 128 Mel frequency bins instead of 80. • A new language token for Cantonese. Training Dataset: ================= Data Collection Method by dataset:[Hybrid: human, automatic] Data Collection Method by dataset: Labeling Method by dataset:[Automated] Labeling Method by dataset: Dataset License(s):NA Dataset License(s): Inference: ========== Engine:Tensor(RT)-LLM, TritonTest Hardware: Engine: Test Hardware: • A100 • H100 For more detail on model usage, evaluation, training data set and implications, please refer toWhisper Model Card. Whisper Model Card (https://github.com/openai/whisper/blob/main/model-card.md) Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "evo2-40b",
    "nvidia_url": "https://build.nvidia.com/arc/evo2-40b",
    "scraped_on": "2025-10-15T21:40:57.966067",
    "tags": [
      "arc",
      "biology",
      "bionemo",
      "dna generation",
      "drug discovery",
      "nim"
    ],
    "model_card": "Evo 2 NIM Overview ================== Description =========== Evo 2 is a biological foundation model that is able to integrate information over long genomic sequences while retaining sensitivity to single-nucleotide change. At 40 billion parameters, the model understands the genetic code for all domains of life and is the largest AI model for biology to date. Evo 2 was trained on a dataset of nearly 9 trillion nucleotides. This model is ready for commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case. License/Terms of Use ==================== GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Open Model License Agreement. ADDITIONAL INFORMATION:Apache 2.0 License. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Open Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Apache 2.0 License (https://github.com/arcinstitute/evo2) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. Deployment Geography ==================== Global Use Case ======== Evo is able to perform zero-shot function prediction for genes. Evo also can perform multi-element generation tasks, such as generating synthetic CRISPR-Cas molecular complexes. Evo 2 can also predict gene essentiality at nucleotide resolution and can generate coding-rich sequences up to at least 1M kb in length. Advances in multi-modal and multi-scale learning with Evo provide a promising path toward improving our understanding and control of biology across multiple levels of complexity. Release Date ============ NGC 08/28/2025 viahttps://catalog.ngc.nvidia.com/orgs/nim/teams/arc/containers/evo2-40b https://catalog.ngc.nvidia.com/orgs/nim/teams/arc/containers/evo2-40b Build.nvidia.com 08/28/2025 viahttps://build.nvidia.com/arc/evo2-40b https://build.nvidia.com/arc/evo2-40b Reference(s) ============ • Evo 2 preprint Evo 2 preprint (https://arcinstitute.org/manuscripts/Evo2) • Evo Designer Evo Designer (https://arcinstitute.org/tools/evo/evo-designer) • Evo Mech Interp Visualizer Evo Mech Interp Visualizer (https://arcinstitute.org/tools/evo/evo-mech-interp) • Evo 2 code Evo 2 code (https://github.com/arcinstitute/evo2) • NVIDIA Blog NVIDIA Blog (https://developer.nvidia.com/blog/understanding-the-language-of-lifes-biomolecules-across-evolution-at-a-new-scale-with-evo-2/) Model Architecture ================== • Architecture Type:Generative Neural Network Architecture Type: • Network Architecture:StripedHyena Network Architecture: • Number of model parameters:40 Billion Number of model parameters: Input ===== • Input Type(s):DNA Sequence Input Type(s): • Input Format(s):String Input Format(s): • Input Parameters:One-Dimensional (1D) Input Parameters: The user supplies an uppercase DNA sequence which may optionally be prepended with a specifically formatted taxonomy prompt (also uppercase) for an organism. The format of this prompt is|D__[DOMAIN];P__[PHYLUM];C__[CLASS];O__[ORDER];F__[FAMILY];G__[GENUS];S__[SPECIES]|\". For example, the token for Escherichia coli would be|D__BACTERIA;P__PSEUDOMONADOTA;C__GAMMAPROTEOBACTERIA;O__ENTEROBACTERALES;F__ENTEROBACTERIACEAE;G__ESCHERICHIA;S__ESCHERICHIA|. You can include a taxonomy token anywhere in your input DNA for your target organism if you want to bias the model's generated result in that direction. Output ====== • Output Type(s):DNA Sequence Output Type(s): • Output Format:String Output Format: • Output Parameters:One-Dimensional (1D) Output Parameters: Our AI models are designed and/or optimized to run onNVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. NVIDIA GPU-accelerated systems Software Integration ==================== Runtime Engine(s) ----------------- • PyTorch • Transformer Engine Supported Hardware Microarchitecture Compatibility -------------------------------------------------- • NVIDIA Hopper Preferred Operating System(s) ----------------------------- • Linux The integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment. Model Version(s) ================ evo2-40b:2.0.0 Training, Testing, and Evaluation Datasets ========================================== • Data Modality:Text Data Modality: • Text Training Data Size:9.3 Trillion Tokens Text Training Data Size: Multiple datasets were used for training, testing and evaluation (see details below). OpenGenome ---------- Link:https://www.science.org/doi/10.1126/science.ado9336 Link: https://www.science.org/doi/10.1126/science.ado9336 Data Collection Method:Automatic/Sensors/Human Data Collection Method: Labeling Method by dataset:N/A (no labels) Labeling Method by dataset: The previously published OpenGenome dataset was used in its entirety as part of the training data for this study. This included representative prokaryotic genomes available through GTDB release v214.1, and curated phage and plasmid sequences retrieved through IMG/VR and IMG/PR. Updated GTDB prokaryotic genomes -------------------------------- Link:https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkab776/6370255 Link: https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkab776/6370255 Data Collection Method:Automatic/Sensors/Human Data Collection Method: Labeling Method by dataset:Automatic Labeling Method by dataset: New prokaryotic reference genomes made available through the GTDB release 220.0 update were added to the training data for this study. New genomes were identified by selecting all species' reference genomes that had no previously published (release 214.1) genomes within their species cluster, resulting in 28,174 additional prokaryotic genomes. NCBI Eukaryotic reference genomes --------------------------------- Link:https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0997-x Link: https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0997-x Data Collection Method:Automatic/Sensors/Human Data Collection Method: Labeling Method by dataset:Automatic Labeling Method by dataset: All available eukaryotic reference genomes were downloaded from NCBI on 05/32/2024, excluding atypical genomes, metagenome-assembled genomes, and genomes from large multi-isolate projects. This resulted in 16,704 genomes including an estimated ~10.7 trillion nucleotides. Only contigs that were annotated as 'Primary Assembly', 'non-nuclear', or 'aGasCar1.hap1' (an aberrant annotation that applied only to GCA_027917425.1) were retained. Mash sketch was run on each individual genome with the flag \"-s 10000\" and the mash distance was calculated between all genomes as an estimate for their pairwise 1-ANI (average nucleotide identity). All genomes with a mash distance < 0.01 were joined with edges in a graph, and clusters were identified by finding connected components. One representative genome per cluster was chosen, prioritizing genomes with a higher assembly level and genomes with longer total sequence length. This clustering resulted in 15,148 candidate genomes. Genomes were further filtered by removing ambiguous nucleotides at the termini of each contig, by removing regions annotated as \"centromere\" in an available GFF file, and by removing contigs that were less than 10 kb in total length. Finally, contigs that were composed of more than 5% ambiguous nucleotides were removed. This final filtered set included 15,032 genomes and 6.98 trillion nucleotides. Bridge Metagenomic Data ----------------------- Link:https://www.nature.com/articles/s41586-024-07552-4 Link: https://www.nature.com/articles/s41586-024-07552-4 Data Collection Method:Automatic/Sensors/Human Data Collection Method: Labeling Method by dataset:Automatic Labeling Method by dataset: A previously described metagenomics dataset was further curated as part of the training data. This included 41,253 metagenomes and metagenome-assembled genomes from NCBI, JGI IMG, MGnify, MG-RAST, Tara Oceans samples, and Youngblut et al. animal gut metagenomes. All contigs were split at consecutive stretches of ambiguous nucleotides of length 5 bp or longer, the split contigs were filtered by a minimum sequence length of 1 kb, and only contigs with at least one open reading frame as predicted by prodigal were kept. Contig-encoded proteins were previously clustered at 90% identity using MMseqs. To further remove redundant sequences, contigs were sorted by descending length, and each contig was only retained if at least 90% of its respective protein clusters were not already in the sequence collection (determined using a bloom filter). NCBI Organelle -------------- Link:https://www.ncbi.nlm.nih.gov/datasets/organelle Link: https://www.ncbi.nlm.nih.gov/datasets/organelle Data Collection Method:Automatic/Sensors/Human Data Collection Method: Labeling Method by dataset:Automatic Labeling Method by dataset: Eukaryotic organelle genomes: 33,457 organelle genomes were identified and downloaded using the \"NCBI Organelle\" web resource. Ambiguous nucleotides at the terminal ends of the organelle genome sequences were removed. Sequences that had over 25 ambiguous nucleotides were removed. This resulted in 32,241 organelle genomes that were used for training, including 1,613 mitochondria, 12,856 chloroplasts, 1,751 plastids, 18 apicoplasts, 1 cyanelle, and 1 kinetoplast. Inference ========= Engine ------ • PyTorch 2.5.0 • Transformer Engine 1.9.0 Test Hardware ------------- • H200 (1 and 2 GPU configurations, 144 GB each) • H100 (2 GPU configuration, 80 GB each) Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report model quality, risk, security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "phi-4-multimodal-instruct",
    "nvidia_url": "https://build.nvidia.com/microsoft/phi-4-multimodal-instruct",
    "scraped_on": "2025-10-15T21:40:54.732663",
    "tags": [
      "chart and table understanding",
      "image-to-text",
      "language generation",
      "microsoft",
      "speech recognition",
      "visual qa"
    ],
    "model_card": "Overview ======== Description: ============ Phi-4-multimodal-instruct is a lightweight open multimodal foundation model that leverages the language, vision, and speech research and datasets used for Phi-3.5 and 4.0 models. The model processes text, image, and audio inputs, generating text outputs, and comes with 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning, and direct preference optimization to support precise instruction adherence and safety measures. This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAPhi-4-Multimodal-Instruct. Phi-4-Multimodal-Instruct (https://huggingface.co/microsoft/Phi-4-multimodal-instruct/blob/main/README.md) License/Terms of Use: --------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:MIT License. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) MIT License (https://huggingface.co/microsoft/Phi-4-multimodal-instruct/blob/main/LICENSE) Deployment Geography: --------------------- Global Release Date: ------------- February 2025 Reference(s): ============= Phi-4-Multimodal-Instruct Model Card Phi-4-Multimodal-Instruct Model Card (https://huggingface.co/microsoft/Phi-4-multimodal-instruct/blob/main/README.md) Intended Use ============ Primary use cases: ------------------ The model is intended for broad multilingual and multimodal commercial and research use. It provides uses for general purpose AI systems and applications which require memory/compute constrained environments, latency bound scenarios, strong reasoning, function and tool calling, general image understanding, optical character recognition, and chart and table understanding. The model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. Out-of-scope use cases ---------------------- The model is not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models and multimodal models, as well as performance differences across languages, as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including but not limited to privacy, trade compliance laws, etc.) that are relevant to their use case. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. Release Notes ============= This release of Phi-4-multimodal-instruct is based on valuable user feedback from the Phi-3 series. Previously, users could use a speech recognition model to talk to the Mini and Vision models. To achieve this, users needed to use a pipeline of two models: one model to transcribe the audio to text, and another model for the language or vision tasks. This pipeline means that the core model was not provided the full breadth of input information – e.g. cannot directly observe multiple speakers, background noises, jointly align speech, vision, language information at the same time on the same representation space. With Phi-4-multimodal-instruct, a single new open model has been trained across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network. The model employed new architecture, larger vocabulary for efficiency, multilingual, and multimodal support, and better post-training techniques were used for instruction following and function calling, as well as additional data leading to substantial gains on key multimodal capabilities. It is anticipated that Phi-4-multimodal-instruct will greatly benefit app developers and various use cases. The enthusiastic support for the Phi-4 series is greatly appreciated. Feedback on Phi-4-multimodal-instruct is welcomed and crucial to the model’s evolution and improvement. Thank you for being part of this journey! Model Architecture: =================== Architecture Type:Phi-4-multimodal-instruct has 5.6B parameters and is a multimodal transformer model. The model has the pretrained Phi-4-mini as the backbone language model, and the advanced encoders and adapters of vision and speech Architecture Type: Input: ====== Input Type(s):Text, Image, AudioInput Format(s):String, [.png, .jpg, .jpeg], [.mp3, .wav]Input Parameters:[1D, 2D]Other Properties Related to Input:Languages in training data | Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, UkrainianNote that these are for TEXT only. There is limited language support for IMAGE and AUDIO modalities. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Note that these are for TEXT only. There is limited language support for IMAGE and AUDIO modalities. • Vision: English • Audio: English, Chinese, German, French, Italian, Japanese, Spanish, Portuguese Vision ------ • Any common RGB/gray image format (e.g., (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\")) can be supported. • Resolution depends on the GPU memory size. Higher resolution and more images will produce more tokens, thus using more GPU memory. During training, 64 crops can be supported. If it is a square image, the resolution would be around (8448 by 8448). For multiple-images, at most 64 frames can be supported, but with more frames as input, the resolution of each frame needs to be reduced to fit in the memory. 448 by 8 Audio ----- • Any audio format that can be loaded by soundfile package should be supported. • To keep the satisfactory performance, maximum audio length is suggested to be 40 seconds. For summarization tasks, the maximum audio length is suggested to 30 minutes. Output: ======= Output Type(s):TextOutput Format(s):StringOutput Parameters:1D Output Type(s): Output Format(s): Output Parameters: Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper [Preferred/Supported] Operating System(s): • Linux Model Version(s): ================= Phi-4-multimodal-instruct v1.0 Training, Testing, and Evaluation Datasets: =========================================== Data Collection Methods: [Hybrid: Automated, Human, Synthetic]GPUS: 512 A100-80GTraining Time: 28 daysTraining Data: 5T text tokens, 2.3M speech hours, and 1.1T image-text tokenTraining Dates: Trained between December 2024 and January 2025Status: This is a static model trained on offline datasets with the cutoff date of June 2024 for publicly available data.Languages in training data: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, UkrainianNote that these are for TEXT only. There is limited language support for IMAGE and AUDIO modalities.Vision: EnglishAudio: English, Chinese, German, French, Italian, Japanese, Spanish, Portuguese Data Collection Methods GPUS Training Time Training Data Training Dates Status Languages in training data Note that these are for TEXT only. There is limited language support for IMAGE and AUDIO modalities.Vision: EnglishAudio: English, Chinese, German, French, Italian, Japanese, Spanish, Portuguese Data Overview ============= Phi-4-multimodal-instruct’s training data includes a wide variety of sources, totaling 5 trillion text tokens, and is a combination of: • Publicly available documents filtered for quality, selected high-quality educational data, and code • Newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (e.g., science, daily activities, theory of mind, etc.) • High quality human labeled data in chat format • Selected high-quality image-text interleave data • Synthetic and publicly available image, multi-image, and video data • Anonymized in-house speech-text pair data with strong/weak transcriptions • Selected high-quality publicly available and anonymized in-house speech data with task-specific supervisions • Selected synthetic speech data • Synthetic vision-speech data Focus was placed on the quality of data that could potentially improve the reasoning ability for the model, and the publicly available documents were filtered to contain a preferred level of knowledge. As an example, the result of a game in premier league on a particular day might be good training data for large foundation models, but such information was removed for the Phi-4-multimodal-instruct to leave more model capacity for reasoning for the model’s small size. The data collection process involved sourcing information from publicly available documents, with a focus on filtering out undesirable documents and images. To safeguard privacy, image and text data sources were filtered to remove or scrub potentially personal data from the training data. The decontamination process involved normalizing and tokenizing the dataset, then generating and comparing n-grams between the target dataset and benchmark datasets. Samples with matching n-grams above a threshold were flagged as contaminated and removed from the dataset. A detailed contamination report was generated, summarizing the matched text, matching ratio, and filtered results for further analysis. Safety ====== Approach -------- The Phi-4 family of models has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated datasets. The overall technique employed for safety alignment is a combination of SFT (Supervised Fine-Tuning), DPO (Direct Preference Optimization), and RLHF (Reinforcement Learning from Human Feedback) approaches by utilizing human-labeled and synthetic English-language datasets, including publicly available datasets focusing on helpfulness and harmlessness, as well as various questions and answers targeted to multiple safety categories. For non-English languages, existing datasets were extended via machine translation. Speech Safety datasets were generated by running Text Safety datasets through Azure TTS (Text-To-Speech) Service, for both English and non-English languages. Vision (text & images) Safety datasets were created to cover harm categories identified both in public and internal multi-modal RAI datasets. Safety Evaluation and Red-Teaming --------------------------------- Various evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets were leveraged to evaluate Phi-4 models’ propensity to produce undesirable outputs across multiple languages and risk categories. Several approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety post-training that was done as detailed in the Phi 3 Safety Post-Training paper had a positive impact across multiple languages and risk categories as observed by refusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Details on prior red team evaluations across Phi models can be found in the Phi 3 Safety Post-Training paper. For this release, the red teaming effort focused on the newest Audio input modality and on the following safety areas: harmful content, self-injury risks, and exploits. The model was found to be more susceptible to providing undesirable outputs when attacked with context manipulation or persuasive techniques. These findings applied to all languages, with the persuasive techniques mostly affecting French and Italian. This highlights the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages, and risk areas that account for cultural nuances where those languages are spoken. Vision Safety Evaluation ------------------------ To assess model safety in scenarios involving both text and images, Microsoft’s Azure AI Evaluation SDK was utilized. This tool facilitates the simulation of single-turn conversations with the target model by providing prompt text and images designed to incite harmful responses. The target model's responses are subsequently evaluated by a capable model across multiple harm categories, including violence, sexual content, self-harm, hateful and unfair content, with each response scored based on the severity of the harm identified. The evaluation results were compared with those of Phi-3.5-Vision and open-source models of comparable size. In addition, we ran both an internal and the public RTVLM and VLGuard multi-modal (text & vision) RAI benchmarks, once again comparing scores with Phi-3.5-Vision and open-source models of comparable size. However, the model may be susceptible to language-specific attack prompts and cultural context. Audio Safety Evaluation ----------------------- In addition to extensive red teaming, the Safety of the model was assessed through three distinct evaluations. First, as performed with Text and Vision inputs, Microsoft’s Azure AI Evaluation SDK was leveraged to detect the presence of harmful content in the model’s responses to Speech prompts. Second, Microsoft’s Speech Fairness evaluation was run to verify that Speech-To-Text transcription worked well across a variety of demographics. Third, we proposed and evaluated a mitigation approach via a system message to help prevent the model from inferring sensitive attributes (such as gender, sexual orientation, profession, medical condition, etc...) from the voice of a user. Model Quality ============= To understand the capabilities, Phi-4-multimodal-instruct was compared with a set of models over a variety of benchmarks using an internal benchmark platform. Users can refer to the Phi-4-Mini model card for details of language benchmarks. Below is a high-level overview of the model quality on representative speech and vision benchmarks: Speech Benchmarks ----------------- Phi-4-multimodal-instruct demonstrated strong performance in speech tasks: • Surpassed expert ASR model WhisperV3 and ST models SeamlessM4T-v2-Large in automatic speech recognition (ASR) and speech translation (ST). • Ranked number 1 on the Huggingface OpenASR leaderboard with a word error rate of 6.14% compared to the current best model at 6.5% as of February 18, 2025. • First open-sourced model capable of performing speech summarization, with performance close to GPT4o. • Exhibited a gap with closed models like Gemini-2.0-Flash and GPT-4o-realtime-preview on the speech QA task. Efforts are ongoing to improve this capability in future iterations. Vision Benchmarks ----------------- Vision-Speech Tasks ------------------- Phi-4-multimodal-instruct can process both image and audio together. The table below shows the model quality when the input query for vision content is synthetic speech on chart/table understanding and document reasoning tasks. Compared to other state-of-the-art omni models, Phi-4-multimodal-instruct achieves stronger performance on multiple benchmarks. Average 72.2 62.6 58.2 66.2 64.7 Vision Tasks ------------ To understand the vision capabilities, Phi-4-multimodal-instruct was compared with a set of models over a variety of zero-shot benchmarks using an internal benchmark platform. Below is a high-level overview of the model quality on representative benchmarks: Average 72.0 60.9 68.7 68.8 73.3 71.1 70.2 74.3 69.1 72.4 Below are the comparison results on existing multi-image tasks. On average, Phi-4-multimodal-instruct outperforms competitor models of the same size and is competitive with much bigger models on multi-frame capabilities. BLINK is an aggregated benchmark with 14 visual tasks that humans can solve very quickly but are still hard for current multimodal LLMs. Overall 61.3 48.1 51.2 55.3 52.5 59.3 64.0 56.9 62.4 Usage ===== Input Format ------------ Given the nature of the training data, Phi-4-Multimodal-Instruct model is best suited for prompts using the chat format as follows: • Text Chat FormatThis format is used for general conversation and instructions:<|system|>You are a helpful assistant.<|end|><|user|>How to explain Internetfora medieval knight?<|end|><|assistant|> Text Chat Format This format is used for general conversation and instructions: • Tool-enabled Function Call Format for TextThis format is used when the user wants the model to provide function calls based on the given tools. The user should provide the available tools in the system prompt, wrapped by<|tool|>and<|/tool|>tokens. The tools should be specified in JSON format, using a JSON dump structure. For example:<|system|>You are a helpful assistant with some tools.<|tool|>[{\"name\":\"get_weather_updates\",\"description\":\"Fetches weather updates for a given city using the RapidAPI Weather API.\",\"parameters\":{\"city\":{\"description\":\"The name of the city for which to retrieve weather information.\",\"type\":\"str\",\"default\":\"London\"}}}]<|/tool|><|end|><|user|>What is the weather likeinParis today?<|end|><|assistant|> Tool-enabled Function Call Format for Text This format is used when the user wants the model to provide function calls based on the given tools. The user should provide the available tools in the system prompt, wrapped by<|tool|>and<|/tool|>tokens. The tools should be specified in JSON format, using a JSON dump structure. For example: • Vision-Language FormatThis format is used for conversation with image:<|user|><|image_1|>Describe the imageindetail.<|end|><|assistant|>For multiple images, the user needs to insert multiple image placeholders in the prompt as below:<|user|><|image_1|><|image_2|><|image_3|>Summarize the content of the images.<|end|><|assistant|> Vision-Language Format This format is used for conversation with image: For multiple images, the user needs to insert multiple image placeholders in the prompt as below: • Speech-Language FormatThis format is used for various speech and audio tasks:<|user|><|audio_1|>{task prompt}<|end|><|assistant|>The task prompt can vary for different task.Automatic Speech Recognition:<|user|><|audio_1|>Transcribe the audio clip into text.<|end|><|assistant|>Automatic Speech Translation:<|user|><|audio_1|>Translate the audio to{lang}.<|end|><|assistant|>Automatic Speech Translation with Chain-of-thought:<|user|><|audio_1|>Transcribe the audio to text, andthentranslate the audio to{lang}. Use<sep>as a separator between the original transcript and the translation.<|end|><|assistant|>Spoken-query Question Answering:<|user|><|audio_1|><|end|><|assistant|> Speech-Language Format This format is used for various speech and audio tasks: The task prompt can vary for different task. • Automatic Speech Recognition:<|user|><|audio_1|>Transcribe the audio clip into text.<|end|><|assistant|> Automatic Speech Recognition: • Automatic Speech Translation:<|user|><|audio_1|>Translate the audio to{lang}.<|end|><|assistant|> Automatic Speech Translation: • Automatic Speech Translation with Chain-of-thought:<|user|><|audio_1|>Transcribe the audio to text, andthentranslate the audio to{lang}. Use<sep>as a separator between the original transcript and the translation.<|end|><|assistant|> Automatic Speech Translation with Chain-of-thought: • Spoken-query Question Answering:<|user|><|audio_1|><|end|><|assistant|> Spoken-query Question Answering: • Vision-Speech Format Vision-Speech Format This format is used for conversation with image and audio. The audio may contain query related to the image: For multiple images, the user needs to insert multiple image placeholders in the prompt as below: Vision ------ • Any common RGB/gray image format (e.g., (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\")) can be supported. • Resolution depends on the GPU memory size. Higher resolution and more images will produce more tokens, thus using more GPU memory. During training, 64 crops can be supported. If it is a square image, the resolution would be around (8448 by 8448). For multiple-images, at most 64 frames can be supported, but with more frames as input, the resolution of each frame needs to be reduced to fit in the memory. 448 by 8 Audio ----- • Any audio format that can be loaded by soundfile package should be supported. • To keep the satisfactory performance, maximum audio length is suggested to be 40 seconds. For summarization tasks, the maximum audio length is suggested to 30 minutes. Loading the Model Locally ------------------------- After obtaining the Phi-4-Multimodal-Instruct model checkpoints, users can use this sample code for inference. Inference: ========== Engine:vLLMTest Hardware:NVIDIA H100 Engine: Test Hardware: Responsible AI Considerations ============================= Like other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: • Quality of Service:The Phi models are trained primarily on English language content across text, speech, and visual inputs, with some additional multilingual coverage. Performance may vary significantly across different modalities and languages:Text:Languages other than English will experience reduced performance, with varying levels of degradation across different non-English languages. English language varieties with less representation in the training data may perform worse than standard American English.Speech:Speech recognition and processing shows similar language-based performance patterns, with optimal performance for standard American English accents and pronunciations. Other English accents, dialects, and non-English languages may experience lower recognition accuracy and response quality. Background noise, audio quality, and speaking speed can further impact performance.Vision:Visual processing capabilities may be influenced by cultural and geographical biases in the training data. The model may show reduced performance when analyzing images containing text in non-English languages or visual elements more commonly found in non-Western contexts. Image quality, lighting conditions, and composition can also affect processing accuracy. Quality of Service: • Text:Languages other than English will experience reduced performance, with varying levels of degradation across different non-English languages. English language varieties with less representation in the training data may perform worse than standard American English. Text: • Speech:Speech recognition and processing shows similar language-based performance patterns, with optimal performance for standard American English accents and pronunciations. Other English accents, dialects, and non-English languages may experience lower recognition accuracy and response quality. Background noise, audio quality, and speaking speed can further impact performance. Speech: • Vision:Visual processing capabilities may be influenced by cultural and geographical biases in the training data. The model may show reduced performance when analyzing images containing text in non-English languages or visual elements more commonly found in non-Western contexts. Image quality, lighting conditions, and composition can also affect processing accuracy. Vision: • Multilingual performance and safety gaps:We believe it is important to make language models more widely available across different languages, but the Phi 4 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards. Multilingual performance and safety gaps: • Representation of Harms & Perpetuation of Stereotypes:These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. Representation of Harms & Perpetuation of Stereotypes: • Inappropriate or Offensive Content:These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the case. Inappropriate or Offensive Content: • Information Reliability:Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. Information Reliability: • Limited Scope for Code:The majority of Phi 4 training data is based in Python and uses common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, it is strongly recommended that users manually verify all API uses. Limited Scope for Code: • Long Conversation:Phi 4 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift. Long Conversation: Developers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi 4 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include: • Allocation:Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. Allocation: • High-Risk Scenarios:Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. High-Risk Scenarios: • Misinformation:Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). Misinformation: • Generation of Harmful Content:Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. Generation of Harmful Content: • Misuse:Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Misuse: Ethical Considerations: ======================= Ethical considerations and guidelines. NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "phi-4-mini-instruct",
    "nvidia_url": "https://build.nvidia.com/microsoft/phi-4-mini-instruct",
    "scraped_on": "2025-10-15T21:40:51.321802",
    "tags": [
      "chat",
      "code generation",
      "language generation",
      "microsoft",
      "text-to-text"
    ],
    "model_card": "Overview ======== Description: ============ Phi-4-Mini is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures. This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAPhi-4-Mini. Phi-4-Mini (https://huggingface.co/microsoft/Phi-4-mini-instruct/blob/main/README.md) License/Terms of Use: --------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:MIT License. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) MIT License (https://huggingface.co/microsoft/Phi-4-mini-instruct/blob/main/LICENSE) Deployment Geography: --------------------- Global Release Date: ------------- February 2025 Reference(s): ============= Phi-4-Mini Model Card Phi-4-Mini Model Card (https://huggingface.co/microsoft/Phi-4-mini-instruct/blob/main/README.md) Intended Use ============ Primary use cases: ------------------ The model is intended for broad multilingual commercial and research use. The model provides uses for general purpose AI systems and applications which require 1) memory/compute constrained environments; 2) latency bound scenarios; 3) strong reasoning (especially math and logic). The model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. Out-of-scope use cases ====================== The model is not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models, as well as performance differences across languages, as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including but not limited to privacy, trade compliance laws, etc.) that are relevant to their use case. Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under. Release Notes ============= This release of Phi-4-Mini is based on valuable user feedback from the Phi-3 series. The Phi-4-Mini model employed new architecture for efficiency, larger vocabulary for multilingual multimodal support, and better post-training techniques were used for instruction following, function calling, as well as additional data leading to substantial gains on key capabilities. It is anticipated that most use cases will benefit from this release, but users are encouraged to test in their particular AI applications. The enthusiastic support for the Phi-4 series is greatly appreciated. Feedback on Phi-4-Mini is welcomed and crucial to the model’s evolution and improvement. Model Architecture: =================== Architecture Type:Dense decoder-only Transformer modelPhi-4-Mini has 3.8B parameters. When compared with Phi-3.5-Mini, the major changes with Phi-4-Mini are 200K vocabulary, grouped-query attention, and shared embedding.<br Architecture Type: Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:1DOther Properties Related to Input:128K token context length. Best suited for chat-completion format prompts. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: • Supported Languages: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian Supported Languages Output: ======= Output Type(s):TextOutput Format(s):StringOutput Parameters:1D Output Type(s): Output Format(s): Output Parameters: Supported Hardware Microarchitecture Compatibility: • NVIDIA Jetson • NVIDIA Volta [Preferred/Supported] Operating System(s): • Linux Model Version(s): ================= Phi-3.5-Mini v1.0 Training, Testing, and Evaluation Datasets: =========================================== Data Collection Methods: [Hybrid: Automated, Human, Synthetic]GPUS: 1024 A100-80GTraining Time: 14 daysTraining Data: Text in response to the inputTraining Dates: Trained between November and December 2024Status: This is a static model trained on an offline dataset with the cutoff date of June 2024 for publicly available data.Languages in training data: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian Data Collection Methods GPUS Training Time Training Data Training Dates Status Languages in training data Data Overview ============= Phi-4-Mini’s training data includes a wide variety of sources, totaling 5 trillion tokens, and is a combination of 1) publicly available documents filtered for quality, selected high-quality educational data, and code; 2) newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (e.g., science, daily activities, theory of mind, etc.); 3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. We focused on the quality of data that could potentially improve the reasoning ability for the model, and the publicly available documents were filtered to contain a preferred level of knowledge. As an example, the result of a game in premier league on a particular day might be good training data for frontier models, but such information was removed to leave more model capacity for reasoning for the model’s small size. More details about data can be found in the Phi-4-Mini technical report. The decontamination process involved normalizing and tokenizing the dataset, then generating and comparing n-grams between the target dataset and benchmark datasets. Samples with matching n-grams above a threshold were flagged as contaminated and removed from the dataset. A detailed contamination report was generated, summarizing the matched text, matching ratio, and filtered results for further analysis. Safety Approach =============== The Phi-4 family of models has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated datasets. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning), DPO (Direct Preference Optimization), and RLHF (Reinforcement Learning from Human Feedback) approaches by utilizing human-labeled and synthetic English-language datasets, including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories. For non-English languages, the existing datasets were extended via machine translation. Safety Evaluation and Red-Teaming ================================= Various evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets were leveraged to evaluate Phi-4 models’ propensity to produce undesirable outputs across multiple languages and risk categories. Several approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety post-training that was done as detailed in the Phi 3 Safety Post-Training paper had a positive impact across multiple languages and risk categories as observed by refusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Details on prior red team evaluations across Phi models can be found in the Phi 3 Safety Post-Training paper. For this release, initial insights from red teaming indicate that the models may at times be mistaken about which company created them; ad-hoc training data were added to correct this behavior. Another insight was that with function calling scenarios, the models could sometimes hallucinate function names or URL’s. Models may also be more susceptible to longer multi-turn jailbreak techniques across both English and non-English languages. These findings highlight the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages, and risk areas that account for cultural nuances where those languages are spoken. Model Quality ============= To understand the capabilities, the 3.8B parameters Phi-4-Mini model was compared with a set of models over a variety of benchmarks using an internal benchmark platform (See Appendix A for benchmark methodology). A high-level overview of the model quality is as follows: Popular Aggregated Benchmarks ----------------------------- Reasoning --------- Multilingual ------------ Math ---- Long Context ------------ Instruction Following --------------------- Function Calling ---------------- Code Generation --------------- Overall, the model with only 3.8B-param achieves a similar level of multilingual language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much factual knowledge, therefore, users may experience factual incorrectness. However, it may be possible to resolve such weakness by augmenting Phi-4 with a search engine, particularly when using the model under RAG settings. Usage ===== Input Formats ------------- Given the nature of the training data, the Phi-4-Mini-Instruct model is best suited for prompts using specific formats. Below are the two primary formats: • Chat FormatThis format is used for general conversation and instructions: Chat Format • Tool-Enabled Function Call FormatThis format is used when the user wants the model to provide function calls based on the given tools. The user should provide the available tools in the system prompt, wrapped by<|tool|>and<|/tool|>tokens. The tools should be specified inJSONformat, using aJSONdump structure. For example: Tool-Enabled Function Call Format Loading the Model Locally ------------------------- After obtaining the Phi-4-Mini-Instruct model checkpoints, users can use this sample code for inference. Inference: ========== Engine:vLLMTest Hardware:NVIDIA L40S Engine: Test Hardware: Responsible AI Considerations ============================= Like other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: • Quality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English. Quality of Service • Multilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 4 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards. Multilingual performance and safety gaps • Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. Representation of Harms & Perpetuation of Stereotypes • Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the case. Inappropriate or Offensive Content • Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. Information Reliability • Limited Scope for Code: The majority of Phi 4 training data is based in Python and uses common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, it is strongly recommended that users manually verify all API uses. Limited Scope for Code • Long Conversation: Phi 4 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift. Long Conversation Developers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi 4 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include: • Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. Allocation • High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. High-Risk Scenarios • Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). Misinformation • Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. Generation of Harmful Content • Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Misuse Ethical Considerations: ======================= Ethical considerations and guidelines. NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "deepseek-r1-distill-qwen-7b",
    "nvidia_url": "https://build.nvidia.com/deepseek-ai/deepseek-r1-distill-qwen-7b",
    "scraped_on": "2025-10-15T21:40:50.895269",
    "tags": [
      "chat",
      "coding",
      "deepseek-ai",
      "distillation",
      "math"
    ],
    "model_card": "Model Overview ============== Model Overview Background ---------- Background DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. DeepSeek-R1 sought to address these issues and enhance reasoning performance by incorporating cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. DescriptionDeepSeek-Distill-Qwen-7B is distilled from DeepSeek-R1 based on Qwen2.5-Math-7B. The reasoning patterns of larger models, DeepSeek-R1 in this case, can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. Using the reasoning data generated by DeepSeek-R1, dense models that are widely used in the research community can be fine-tuned. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. Description This model is ready for commercial use. Third-Party Community Consideration ----------------------------------- Third-Party Community Consideration This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to theDeepSeek-R1-Distill-Qwen-7B Model Card. DeepSeek-R1-Distill-Qwen-7B Model Card (https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B) License/Terms of Use -------------------- License/Terms of Use The NIM container is governed by theNVIDIA Software License AgreementandProduct-Specific Terms for AI Products; and the use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:MIT LicenseandApache 2.0 License. NVIDIA Software License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) Product-Specific Terms for AI Products (https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) MIT License (https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE) Apache 2.0 License (https://huggingface.co/Qwen/Qwen2.5-Math-7B/blob/main/LICENSE) Model Developer: DeepSeek-AI Model Developer Model ArchitectureArchitecture Type: TransformerNetwork Architecture: QwenVersion: 2.5 Model Architecture InputInput Type: TextInput Format: StringInput Parameters: 1DOther Properties Related to Input: Input Input Type Input Format Input Parameters Other Properties Related to Input DeepSeek recommends adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance: • Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs. • Avoid adding a system prompt; all instructions should be contained within the user prompt. • For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\" • When evaluating model performance, it is recommended to conduct multiple tests and average the results. Additionally, the DeepSeek-R1 series models tend to bypass thinking patterns (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance. To ensure that the model engages in thorough reasoning, DeepSeek recommends enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output. OutputOutput Type: TextOutput Format: StringOutput Parameters: 1D Output Software Integration -------------------- Software Integration Runtime Engine:TensorRT-LLMSupported Hardware Microarchitecture Compatibility:NVIDIA Hopper, NVIDIA LovelacePreferred/Supported Operating System(s):Linux Runtime Engine: Supported Hardware Microarchitecture Compatibility: Preferred/Supported Operating System(s): Model Version: 2.5 Training, Testing, and Evaluation Dataset: ========================================== Training, Testing, and Evaluation Dataset: Training Dataset ================ Training Dataset Data Collection Method by dataset:Automated. Reasoning data generated by DeepSeek-R1.Labelling Method by dataset:Automated Data Collection Method by dataset: Labelling Method by dataset: Evaluation Dataset ================== Evaluation Dataset Please see the Evaluation section of theDeepSeek-R1-Distill-Qwen-7B Model Cardfor more information. DeepSeek-R1-Distill-Qwen-7B Model Card (https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B) Data Collection Method by dataset:Hybrid: Human, Automated ========================================================== Data Collection Method by dataset: Labeling Method by dataset:Hybrid: Human, Automated Labeling Method by dataset: Inference ========= Inference Engine:TensorRT-LLMTest Hardware:L20, H20 Engine: Test Hardware: Ethical Considerations: ======================= Ethical Considerations: NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Model Limitations:The base model was trained on data that contains toxic language and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive. ===================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================== Model Limitations: You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws."
  },
  {
    "name": "deepseek-r1-distill-qwen-14b",
    "nvidia_url": "https://build.nvidia.com/deepseek-ai/deepseek-r1-distill-qwen-14b",
    "scraped_on": "2025-10-15T21:40:50.535042",
    "tags": [
      "chat",
      "coding",
      "deepseek-ai",
      "distillation",
      "math",
      "reasoning"
    ],
    "model_card": "Model Overview ============== Model Overview Background ---------- Background DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. DeepSeek-R1 sought to address these issues and further enhance reasoning performance by incorporating cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. Description ----------- Description DeepSeek-R1-Distill-Qwen-14B is a distilled version of the DeepSeek-R1 series, built upon the Qwen2.5-14B architecture. This model is designed to deliver efficient performance for reasoning, math, and code tasks while maintaining high accuracy. By distilling knowledge from the larger DeepSeek-R1 model, it provides state-of-the-art performance with reduced computational requirements. This model is ready for commercial use. For more details, visit theDeepSeek website. DeepSeek website (https://www.deepseek.com/) Third-Party Community Consideration =================================== Third-Party Community Consideration This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to theDeepSeek-R1-Distill-Qwen-14B Model Card. DeepSeek-R1-Distill-Qwen-14B Model Card (https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B) License/Terms of Use ==================== License/Terms of Use Governing NVIDIA Download Terms：The NIM container is governed by theNVIDIA Software License AgreementandProduct-Specific Termsfor AI Products; and the use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:MIT LicenseandApache 2.0 License. NVIDIA Software License Agreement (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) Product-Specific Terms (https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) MIT License (https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE) Apache 2.0 License (https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE) Model DeveloperDeepSeek AI Model Developer Model Architecture ================== Model Architecture Architecture Type: Distilled version of Mixture of Experts (MoE)Network Architecture: QwenVersion: 2.5 Input ----- Input Input Type:TextInput Format:StringInput Parameters:1DOther Properties Related to Input:DeepSeek recommends adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance: Input Type: Input Format: Input Parameters: Other Properties Related to Input: • Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs. • Avoid adding a system prompt; all instructions should be contained within the user prompt. • For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\" • When evaluating model performance, it is recommended to conduct multiple tests and average the results. Additionally, the DeepSeek-R1 series models tend to bypass thinking patterns (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance. To ensure that the model engages in thorough reasoning, DeepSeek recommends enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output. Output ------ Output Output Type:TextOutput Format:StringOutput Parameters:1D Output Type: Output Format: Output Parameters: Software Integration ==================== Software Integration Runtime Engine:TensorRT-LLMSupported Hardware Microarchitecture Compatibility:NVIDIA Hopper, NVIDIA LovelacePreferred/Supported Operating System(s):Linux Runtime Engine: Supported Hardware Microarchitecture Compatibility: Preferred/Supported Operating System(s): Training Dataset ---------------- Training Dataset Data Collection Method by dataset: AutomatedLabelling Method by dataset:AutomatedProperties:800k samples curated with DeepSeek-R1 Data Collection Method by dataset Labelling Method by dataset: Properties: Testing Dataset --------------- Testing Dataset Data Collection Method by dataset:Automated. Reasoning data generated by DeepSeek-R1.Labelling Method by dataset:Automated Data Collection Method by dataset: Labelling Method by dataset: Evaluation Dataset ------------------ Evaluation Dataset Please see the Evaluation section of theDeepSeek-R1-Distill-Qwen-14B Model Cardfor more information.Data Collection Method by dataset:Hybrid: Human, Automated DeepSeek-R1-Distill-Qwen-14B Model Card (https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B) Data Collection Method by dataset: Labeling Method by dataset:Hybrid: Human, Automated Labeling Method by dataset: Inference ========= Inference Engine:TensorRT-LLMTest Hardware:H20, L20 Engine: Test Hardware: Ethical Considerations ====================== Ethical Considerations NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Model Limitations ================= Model Limitations The base model was trained on data that contains toxic language and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive. You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws."
  },
  {
    "name": "deepseek-r1-distill-qwen-32b",
    "nvidia_url": "https://build.nvidia.com/deepseek-ai/deepseek-r1-distill-qwen-32b",
    "scraped_on": "2025-10-15T21:40:50.081695",
    "tags": [
      "chat",
      "coding",
      "deepseek-ai",
      "distillation",
      "math",
      "reasoning"
    ],
    "model_card": "Model Overview ============== Model Overview Background ---------- Background DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. DeepSeek-R1 sought to address these issues and further enhance reasoning performance by incorporating cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models. Description: Description DeepSeek-Distill-Qwen-32B is distilled from DeepSeek-R1 based on Qwen2.5-32B. The reasoning patterns of larger models, DeepSeek-R1 in this case, can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. Using the reasoning data generated by DeepSeek-R1, dense models that are widely used in the research community can be fine-tuned. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. This model is ready for commercial use. Third-Party Community Consideration ----------------------------------- Third-Party Community Consideration This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to theDeepSeek-R1-Distill-Qwen-32B Model Card. DeepSeek-R1-Distill-Qwen-32B Model Card (https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) License/Terms of Use -------------------- License/Terms of Use The NIM container is governed by theNVIDIA Software License AgreementandProduct-Specific Terms for AI Products; and the use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:MIT LicenseandApache 2.0 License. NVIDIA Software License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) Product-Specific Terms for AI Products (https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) MIT License (https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE) Apache 2.0 License (https://huggingface.co/Qwen/Qwen2.5-32B/blob/main/LICENSE) Model Developer: Deepseek-AI Model Developer Model ArchitectureArchitecture Type: TransformerNetwork Architecture: QwenVersion: 2.5 Model Architecture InputInput Type:TextInput Format:StringInput Parameters:1DOther Properties Related to Input:DeepSeek recommends adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance: Input Input Type: Input Format: Input Parameters: Other Properties Related to Input • Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs. • Avoid adding a system prompt; all instructions should be contained within the user prompt. • For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\" • When evaluating model performance, it is recommended to conduct multiple tests and average the results. Additionally, the DeepSeek-R1 series models tend to bypass thinking patterns (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance. To ensure that the model engages in thorough reasoning, DeepSeek recommends enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output. OutputOutput Type:TextOutput Format:StringOutput Parameters:1D Output Output Type: Output Format: Output Parameters: Software Integration -------------------- Software Integration Runtime Engine:TensorRT-LLMSupported Hardware Microarchitecture Compatibility:NVIDIA Hopper, NVIDIA LovelacePreferred/Supported Operating System(s):Linux Runtime Engine: Supported Hardware Microarchitecture Compatibility: Preferred/Supported Operating System(s): Training, Testing, and Evaluation Dataset: ========================================== Training, Testing, and Evaluation Dataset: Training Dataset: ================= Training Dataset: Data Collection Method by dataset: AutomatedLabelling Method by dataset:AutomatedProperties:800k samples curated with DeepSeek-R1 Data Collection Method by dataset Labelling Method by dataset: Properties: Testing Dataset:Data Collection Method by dataset:Automated. Reasoning data generated by DeepSeek-R1.Labelling Method by dataset:Automated Testing Dataset: Data Collection Method by dataset: Labelling Method by dataset: Evaluation Dataset: =================== Evaluation Dataset: Link:See Evaluation section of theHugging Face DeepSeek-R1-Distill-Qwen-32B Model CardData Collection Method by dataset:Hybrid: Human, AutomatedLabeling Method by dataset:Hybrid: Human, Automated Link: Hugging Face DeepSeek-R1-Distill-Qwen-32B Model Card (https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) Data Collection Method by dataset: Labeling Method by dataset: Inference ========= Inference Engine:TensorRT-LLMTest Hardware:L20, H20 Engine: Test Hardware: Ethical Considerations: ======================= Ethical Considerations: NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Model Limitations: ================== Model Limitations: The base model was trained on data that contains toxic language and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive. You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws."
  },
  {
    "name": "nemoretriever-parse",
    "nvidia_url": "https://build.nvidia.com/nvidia/nemoretriever-parse",
    "scraped_on": "2025-10-15T21:40:46.941023",
    "tags": [
      "data ingestion",
      "nemo retriever",
      "nvidia",
      "optical character recognition",
      "supported language - english",
      "table extraction"
    ],
    "model_card": "nemoretriever-parse Overview ============================ Description: ============ nemoretriever-parse is a general purpose text-extraction model, specifically designed to handle documents. Given an image, nemoretriever-parse is able to extract formatted-text, with bounding-boxes and the corresponding semantic class. This has downstream benefits for several tasks such as increasing the availability of training-data for Large Language Models (LLMs), improving the accuracy of retriever systems, and enhancing document understanding pipelines. This model is for demonstration purposes and it is not for production usage. License/Terms of Use: --------------------- GOVERNING TERMS: The NIM container is governed by theNVIDIA Software License AgreementandProduct-Specific Terms for NVIDIA AI Products. Use of this model is governed by theNVIDIA Community Model License. NVIDIA Software License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) Product-Specific Terms for NVIDIA AI Products (https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Deployment Geography: --------------------- Global Use Case: --------- nemoretriever-parse will be capable of comprehensive text understanding and document structure understanding. It will be used in retriever and curator solutions. Its text extraction datasets and capabilities will help with LLM and VLM training, as well as improve run-time inference accuracy of VLMs.The nemoretriever-parse model will perform text extraction from PDF and PPT documents. The nemoretriever-parse can classify the objects (title, section, caption, index, footnote, lists, tables, bibliography, image) in a given document, and provide bounding boxes with coordinates. Release Date ------------ March 5th 2025 Reference: ========== https://huggingface.co/docs/transformers/en/model_doc/mbart Model Architecture: =================== Architecture Type:Transformer-based vision-encoder-decoder modelNetwork Architecture: Architecture Type: Network Architecture: • Vision Encoder: ViT-H model (https://huggingface.co/nvidia/C-RADIO) https://huggingface.co/nvidia/C-RADIO • Adapter Layer: 1D convolutions & norms to compress dimensionality and sequence length of the latent space (1280 tokens to 320 tokens) • Decoder: mBart [1] 10 blocks • Tokenizer: Galactica (https://arxiv.org/abs/2211.09085); same as Nougat tokenizer https://arxiv.org/abs/2211.09085 Input: ====== Input Type(s):ImageInput Format(s):Red, Green, Blue (RGB)Input Parameters:2DOther Properties Related to Input: Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: • Max Input Resolution (Width, Height): 1648, 2048 • Min Input Resolution (Width, Height): 1024, 1280 • Channel Count: 3 Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:1DOther Properties Related to Output:nemoretriever-parse output format is a string which encodes text content (formatted or not) as well as bounding boxes and class attributes. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: ===================== Runtime Engine(s):TensorRT-LLMSupported Hardware Microarchitecture Compatibility: Runtime Engine(s): Supported Hardware Microarchitecture Compatibility: • NVIDIA Hopper • NVIDIA Ampere • NVIDIA Turing [Preferred/Supported] Operating System(s): • Linux Model Version(s): ================= nemoretriever-parse-v0.2-beta: As part of this first release, we share the set of weights named overjoyed-adder. Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ----------------- nemoretriever-parse is first pre-trained on our internal datasets: human, synthetic, and automated. Testing and Evaluation: ----------------------- nemoretriever-parse is evaluated on multiple datasets for robustness, including public and internal dataset. Inference: ========== Engine:TensorRT-LLMTest Hardware: Engine: Test Hardware: • NVIDIA H100 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "gemma-3-1b-it",
    "nvidia_url": "https://build.nvidia.com/google/gemma-3-1b-it",
    "scraped_on": "2025-10-15T21:40:43.712934",
    "tags": [
      "chat",
      "google",
      "language generation",
      "text-to-text",
      "translation"
    ],
    "model_card": "Gemma 3 model ============= Description =========== Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. This model is ready for commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAGemma 3model card. Gemma 3 (https://ai.google.dev/gemma/docs/core) License/Terms of Use ==================== GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service; and the use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:Gemma Terms of Use. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Gemma Terms of Use (https://ai.google.dev/gemma/terms) Deployment Geography ==================== Global Use Case ======== Models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Benefits ======== At the time of release, this family of models provides high-performance open vision-language model implementations designed from the ground up for responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown to provide superior performance to other, comparably-sized open model alternatives. Release Date ============ • Build.Nvidia.com- 3/12/2025 viahttps://build.nvidia.com/google/gemma-3-1b-itandhttps://build.nvidia.com/google/gemma-3-27b-it Build.Nvidia.com https://build.nvidia.com/google/gemma-3-1b-it https://build.nvidia.com/google/gemma-3-27b-it References ========== Model Page:GemmaAuthors: Google DeepMind Model Page Gemma (https://ai.google.dev/gemma/docs/core) Authors • Gemma 3 Technical Report Gemma 3 Technical Report (https://goo.gle/Gemma3Report) • Responsible Generative AI Toolkit Responsible Generative AI Toolkit (https://ai.google.dev/responsible) • Gemma on Kaggle Gemma on Kaggle (https://www.kaggle.com/models/google/gemma-3) • Gemma on Vertex Model Garden Gemma on Vertex Model Garden (https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3) Model Architecture ================== Architecture Type: Dense decoder-only Transformer model Architecture Type Inputs and outputs ------------------ Input ----- Input Type(s): Text (1B variant), Text+Image (4B, 12B and 27B variants) Input Type(s) Input Format(s): Input Format(s) • String • Image: jpg Input Parameters: Input Parameters • Text: One-dimensional (1D) Text • Image: Two-dimensional (2D) Image Other Properties Related to Input: Other Properties Related to Input • Text string, such as a question, a prompt, or a document to be summarized • Images, normalized to 896 x 896 resolution and encoded to 256 tokens each • Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and 32K tokens for the 1B size Output ------ Output Type(s): TextOutput Format: StringOutput Parameters: (1D)Other Properties Related to Output: Output Type(s) Output Format Output Parameters Other Properties Related to Output • Generated text in response to the input, such as an answer to a question, analysis of image content, or a summary of a document • Total output context of 8192 tokens Software Integration ==================== Runtime Engine(s): TRT-LLMSupported Hardware Microarchitecture Compatibility: NVIDIA Ampere, NVIDIA Blackwell, NVIDIA Jetson, NVIDIA Hopper, NVIDIA Lovelace, NVIDIA Pascal, NVIDIA Turing, and NVIDIA Volta architectures[Preferred/Supported] Operating System(s): Linux Runtime Engine(s) Supported Hardware Microarchitecture Compatibility [Preferred/Supported] Operating System(s) Model Version(s): ================= • Gemma 3 IT 1B: 1.0 (3/12/2025) Gemma 3 IT 1B • Gemma 3 IT 27B: 1.0 (3/12/2025) Gemma 3 IT 27B Software -------- Training was done usingJAXandML Pathways. JAX (https://github.com/jax-ml/jax) ML Pathways (https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/) JAX allows researchers to take advantage of the latest generation of hardware, including TPUs, for faster and more efficient training of large models. ML Pathways is Google's latest effort to build artificially intelligent systems capable of generalizing across multiple tasks. This is especially suitable for foundation models, including large language models like these ones. Together, JAX and ML Pathways are used as described in thepaper about the Gemini family of models;\"the 'single controller' programming model of Jax and Pathways allows a single Python process to orchestrate the entire training run, dramatically simplifying the development workflow.\" paper about the Gemini family of models (https://arxiv.org/abs/2312.11805) \"the 'single controller' programming model of Jax and Pathways allows a single Python process to orchestrate the entire training run, dramatically simplifying the development workflow.\" Training, Testing, and Evaluation Datasets ========================================== Training Dataset ---------------- Data Collection Method by dataset: Hybrid: Human, AutomatedLabeling Method by dataset: Hybrid: Human, Automated Data Collection Method by dataset Labeling Method by dataset These models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 14 trillion tokens, the 12B model was trained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and 1B with 2 trillion tokens. Here are the key components: • Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 140 languages. • Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code and understand code-related questions. • Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and address mathematical queries. • Images: A wide range of images enables the model to perform image analysis and visual data extraction tasks. The combination of these diverse data sources is crucial for training a powerful multimodal model that can handle a wide variety of different tasks and data formats. Data Preprocessing ------------------ Here are the key data cleaning and filtering methods applied to the training data: • CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content. • Sensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets. • Additional methods: Filtering based on content quality and safety in line withGoogle Responsible AI policies. Google Responsible AI policies (https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf) Testing Dataset --------------- Data Collection Method by dataset: Hybrid: Human, AutomatedLabeling Method by dataset: Hybrid: Human, Automated Data Collection Method by dataset Labeling Method by dataset Evaluation Dataset ------------------ Data Collection Method by dataset: Hybrid: Human, AutomatedLabeling Method by dataset: Hybrid: Human, Automated Data Collection Method by dataset Labeling Method by dataset Evaluation ========== Model evaluation metrics and results are highlighted below. Benchmark Results ----------------- These models were evaluated against a large collection of different datasets and metrics to cover different aspects of text generation: Reasoning and factuality ------------------------ HellaSwag (https://arxiv.org/abs/1905.07830) BoolQ (https://arxiv.org/abs/1905.10044) PIQA (https://arxiv.org/abs/1911.11641) SocialIQA (https://arxiv.org/abs/1904.09728) TriviaQA (https://arxiv.org/abs/1705.03551) Natural Questions (https://github.com/google-research-datasets/natural-questions) ARC-c (https://arxiv.org/abs/1911.01547) ARC-e (https://arxiv.org/abs/1911.01547) WinoGrande (https://arxiv.org/abs/1907.10641) BIG-Bench Hard (https://paperswithcode.com/dataset/bbh) DROP (https://arxiv.org/abs/1903.00161) STEM and code ------------- MMLU (https://arxiv.org/abs/2009.03300) AGIEval (https://arxiv.org/abs/2304.06364) MATH (https://arxiv.org/abs/2103.03874) GSM8K (https://arxiv.org/abs/2110.14168) GPQA (https://arxiv.org/abs/2311.12022) MBPP (https://arxiv.org/abs/2108.07732) HumanEval (https://arxiv.org/abs/2107.03374) Multilingual ------------ MGSM (https://arxiv.org/abs/2210.03057) Global-MMLU-Lite (https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite) WMT24++ (https://arxiv.org/abs/2502.12404v1) FloRes (https://arxiv.org/abs/2106.03193) XQuAD (https://arxiv.org/abs/1910.11856v3) ECLeKTic (https://arxiv.org/abs/2502.21228) IndicGenBench (https://arxiv.org/abs/2404.16816) Multimodal ---------- COCOcap (https://cocodataset.org/#home) DocVQA (https://www.docvqa.org/) InfoVQA (https://arxiv.org/abs/2104.12756) MMMU (https://arxiv.org/abs/2311.16502) TextVQA (https://textvqa.org/) RealWorldQA (https://paperswithcode.com/dataset/realworldqa) ReMI (https://arxiv.org/html/2406.09175v1) AI2D (https://allenai.org/data/diagrams) ChartQA (https://arxiv.org/abs/2203.10244) VQAv2 (https://visualqa.org/index.html) BLINK (https://arxiv.org/abs/2404.12390) OKVQA (https://okvqa.allenai.org/) TallyQA (https://arxiv.org/abs/1810.12440) SpatialSense VQA (https://arxiv.org/abs/1908.02660) CountBenchQA (https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/) Inference ========= Engine: TRT-LLMTest Hardware: NVIDIA Hopper Engine Test Hardware Ethics and Safety ================= Ethics and safety evaluation approach and results are highlighted below. Evaluation Approach ------------------- The evaluation method included structured evaluations and internal red-teaming testing of relevant content policies. Red-teaming was conducted by a number of different teams, each with different goals and human evaluation metrics. These models were evaluated against a number of different categories relevant to ethics and safety, including: • Child Safety: Evaluation of text-to-text and image to text prompts covering child safety policies, including child sexual abuse and exploitation. Child Safety • Content Safety:Evaluation of text-to-text and image to text prompts covering safety policies including, harassment, violence and gore, and hate speech. Content Safety: • Representational Harms: Evaluation of text-to-text and image to text prompts covering safety policies including bias, stereotyping, and harmful associations or inaccuracies. Representational Harms In addition to development level evaluations, assurance evaluations were conducted using the \"arms-length\" internal evaluations for responsibility governance decision making. They are conducted separately from the model development team, to inform decision making about release. High level findings are fed back to the model team, but prompt sets are held-out to prevent overfitting and preserve the results' ability to inform decision making. Assurance evaluation results are reported to the Responsibility & Safety Council as part of release review. Evaluation Results ------------------ For all areas of safety testing, there were major improvements in the categories of child safety, content safety, and representational harms relative to previous Gemma models. All testing was conducted without safety filters to evaluate the model capabilities and behaviors. For both text-to-text and image-to-text, and across all model sizes, the model produced minimal policy violations, and showed significant improvements over previous Gemma models' performance with respect to ungrounded inferences. One limitation of the evaluation was that the models incorporated only English language prompts. Usage and Limitations ===================== The potential limitations for these models are outlined below. Intended Usage -------------- Open vision-language models (VLMs) models have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development. • Content Creation and CommunicationText Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts.Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications.Text Summarization: Generate concise summaries of a text corpus, research papers, or reports.Image Data Extraction: These models can be used to extract, interpret, and summarize visual data for text communications. • Text Generation: These models can be used to generate creative text formats such as poems, scripts, code, marketing copy, and email drafts. • Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications. • Text Summarization: Generate concise summaries of a text corpus, research papers, or reports. • Image Data Extraction: These models can be used to extract, interpret, and summarize visual data for text communications. • Research and EducationNatural Language Processing (NLP) and VLM Research: These models can serve as a foundation for researchers to experiment with VLM and NLP techniques, develop algorithms, and contribute to the advancement of the field.Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice.Knowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics. • Natural Language Processing (NLP) and VLM Research: These models can serve as a foundation for researchers to experiment with VLM and NLP techniques, develop algorithms, and contribute to the advancement of the field. • Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice. • Knowledge Exploration: Assist researchers in exploring large bodies of text by generating summaries or answering questions about specific topics. Model Limitations ----------------- • Training DataThe quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses.The scope of the training dataset determines the subject areas the model can handle effectively. • The quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses. • The scope of the training dataset determines the subject areas the model can handle effectively. • Context and Task ComplexityModels are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging.A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • Models are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging. • A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • Language Ambiguity and NuanceNatural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language. • Natural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language. • Factual AccuracyModels generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • Models generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • Common SenseModels rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. • Models rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. Identified risks and mitigations -------------------------------- • Perpetuation of biases: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases. Perpetuation of biases • Generation of harmful content: Mechanisms and guidelines for content safety are essential. Developers are encouraged to exercise caution and implement appropriate content safety safeguards based on their specific product policies and application use cases. Generation of harmful content • Misuse for malicious purposes: Technical limitations and developer and end-user education can help mitigate against malicious applications of VLMs. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in theGemma Prohibited Use Policy. Misuse for malicious purposes Gemma Prohibited Use Policy (https://ai.google.dev/gemma/prohibited_use_policy) • Privacy violations: Models were trained on data filtered for removal of certain personal information and other sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques. Privacy violations Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "msa-search",
    "nvidia_url": "https://build.nvidia.com/colabfold/msa-search",
    "scraped_on": "2025-10-15T21:40:34.120675",
    "tags": [
      "biology",
      "bionemo",
      "colabfold",
      "drug discovery",
      "nim",
      "protein folding"
    ],
    "model_card": "Model Overview ============== Description: ------------ The MSA search NIM is powered by GPU MMSeqs2. GPU MMSeqs2 is a GPU-accelerated toolkit for protein database search and Multiple Sequence Alignment (MSA). While not a deep learning model, MMSeqs2 does require large protein databases for sequence similarity search. This NIM is ready for commercial use. Third-Party Community Consideration ----------------------------------- This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. ColabFold was developed by the authors of Mirditaet al. 2022. GPU MMSeqs2 was developed by the authors of Kallenbornet al. 2025. et al License / Terms of Use ---------------------- GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. Deployment Geography -------------------- Global Use Case -------- The MSA Search NIM enables researchers and commercial entities in the Drug Discovery, Life Sciences, and Digital Biology fields to rapidly generate multiple sequence alignments (MSA). The output MSA can be used in downstream protein structure prediction and evolutionary analysis applications. Release Date ------------ Build.nvidia.com March 16, 2025 viabuild.nvidia.com/colabfold/msa-search build.nvidia.com/colabfold/msa-search NGC March 16, 2025 References: ----------- Model Architecture: ------------------- Architecture Type:Not ApplicableNetwork Architecture:Not Applicable Architecture Type: Network Architecture: Input: ------ Input Type(s):Protein Sequence, DatabasesInput Format(s):String (less than or equal to 4096 characters), Constrained List of Strings (one or more valid database names)Input Parameters:String: 1D; Constrained List of Strings: 1DOther Properties Related to Input:NA Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ------- Output Type(s):Multiple Sequence Alignment in A3M or FASTA formatOutput Format:A3M or FASTA (text file)Output Parameters:1DOther Properties Related to Output:N/A Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: --------------------- Runtime Engine(s): • Python, C++, CUDA Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere, NVIDIA Hopper, NVIDIA Ada Lovelace [Preferred/Supported] Operating System(s): • [Linux] Model Version(s): ----------------- MMSeqs2 GPU 17-b804fUniref30_2302colabfold_envdb_202108PDB70_220313 Training & Evaluation: ====================== Not Applicable. Training Dataset: ----------------- Link:Not Applicable.** Data Collection Method by dataset Link: • [Not Applicable] ** Labeling Method by dataset • [Not Applicable] Properties:Not Applicable. Properties: Evaluation Dataset: ------------------- Link:Not Applicable.** Data Collection Method by dataset Link: • [Not Applicable] ** Labeling Method by dataset • [Not Applicable] Properties:Not Applicable Properties: Inference: ---------- Engine:Python, C++, CUDATest Hardware: Engine: Test Hardware: • NVIDIA A6000 • NVIDIA A100 • NVIDIA L40 • NVIDIA H100 Ethical Considerations: ----------------------- NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws."
  },
  {
    "name": "nemoretriever-page-elements-v2",
    "nvidia_url": "https://build.nvidia.com/nvidia/nemoretriever-page-elements-v2",
    "scraped_on": "2025-10-15T21:40:30.902185",
    "tags": [
      "chart detection",
      "data ingestion",
      "nemo retriever",
      "nvidia",
      "object detection",
      "run-on-rtx",
      "table detection"
    ],
    "model_card": "Model Overview ============== Description ----------- TheNeMo Retriever Page Elements v2model is a specialized object detection model designed to identify and extract key elements from charts and graphs. While the underlying technology builds upon work fromMegvii Technology, we developed our own base model through complete retraining rather than using pre-trained weights. YOLOX is an anchor-free version of YOLO (You Only Look Once), this model combines a simpler architecture with enhanced performance. The model is trained to detecttables,charts,infographics, andtitlesin documents. NeMo Retriever Page Elements v2 Megvii Technology (https://github.com/Megvii-BaseDetection/YOLOX) tables charts infographics titles This model supersedes thenv-yolox-page-elementsmodel. nv-yolox-page-elements (https://build.nvidia.com/nvidia/nv-yolox-page-elements-v1) This model is ready for commercial use and is a part of the NVIDIA NeMo Retriever family of NIM microservices specifically for object detection and multimodal extraction of enterprise documents. License/Terms of use -------------------- The use of this model is governed by theNVIDIA AI Foundation Models Community License Agreement. NVIDIA AI Foundation Models Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. Model Architecture ------------------ Architecture Type: YOLOXNetwork Architecture: DarkNet53 Backbone + FPN Decoupled head (one 1x1 convolution + 2 parallel 3x3 convolutions (one for the classification and one for the bounding box prediction). YOLOX is a single-stage object detector that improves on Yolo-v3.Deployment Geography: Global Architecture Type Network Architecture Deployment Geography Use Case:This model is designed for automating extraction of charts, tables, infographics, and titles in enterprise documents. Key applications include: Use Case • Enterprise document extraction, embedding and indexing • Augmenting Retrieval Augmented Generation (RAG) workflows with multimodal retrieval • Data extraction from legacy documents and reports Release Date: 2025-03-17 Release Date Intended use ------------ TheNeMo Retriever Page Elements v2model is suitable for users who want to extract, and ultimately retrieve, tables, charts and infographics. It can be used for document analysis, understanding and processing. NeMo Retriever Page Elements v2 Technical Details ================= Input ----- Input Type(s): ImageInput Format(s): Red, Green, Blue (RGB)Input Parameters: Two Dimensional (2D)Other Properties Related to Input: Image size resized to(1024, 1024) Input Type(s) Input Format(s) Input Parameters Other Properties Related to Input Output ------ Output Type(s): ArrayOutput Format: A dictionary of dictionaries containingnp.ndarray. The outer dictionary contains each sample (page). Inner dictionary contains list of dictionaries with bounding boxes, class, and confidence for that pageOutput Parameters: 1DOther Properties Related to Output: Output contains Bounding box, detection confidence and object class (chart, table, infographic, title). Thresholds used for non-maximum suppressionconf_thresh = 0.01;iou_thresh = 0.5Output Classes: Output Type(s) Output Format Output Parameters Other Properties Related to Output Output Classes • TableData structured in rows and columns • Data structured in rows and columns • ChartSpecifically bar charts, line charts, or pie charts • Specifically bar charts, line charts, or pie charts • InfographicVisual representations of information that is more complex than a chart, including diagrams and flowchartsMaps arenotconsidered infographics • Visual representations of information that is more complex than a chart, including diagrams and flowcharts • Maps arenotconsidered infographics not • TitleTitles can be page titles, section titles, or table/chart/infographic titles • Titles can be page titles, section titles, or table/chart/infographic titles Software Integration -------------------- Runtime:NeMo Retriever Page Elements v2NIMSupported Hardware Microarchitecture Compatibility: NVIDIA Ampere, NVIDIA Hopper, NVIDIA LovelaceSupported Operating System(s): Linux Runtime NeMo Retriever Page Elements v2 Supported Hardware Microarchitecture Compatibility Supported Operating System(s) Model Version(s): ================= • nemoretriever-page-elements-v2 Training Dataset & Evaluation ============================= Training Dataset ---------------- Data collection method by dataset: AutomatedLabeling method by dataset: Hybrid: Automated, HumanPretraining (by NVIDIA): 118,287 images of theCOCO train2017datasetFinetuning (by NVIDIA): 36,093 images fromDigital Corpora dataset, with annotations fromAzure AI Document Intelligenceand data annotation teamNumber of bounding boxes per class: 35,328 tables, 44,178 titles, 11,313 charts and 6,500 infographics. The layout model of Document Intelligence was used with2024-02-29-previewAPI version. Data collection method by dataset Labeling method by dataset Pretraining (by NVIDIA) COCO train2017 (https://cocodataset.org/#download) Finetuning (by NVIDIA) Digital Corpora dataset (https://digitalcorpora.org/) Azure AI Document Intelligence (https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence) Number of bounding boxes per class Evaluation Results ------------------ The primary evaluation set is a cut of the Azure labels and digital corpora images. Number of bounding boxes per class: 1,483 tables, 1,965 titles, 404 charts and 500 infographics. Mean Average Precision (mAP) was used as an evaluation metric, which measures the model's ability to correctly identify and localize objects across different confidence thresholds. Data collection method by dataset: Hybrid: Automated, HumanLabeling method by dataset: Hybrid: Automated, HumanProperties: We evaluated with Azure labels from manually selected pages, as well as manual inspection on public PDFs and powerpoint slides. Data collection method by dataset Labeling method by dataset Properties Per-class Performance Metrics: Per-class Performance Metrics Inference: ========== Engine: TensorRTTest hardware: See Support Matrix from NIM documentation Engine Test hardware Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. For more detailed information on ethical considerations for this model Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nemoretriever-graphic-elements-v1",
    "nvidia_url": "https://build.nvidia.com/nvidia/nemoretriever-graphic-elements-v1",
    "scraped_on": "2025-10-15T21:40:27.580265",
    "tags": [
      "chart detection",
      "data ingestion",
      "nemo retriever",
      "nvidia",
      "object detection",
      "run-on-rtx",
      "table detection"
    ],
    "model_card": "Model Overview ============== Model Overview Description ----------- Description TheNeMo Retriever Graphic Elements v1model is a specialized object detection system designed to identify and extract key elements from charts and graphs. Based on YOLOX, an anchor-free version of YOLO (You Only Look Once), this model combines a simpler architecture with enhanced performance. While the underlying technology builds upon work fromMegvii Technology, we developed our own base model through complete retraining rather than using pre-trained weights. NeMo Retriever Graphic Elements v1 Megvii Technology (https://github.com/Megvii-BaseDetection/YOLOX) The model excels at detecting and localizing various graphic elements within chart images, including titles, axis labels, legends, and data point annotations. This capability makes it particularly valuable for document understanding tasks and automated data extraction from visual content. This model is ready for commercial use and is a part of the NVIDIA NeMo Retriever family of NIM microservices specifically for object detection and multimodal extraction of enterprise documents. This model supersedes theCACHEDmodel. CACHED (https://build.nvidia.com/university-at-buffalo/cached) License/Terms of use -------------------- License/Terms of use Use of this model is governed by theNVIDIA AI Foundation Models Community License Agreement. NVIDIA AI Foundation Models Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-ai-foundation-models-community-license-agreement/) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. Deployment Geography: Global Deployment Geography Use Case: Use Case This model is designed for automating extraction of graphic elements of charts in enterprise documents. Key applications include: • Enterprise document extraction, embedding and indexing • Augmenting Retrieval Augmented Generation (RAG) workflows with multimodal retrieval • Data extraction from legacy documents and reports Release Date: 2025-03-17 Release Date Model Architecture ------------------ Model Architecture Architecture type:YOLOXNetwork architecture:DarkNet53 Backbone + FPN Decoupled head (one 1x1 convolution + 2 parallel 3x3 convolutions (one for the classification and one for the bounding box prediction) Architecture type: Network architecture: YOLOX is a single-stage object detector that improves on Yolo-v3. The model is fine-tuned to detect 10 classes of objects in documents: • Chart title • X-axis title • Y-axis title • X-axis label(s) • Y-axis label(s) • Legend label(s) • Legend title • Markings and values labels • Miscellaneous other texts on the chart Input ===== Input Input type(s):ImageInput format(s):Red, Green, Blue (RGB)Input parameters:Two Dimensional (2D)Other properties related to input:Expected input is anp.ndarrayimage of shape[Channel, Width, Height], or annp.ndarraybatch of image of shape[Batch, Channel, Width, Height]. Input type(s): Input format(s): Input parameters: Other properties related to input: Output ====== Output Output type(s):Text associated to each of the following classes : Output type(s): • [\"chart_title\", \"x_title\", \"y_title\", \"xlabel\", \"ylabel\", \"other\", \"legend_label\", \"legend_title\", \"mark_label\", \"value_label\"] Output format:Dict of StringOutput parameters:1DOther properties related to output:None Output format: Output parameters: Other properties related to output: Software Integration -------------------- Runtime Engine:NeMo Retriever Graphic Elements v1NIMSupported Hardware Microarchitecture Compatibility: NVIDIA Ampere, NVIDIA Hopper, NVIDIA LovelaceSupported Operating System(s): Linux Runtime Engine NeMo Retriever Graphic Elements v1 Supported Hardware Microarchitecture Compatibility Supported Operating System(s) Model Version(s): ================= Model Version(s): • nemoretriever-graphic-elements-v1 Training Dataset: ================= Training Dataset: • PubMed Central (PMC) Chart DatasetLink:https://chartinfo.github.io/index_2022.htmlData collection method:Automated, HumanLabeling method: HumanDescription:A real-world dataset collected from PubMed Central Documents and manually annotated, released in the ICPR 2022 CHART-Infographic competition. There are 5,614 images for chart element detection, 4,293 images for final plot detection and data extraction, and 22,924 images for chart classification. PubMed Central (PMC) Chart Dataset • Link:https://chartinfo.github.io/index_2022.html Link: https://chartinfo.github.io/index_2022.html • Data collection method:Automated, Human Data collection method: • Labeling method: Human Labeling method • Description:A real-world dataset collected from PubMed Central Documents and manually annotated, released in the ICPR 2022 CHART-Infographic competition. There are 5,614 images for chart element detection, 4,293 images for final plot detection and data extraction, and 22,924 images for chart classification. Description: • DeepRule datasetLink:https://github.com/soap117/DeepRuleData collection method:Automated, HumanLabeling method: Distillation by the CACHED modelDescription:The original dataset consists of 386,966 chart images obtained by crawling public Excel sheets from the web with texts overwritten to protect privacy. The CACHED model is used to pseudo-label the relevant classes. We used a subsample of 9,091 charts where a title was detected for training alongside with the 5,614 PMC training images. DeepRule dataset • Link:https://github.com/soap117/DeepRule Link: https://github.com/soap117/DeepRule • Data collection method:Automated, Human Data collection method: • Labeling method: Distillation by the CACHED model Labeling method • Description:The original dataset consists of 386,966 chart images obtained by crawling public Excel sheets from the web with texts overwritten to protect privacy. The CACHED model is used to pseudo-label the relevant classes. We used a subsample of 9,091 charts where a title was detected for training alongside with the 5,614 PMC training images. Description: Evaluation Results ================== Evaluation Results Results were evaluated using thePMC Chart dataset. TheMean Average Precision (mAP)was used as the evaluation metric to measure the model's ability to correctly identify and localize objects across different confidence thresholds. PMC Chart dataset Mean Average Precision (mAP) Data Collection & Labeling -------------------------- Data Collection & Labeling • Data collection method:Hybrid (Automated & Human) Data collection method: Hybrid (Automated & Human) • Labeling method:Hybrid (Automated & Human) Labeling method: Hybrid (Automated & Human) • Properties:The validation dataset is the same as thePMC Chart dataset. Properties: PMC Chart dataset Dataset Overview ---------------- Dataset Overview Number of bounding boxes and images per class: Label Images Boxes chart_title legend_label legend_title mark_label other value_label x_title xlabel y_title ylabel Total 11,520 Per-Class Performance Metrics ----------------------------- Per-Class Performance Metrics Average Precision (AP) ---------------------- Average Precision (AP) Class AP Class AP Class AP chart_title x_title y_title xlabel ylabel other legend_label legend_title mark_label value_label Average Recall (AR) ------------------- Average Recall (AR) Class AR Class AR Class AR chart_title x_title y_title xlabel ylabel other legend_label legend_title mark_label value_label Inference: ========== Inference: Engine:Tensor(RT)Test hardware:Tested on all supported hardware listed in compatibility section Engine: Test hardware: Ethical Considerations: ======================= Ethical Considerations: NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. For more detailed information on ethical considerations for this model Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nemoretriever-table-structure-v1",
    "nvidia_url": "https://build.nvidia.com/nvidia/nemoretriever-table-structure-v1",
    "scraped_on": "2025-10-15T21:40:24.249785",
    "tags": [
      "chart detection",
      "data ingestion",
      "nemo retriever",
      "nvidia",
      "object detection",
      "run-on-rtx",
      "table detection"
    ],
    "model_card": "Model Overview ============== Description ----------- TheNeMo Retriever Table Structure v1model is a specialized object detection model designed to identify and extract the structure of tables in images. Based on YOLOX, an anchor-free version of YOLO (You Only Look Once), this model combines a simpler architecture with enhanced performance. While the underlying technology builds upon work fromMegvii Technology, we developed our own base model through complete retraining rather than using pre-trained weights. NeMo Retriever Table Structure v1 Megvii Technology (https://github.com/Megvii-BaseDetection/YOLOX) The model excels at detecting and localizing the fundamental structural elements within tables. Through careful fine-tuning, it can accurately identify and delineate three key components within tables: • Individual cells (including merged cells) • Rows • Columns This specialized focus on table structure enables precise decomposition of complex tables into their constituent parts, forming the foundation for downstream retrieval tasks. This model helps convert tables into the markdown format which can improve retrieval accuracy. This model is ready for commercial use and is a part of the NVIDIA NeMo Retriever family of NIM microservices specifically for object detection and multimodal extraction of enterprise documents. License/Terms of use -------------------- The use of this model is governed by theNVIDIA AI Foundation Models Community License Agreement. NVIDIA AI Foundation Models Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. Model Architecture ------------------ Architecture Type: YOLOXNetwork Architecture: DarkNet53 Backbone + FPN Decoupled head (one 1x1 convolution + 2 parallel 3x3 convolutions (one for the classification and one for the bounding box prediction). The YOLOX architecture is a single-stage object detector that improves on Yolo-v3.Deployment Geography: Global Architecture Type Network Architecture Deployment Geography Use Case:This model specializes in analyzing images containing tables by: Use Case • Detecting and extracting table structure elements (rows, columns, and cells) • Providing precise location information for each detected element • Supporting downstream tasks like table analysis and data extraction The model is designed to work in conjunction with OCR (Optical Character Recognition) systems to: • Identify the structural layout of tables • Preserve the relationships between table elements • Enable accurate extraction of tabular data from images Ideal for: • Document processing systems • Automated data extraction pipelines • Digital content management solutions • Business intelligence applications Release Date: 2025-03-17 Release Date Technical Details ================= Input ----- Input type(s): ImageInput format(s): Red, Green, Blue (RGB)Input parameters: Two Dimensional (2D)Other properties related to input: Image size resized to(1024, 1024) Input type(s) Input format(s) Input parameters Other properties related to input Output ------ Output Type(s): ArrayOutput Format: A dictionary of dictionaries containingnp.ndarrayobjects. The outer dictionary contains each sample (table). Inner dictionary contains list of dictionaries with bounding boxes, class, and confidence for that tableOutput Parameters: 1DOther Properties Related to Output: Output contains Bounding box, detection confidence and object class (cell, row, column). Thresholds used for non-maximum suppressionconf_thresh = 0.01;iou_thresh = 0.25 Output Type(s) Output Format Output Parameters Other Properties Related to Output Software Integration -------------------- Runtime:NeMo Retriever Table Structure v1NIMSupported Hardware Microarchitecture Compatibility: NVIDIA Ampere, NVIDIA Hopper, NVIDIA LovelaceSupported Operating System(s): Linux Runtime NeMo Retriever Table Structure v1 Supported Hardware Microarchitecture Compatibility Supported Operating System(s) Model Version(s): ================= • nemoretriever-table-structure-v1 Training Dataset & Evaluation ============================= Training Dataset ---------------- Data collection method by dataset: AutomatedLabeling method by dataset: AutomatedPretraining:COCO train2017Finetuning (by NVIDIA): 23,977 images fromDigital Corpora dataset, with annotations fromAzure AI Document Intelligence. Number of bounding boxes per class: 1,828,978 cells, 134,089 columns and 316,901 rows. The layout model of Document Intelligence was used with2024-02-29-previewAPI version. Data collection method by dataset Labeling method by dataset Pretraining COCO train2017 (https://cocodataset.org/#download) Finetuning (by NVIDIA) Digital Corpora dataset (https://digitalcorpora.org/) Azure AI Document Intelligence (https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence) Evaluation Results ------------------ The primary evaluation set: 2,459 digital corpora images with Azure labels. Number of bounding boxes per class: 200,840 cells, 13,670 columns and 34,575 rows. mAP was used as an evaluation metric.Data collection method by dataset: Hybrid: Automated, HumanLabeling method by dataset: Hybrid: Automated, HumanProperties: We evaluated with Azure labels from manually selected pages, as well as manual inspection on public PDFs and powerpoint slides. The primary evaluation set Data collection method by dataset Labeling method by dataset Properties Per-class Performance Metrics: Per-class Performance Metrics Inference: ========== Engine: TensorRT.Test hardware: See Support Matrix from NIM documentation. Engine Test hardware Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. For more detailed information on ethical considerations for this model Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "deepseek-r1-distill-llama-8b",
    "nvidia_url": "https://build.nvidia.com/deepseek-ai/deepseek-r1-distill-llama-8b",
    "scraped_on": "2025-10-15T21:40:21.061422",
    "tags": [
      "chat",
      "coding",
      "deepseek-ai",
      "distillation",
      "math",
      "reasoning",
      "run-on-rtx"
    ],
    "model_card": "Model Overview Description: ============ DeepSeek-R1-Distill-Llama-8B is a distilled version of the DeepSeek-R1 series, built upon the Llama3.1-8B-Instruct architecture. This model is designed to deliver efficient performance for reasoning, math, and code tasks while maintaining high accuracy. By distilling knowledge from the larger DeepSeek-R1 model, it provides state-of-the-art performance with reduced computational requirements. This model is ready for both research and commercial use. For more details, visit theDeepSeek website. DeepSeek website (https://www.deepseek.com/) Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIADeepSeek-R1 Model Card. DeepSeek-R1 Model Card (https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B) License/Terms of Use -------------------- GOVERNING TERMS: The NIM container is governed by theNVIDIA Software License AgreementandProduct-Specific Terms for AI Products; and the use of this model is governed by theNVIDIA Community Model License. Additional Information:MIT License;Meta Llama 3.1 Community License Agreement. Built with Llama. NVIDIA Software License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) Product-Specific Terms for AI Products (https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) MIT License (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md) Meta Llama 3.1 Community License Agreement (https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/blob/main/LICENSE) References: =========== • DeepSeek GitHub Repository DeepSeek GitHub Repository (https://github.com/deepseek-ai/DeepSeek-V3) • DeepSeek-R1 Paper DeepSeek-R1 Paper (https://arxiv.org/abs/2501.12948) • Hugging Face Model Card for DeepSeek-R1-Distill-Llama-8B Hugging Face Model Card for DeepSeek-R1-Distill-Llama-8B (https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B) Model Architecture: =================== Architecture Type:Distilled version of Mixture of Experts (MoE)Base Model:Llama3.1-8B-Instruct Architecture Type: Base Model: Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:(1D)Other Properties Related to Input:DeepSeek recommends adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance: Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: • Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs. • Avoid adding a system prompt; all instructions should be contained within the user prompt. Avoid adding a system prompt; all instructions should be contained within the user prompt. • For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\" • When evaluating model performance, it is recommended to conduct multiple tests and average the results. Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:(1D) Output Type(s): Output Format: Output Parameters: Software Integration: ===================== Runtime Engine(s):TensorRT-LLMSupported Hardware Microarchitecture Compatibility:NVIDIA's Ampere, NVIDIA Blackwell, NVIDIA Jetson, NVIDIA Hopper, NVIDIA Lovelace, NVIDIA Pascal, NVIDIA Turing, and NVIDIA Volta architectures[Preferred/Supported] Operating System(s):Linux Runtime Engine(s): Supported Hardware Microarchitecture Compatibility: [Preferred/Supported] Operating System(s): Model Version(s): ================= DeepSeek-R1-Distill-Llama-8B Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= Data Collection Method by dataset:Hybrid: Human, AutomatedLabeling Method by dataset:Hybrid: Human, Automated Data Collection Method by dataset: Labeling Method by dataset: Testing Dataset: ================ Data Collection Method by dataset:Hybrid: Human, AutomatedLabeling Method by dataset:Hybrid: Human, Automated Data Collection Method by dataset: Labeling Method by dataset: Evaluation Dataset: =================== Data Collection Method by dataset:Hybrid: Human, AutomatedLabeling Method by dataset:Hybrid: Human, Automated Data Collection Method by dataset: Labeling Method by dataset: Inference: ========== Engine:TensorRT-LLMTest Hardware:NVIDIA Hopper Engine: Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Model Limitations: ================== The DeepSeek-R1-Distill model may struggle with open-ended or complex tasks, such as mathematical problems, if a directive is not included in the prompt to reason step by step and put the final answer within a boxed notation. Additionally, the model may face challenges with benchmarks requiring sampling if the temperature, top-p value, and number of responses per query are not set correctly. The base model was trained on data that contains toxic language and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive."
  },
  {
    "name": "nv-embedcode-7b-v1",
    "nvidia_url": "https://build.nvidia.com/nvidia/nv-embedcode-7b-v1",
    "scraped_on": "2025-10-15T21:40:20.730268",
    "tags": [
      "embedding",
      "nemo retriever",
      "nvidia",
      "retrieval augmented generation"
    ],
    "model_card": "Model Overview ============== Description: ============ The NV-EmbedCode model is a 7B Mistral-based embedding model optimized for code retrieval, supporting text, code, and hybrid queries. Code retrieval is a critical task in many domains including coding assistance, code explanation, summarization, and documentation search. NV-EmbedCode transforms the input code or textual data into dense vector representations, known as embeddings, enabling effective retrieval and search. This model is ready for commercial use. NV-EmbdeCode is part of NVIDIA's effort to provide state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. Intended use ============ The NV-EmbedCode model is most suitable for users who want to build a code retrieval system over a large text or code corpus, leveraging the latest dense retrieval technologies. License/Terms of Use -------------------- The use of this model is governed by theNVIDIA AI Foundation Models Community License Agreementand theApache License 2.0. NVIDIA AI Foundation Models Community License Agreement (https://developer.nvidia.com/downloads/nv-ai-foundation-models-license) Apache License 2.0 (https://choosealicense.com/licenses/apache-2.0/) Technology can have a profound impact on people and the world, and NVIDIA is committed to enabling trust and transparency in AI development. NVIDIA encourages users to adopt principles of AI ethics and trustworthiness to guide your business decisions by following the guidelines in the NVIDIA AI Foundation Models Community License Agreement. Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Fine-tuned NVIDIA Retrieval QA Mistral 7B Embedding modelEmbedding Dimension:4096Parameter Count:7.1 billion Architecture Type: Network Architecture: Embedding Dimension: Parameter Count: The NV-EmbedCode model is a transformer encoder - a fine-tuned version ofNVIDIA Retrieval QA Mistral 7B Embedding model, with 32 layers and 4096 as embedding size, which is trained on public datasets. Mistral Models are pre-trained with casual attention. As ourresearchdemonstrated that bi-directional attention improved the performance, NV-Embed series of models use bi-directional attention. Embedding models for retrieval are typically trained using a bi-encoder architecture. This involves encoding a pair of query and chunked passages independently using the embedding model. Contrastive learning is used to maximize the similarity between the query and its relevant (positive) passage, while minimizing the similarity to irrelevant (negative) passages. NVIDIA Retrieval QA Mistral 7B Embedding model (https://build.nvidia.com/nvidia/nv-embedqa-mistral-7b-v2) research (https://arxiv.org/abs/2405.17428) Model Version(s): ================= NVIDIA Code Embedding v1 Short name: NV-EmbedCode-v1 Input ----- Input Type:Code or textInput Format:List of strings (any list length, any string length)Other Properties Related to Input:The model was trained with documents of length up to 512 tokens however, similar to Mistral-7b, it has a theoretical attention span of approximately 131K tokens. Input Type: Input Format: Other Properties Related to Input: Output ------ Output Type:FloatsOutput Format:List of float arrays (same length as input list, 4096 dimensions per float array)Other Properties Related to Output:Model outputs embedding vectors of dimension 4096 for each text string. Output Type: Output Format: Other Properties Related to Output: Training Dataset & Evalution: ============================= Training Dataset: ================= Our training dataset is a carefully curated blend of multiple sources. It includes publicly available code retrieval datasets with commercial licenses, issue description–code pairs sourced from public GitHub repositories with commercial licenses, and synthetic data generated in response to coding questions. We prefix the queries with task-specific instructions following our research inNV-Embed. For general tasks, we used ''Instruct: Retrieve code or text based on user query.\\nQuery:''. The instruction can be changed based on the retrieval task. NV-Embed (https://arxiv.org/abs/2405.17428) The training dataset details are as follows: Use Case:Code retrieval from text or code data.Data Sources:Public datasets licensed for commercial use and synthetically-generated data.Language:English (US), programming languages including Python, C/C++, Java, JavaScript, SQL, Go, Ruby, PHP.Volume:534k pairs of query-positive document.Data Collection Method by dataset:UnknownLabeling Method by dataset:The synthetic data is generated usingDeepSeek-V2.5. Use Case: Data Sources: Language: Volume: Data Collection Method by dataset: Labeling Method by dataset: DeepSeek-V2.5 (https://huggingface.co/deepseek-ai/DeepSeek-V2.5) Evaluation Dataset: =================== We evaluated NV-EmbedCode model using theCoIR benchmarkand a curated set based onSWE-bench. CoIR consists of 10 code datasets across four retrieval tasks: (1) Text-to-Code Retrieval, (2) Code-to-Code Retrieval, (3) Code-to-Text Retrieval, and (4) Hybrid Code Retrieval. The default evaluation metric for CoIR is average NDCG@10 across all datasets. SWE-bench originally consists of real-world software engineering problems from GitHub issues and their corresponding pull requests. We adapted it into a retrieval task, where the goal is to identify the files that need to be edited to resolve an issue. These files are identified using the pull request that solved the issue. For SWE-bench Lite, we use Recall@1 to measure whether the top retrieved file is the correct one for resolving the issue, as each instance typically involves editing just one file. CoIR benchmark (https://arxiv.org/html/2407.02883v1) SWE-bench (https://arxiv.org/abs/2310.06770) Technical Details ================= Software Integration -------------------- Runtime:NeMo Retriever Text Embedding NIMSupported Hardware Microarchitecture Compatibility:NVIDIA Ampere, NVIDIA Hopper, NVIDIA LovelaceSupported Operating System(s):LinuxEngine:TensorRTTest Hardware:See Support Matrix fromNIM documentation. Runtime: Supported Hardware Microarchitecture Compatibility: Supported Operating System(s): Engine: TensorRT (https://developer.nvidia.com/tensorrt-getting-started) Test Hardware: NIM documentation (https://docs.nvidia.com/nim/nemo-retriever/text-embedding/latest/overview.html) Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++Explainability,Bias,Safety & Security, andPrivacySubcards. Explainability (./explainability.md) Bias (./bias.md) Safety & Security (./safety.md) Privacy (./privacy.md) Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "magpie-tts-multilingual",
    "nvidia_url": "https://build.nvidia.com/nvidia/magpie-tts-multilingual",
    "scraped_on": "2025-10-15T21:40:17.583256",
    "tags": [
      "multilingual",
      "nvidia",
      "nvidia nim",
      "nvidia riva",
      "text-to-speech",
      "tts"
    ],
    "model_card": "Overview Speech Synthesis: Magpie TTS Multilingual Model Overview ======================================================== Description: ============ The Magpie TTS Multilingual model converts text into audio (speech). Magpie TTS is a generative model, designed to be used as the first part of a neural text-to-speech system in conjunction with an audio codec model. This model uses the International Phonetic Alphabet (IPA) for inference and training, and it can output a female or a male voice for English-US and European-Spanish. In addition, it uses character-based encoding for French. Audio Codec is a neural codec model for speech applications. It is the second part of a two-stage speech synthesis pipeline. This model is ready for commercial use. License/Terms of Use: --------------------- GOVERNING TERMS: Use of this model is governed by theNVIDIA Community Model License Agreement. NVIDIA Community Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) References: =========== TTS model papers:Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free GuidanceImproving Robustness of LLM-based Speech Synthesis by Learning Monotonic AlignmentLow Frame-rate Speech Codec: A Codec Designed for Fast High-quality Speech LLM Training and Inference Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance (https://arxiv.org/pdf/2502.05236) Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment (https://arxiv.org/pdf/2406.17957) Low Frame-rate Speech Codec: A Codec Designed for Fast High-quality Speech LLM Training and Inference (https://arxiv.org/abs/2409.12117) Model Architecture: =================== Network Architecture: T5-TTS + Audio codec T5-TTS is an encoder-decoder transformer model for text-to-speech synthesis that improves robustness by learning monotonic alignment between text and speech tokens. The model takes text tokens and reference audio codes as input and autoregressively predicts acoustic tokens of the target speech. Low Frame Rate Speech Codec - 21Hz is a neural audio compression model that quantizes speech or audio signals into discrete tokens at a low temporal rate of 21 frames per second. The model typically employs a multi-stage encoding process to compress the input audio into a sequence of discrete codes while preserving essential acoustic characteristics despite the aggressive temporal compression. During encoding, it analyzes longer windows of audio to capture relevant acoustic features before downsampling to 21Hz, and during decoding, it uses neural upsampling techniques to reconstruct high-fidelity audio at the original sampling rate. This lower frame rate allows for efficient storage and transmission while still maintaining reasonable audio quality for applications like speech synthesis and audio compression. Input: ====== Input Type:TextInput Format:Strings (Graphemes in US English)Input Parameters:One-Dimensional (1D) Input Type: Input Format: Input Parameters: Output: ======= Output Type:AudioOutput Format:Audio of shape (batch x time) in wav formatOutput Parameters:Mono, PCM-encoded 16 bit audio; sampling rate of 22.05 kHz; 20 Second Maximum Length; Depending on input, this model can output a female or a male voice for English US with two (2) emotions for the female voice and six (6) emotions for male voices. The female voice is classified as “neutral” and “calm.” The male voice is classified as “neutral,” “calm,” “happy,” and “fearful”, “sad”, and “angry.” Output Type: Output Format: Output Parameters: Software Integration: ===================== Runtime Engine(s):Riva 2.19.0 or greater Runtime Engine(s): Supported Hardware Platform(s): • NVIDIA A30 • NVIDIA A100 • NVIDIA H100 • NVIDIA A2 • NVIDIA A10 • NVIDIA A16 • NVIDIA A40 • NVIDIA L4 • NVIDIA L40 • GeForce RTX 40xx • GeForce RTX 50xx • Blackwell RTX 60xx Supported Operating System(s): • Linux Inference ========= Engine:TritonTest Hardware: Engine: Test Hardware: • NVIDIA A30 • NVIDIA A100 • NVIDIA H100 • NVIDIA A2 • NVIDIA A10 • NVIDIA A16 • NVIDIA A40 • NVIDIA L4 • NVIDIA L40 • GeForce RTX 40xx • GeForce RTX 50xx • Blackwell RTX 60xx Model Version(s): ================= magpie-tts-multilingual v1 Ethical Considerations (For NVIDIA Models Only): ================================================ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) GOVERNING TERMS: ================ This trial is governed by theNVIDIA API Trial Terms of Service. The use of this model is governed by theAI Foundation Models Community License Agreement NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) AI Foundation Models Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)"
  },
  {
    "name": "llama-3.1-nemotron-nano-8b-v1",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1",
    "scraped_on": "2025-10-15T21:40:14.467600",
    "tags": [
      "advanced reasoning",
      "function calling",
      "instruction following",
      "math",
      "nvidia"
    ],
    "model_card": "Llama-3.1-Nemotron-Nano-8B-v1 ============================= Model Overview ============== Llama-3.1-Nemotron-Nano-8B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-8B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling. Llama-3.1-Nemotron-Nano-8B-v1 is a model which offers a great tradeoff between model accuracy and efficiency. It is created from Llama 3.1 8B Instruct and offers improvements in model accuracy. The model fits on a single RTX GPU and can be used locally. The model supports a context length of 128K. This model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, and Tool Calling as well as multiple reinforcement learning (RL) stages using REINFORCE (RLOO) and Online Reward-aware Preference Optimization (RPO) algorithms for both chat and instruction-following. The final model checkpoint is obtained after merging the final SFT and Online RPO checkpoints. This model is part of the Llama Nemotron Collection. You can find the other model(s) in this family here:Llama-3_3-Nemotron-Super-49B-v1 Llama-3_3-Nemotron-Super-49B-v1 (https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1) This model is ready for commercial use. License/Terms of Use ==================== GOVERNING TERMS: Your use of this model is governed by theNVIDIA Open Model License. Additional Information:Llama 3.1 Community License Agreement. Built with Llama. NVIDIA Open Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Llama 3.1 Community License Agreement (https://www.llama.com/llama3_1/license/) Model Developer:NVIDIA Model Developer: Model Dates:Trained between August 2024 and March 2025 Model Dates: Data Freshness:The pretraining data has a cutoff of 2023 per Meta Llama 3.1 8B Data Freshness: Use Case: ========= Developers designing AI Agent systems, chatbots, RAG systems, and other AI-powered applications. Also suitable for typical instruction-following tasks. Balance of model accuracy and compute efficiency (the model fits on a single RTX GPU and can be used locally). Release Date: ============= 3/18/2025 References ========== • [2502.00203]Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment (https://arxiv.org/abs/2502.00203) Model Architecture ================== Architecture Type:Dense decoder-only Transformer modelNetwork Architecture:Llama 3.1 8B Instruct Architecture Type: Network Architecture: Intended use ============ Llama-3.1-Nemotron-Nano-8B-v1 is a general purpose reasoning and chat model intended to be used in English and coding languages. Other non-English languages (German, French, Italian, Portuguese, Hindi, Spanish, and Thai) are also supported. Input: ====== • Input Type:Text Input Type: • Input Format:String Input Format: • Input Parameters:One-Dimensional (1D) Input Parameters: • Other Properties Related to Input:Context length up to 131,072 tokens Other Properties Related to Input: Output: ======= • Output Type:Text Output Type: • Output Format:String Output Format: • Output Parameters:One-Dimensional (1D) Output Parameters: • Other Properties Related to Output:Context length up to 131,072 tokens Other Properties Related to Output: Model Version: ============== 1.0 (3/18/2025) Software Integration ==================== • Runtime Engine:NeMo 24.12 Runtime Engine: • Recommended Hardware Microarchitecture Compatibility:NVIDIA HopperNVIDIA Ampere Recommended Hardware Microarchitecture Compatibility: • NVIDIA Hopper • NVIDIA Ampere Quick Start and Usage Recommendations: ====================================== • Reasoning mode (ON/OFF) is controlled via the system prompt, which must be set as shown in the example below. All instructions should be contained within the user prompt • We recommend setting temperature to0.6, and Top P to0.95for Reasoning ON mode • We recommend using greedy decoding for Reasoning OFF mode • We have provided a list of prompts to use for evaluation for each benchmark where a specific template is required • The model will include<think></think>if no reasoning was necessary in Reasoning ON model, this is expected behaviour You can try this model out through the preview API, using this link:Llama-3_1-Nemotron-Nano-8B-v1. Llama-3_1-Nemotron-Nano-8B-v1 (https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1) Inference: ========== Engine:TransformersTest Hardware: Engine: Test Hardware: • BF16:1x RTX 50 Series GPUs1x RTX 40 Series GPUs1x RTX 30 Series GPUs1x H100-80GB GPU1x A100-80GB GPU • 1x RTX 50 Series GPUs • 1x RTX 40 Series GPUs • 1x RTX 30 Series GPUs • 1x H100-80GB GPU • 1x A100-80GB GPU Preferred/Supported] Operating System(s):Linux Preferred/Supported] Operating System(s): Training Datasets ================= A large variety of training data was used for the post-training pipeline, including manually annotated data and synthetic data. The data for the multi-stage post-training phases for improvements in Code, Math, and Reasoning is a compilation of SFT and RL data that supports improvements of math, code, general reasoning, and instruction following capabilities of the original Llama instruct model. Prompts have been sourced from either public and open corpus or synthetically generated. Responses were synthetically generated by a variety of models, with some prompts containing responses for both Reasoning On and Off modes, to train the model to distinguish between two modes. Data Collection for Training Datasets:Hybrid: Automated, Human, Synthetic Data Collection for Training Datasets: Data Labeling for Training Datasets:N/A Data Labeling for Training Datasets: Evaluation Datasets =================== We used the datasets listed below to evaluate Llama-3.1-Nemotron-Nano-8B-v1. Data Collection for Evaluation Datasets:Hybrid: Human/Synthetic Data Collection for Evaluation Datasets: Data Labeling for Evaluation Datasets:Hybrid: Human/Synthetic/Automatic Data Labeling for Evaluation Datasets: Evaluation Results ================== These results contain both “Reasoning On”, and “Reasoning Off”. We recommend using temperature=0.6, top_p=0.95for “Reasoning On” mode, and greedy decoding for “Reasoning Off” mode. All evaluations are done with 32k sequence length. We run the benchmarks up to 16 times and average the scores to be more accurate. NOTE: Where applicable, a Prompt Template will be provided. While completing benchmarks, please ensure that you are parsing for the correct output format as per the provided prompt in order to reproduce the benchmarks seen below. MT-Bench -------- MATH500 ------- User Prompt Template: AIME25 ------ User Prompt Template: GPQA-D ------ User Prompt Template: IFEval Average -------------- BFCL v2 Live ------------ User Prompt Template: MBPP 0-shot ----------- User Prompt Template: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards, which you can find by clicking the ModelCard++ tab above, next to Overview. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.3-nemotron-super-49b-v1",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1",
    "scraped_on": "2025-10-15T21:40:11.336044",
    "tags": [
      "advanced reasoning",
      "chat",
      "function calling",
      "instruction following",
      "math",
      "nvidia"
    ],
    "model_card": "Llama-3.3-Nemotron-Super-49B-v1 =============================== Model Overview ============== Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.3-70B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling. The model supports a context length of 128K tokens. Llama-3.3-Nemotron-Super-49B-v1 is a model which offers a great tradeoff between model accuracy and efficiency. Efficiency (throughput) directly translates to savings. Using a novel Neural Architecture Search (NAS) approach, we greatly reduce the model’s memory footprint, enabling larger workloads, as well as fitting the model on a single GPU at high workloads (H200). This NAS approach enables the selection of a desired point in the accuracy-efficiency tradeoff. The model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, and Tool Calling as well as multiple reinforcement learning (RL) stages using REINFORCE (RLOO) and Online Reward-aware Preference Optimization (RPO) algorithms for both chat and instruction-following. The final model checkpoint is obtained after merging the final SFT and Online RPO checkpoints. For more details on how the model was trained, please seethis blog. this blog (https://developer.nvidia.com/blog/build-enterprise-ai-agents-with-advanced-open-nvidia-llama-nemotron-reasoning-models/) This model is part of the Llama Nemotron Collection. You can find the other model(s) in this family here:Llama-3_1-Nemotron-Nano-8B-v1 Llama-3_1-Nemotron-Nano-8B-v1 (https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1) This model is ready for commercial use. License/Terms of Use ==================== GOVERNING TERMS: Your use of this model is governed by theNVIDIA Open Model License.Additional Information:Llama 3.3 Community License Agreement. Built with Llama. NVIDIA Open Model License. (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Llama 3.3 Community License Agreement (https://www.llama.com/llama3_3/license/) Model Developer:NVIDIA Model Developer: Model Dates:Trained between November 2024 and February 2025 Model Dates: Data Freshness:The pretraining data has a cutoff of 2023 per Meta Llama 3.3 70B Data Freshness: Use Case: --------- Developers designing AI Agent systems, chatbots, RAG systems, and other AI-powered applications. Also suitable for typical instruction-following tasks. Release Date: ------------- 3/18/2025 References ========== • [2502.00203]Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment (https://arxiv.org/abs/2502.00203) Model Architecture ================== Architecture Type:Dense decoder-only Transformer modelNetwork Architecture:Llama 3.3 70B Instruct, customized through Neural Architecture Search (NAS) Architecture Type: Network Architecture: The model is a derivative of Meta’s Llama-3.3-70B-Instruct, using Neural Architecture Search (NAS). The NAS algorithm results in non-standard and non-repetitive blocks. This includes the following: Skip attention: In some blocks, the attention is skipped entirely, or replaced with a single linear layer. Variable FFN: The expansion/compression ratio in the FFN layer is different between blocks. We utilize a block-wise distillation of the reference model, where for each block we create multiple variants providing different tradeoffs of quality vs. computational complexity, discussed in more depth below. We then search over the blocks to create a model which meets the required throughput and memory (optimized for a single H100-80GB GPU) while minimizing the quality degradation. The model then undergoes knowledge distillation (KD), with a focus on English single and multi-turn chat use-cases. The KD step included 40 billion tokens consisting of a mixture of 3 datasets - FineWeb, Buzz-V1.2 and Dolma. Intended use ============ Llama-3.3-Nemotron-Super-49B-v1 is a general purpose reasoning and chat model intended to be used in English and coding languages. Other non-English languages (German, French, Italian, Portuguese, Hindi, Spanish, and Thai) are also supported. Input ===== • Input Type:Text Input Type: • Input Format:String Input Format: • Input Parameters:One-Dimensional (1D) Input Parameters: • Other Properties Related to Input:Context length up to 131,072 tokens Other Properties Related to Input: Output ====== • Output Type:Text Output Type: • Output Format:String Output Format: • Output Parameters:One-Dimensional (1D) Output Parameters: • Other Properties Related to Output:Context length up to 131,072 tokens Other Properties Related to Output: Model Version ============= 1.0 (3/18/2025) Software Integration ==================== • Runtime Engine:Transformers Runtime Engine: • Recommended Hardware Microarchitecture Compatibility:NVIDIA HopperNVIDIA Ampere Recommended Hardware Microarchitecture Compatibility: • NVIDIA Hopper • NVIDIA Ampere Quick Start and Usage Recommendations: ====================================== • Reasoning mode (ON/OFF) is controlled via the system prompt, which must be set as shown in the example below. All instructions should be contained within the user prompt • We recommend setting temperature to0.6, and Top P to0.95for Reasoning ON mode • We recommend using greedy decoding for Reasoning OFF mode • We have provided a list of prompts to use for evaluation for each benchmark where a specific template is required • The model will include<think></think>if no reasoning was necessary in Reasoning ON model, this is expected behaviour You can try this model out through the preview API, using this link:Llama-3_3-Nemotron-Super-49B-v1. Llama-3_3-Nemotron-Super-49B-v1 (https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1) Inference: ========== Engine:TransformersTest Hardware: Engine: Test Hardware: • FP8: 1x NVIDIA H100-80GB GPU • BF16:2x NVIDIA H100-80GB GPUs2x NVIDIA A100-80GB GPUs • 2x NVIDIA H100-80GB GPUs • 2x NVIDIA A100-80GB GPUs [Preferred/Supported] Operating System(s):Linux [Preferred/Supported] Operating System(s): Training Datasets ================= A large variety of training data was used for the knowledge distillation phase before post-training pipeline, 3 of which included: FineWeb, Buzz-V1.2, and Dolma. The data for the multi-stage post-training phases for improvements in Code, Math, and Reasoning is a compilation of SFT and RL data that supports improvements of math, code, general reasoning, and instruction following capabilities of the original Llama instruct model. In conjunction with this model release, NVIDIA has released 30M samples of post-training data, as public and permissive.Llama-Nemotron-Post-Training-Dataset-v1 Llama-Nemotron-Post-Training-Dataset-v1 (https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset-v1) Distribution of the domains is as follows: Prompts have been sourced from either public and open corpus or synthetically generated. Responses were synthetically generated by a variety of models, with some prompts containing responses for both reasoning on and off modes, to train the model to distinguish between two modes. Models that were used in the creation of this dataset: • Llama-3.3-70B-Instruct • Llama-3.1-Nemotron-70B-Instruct • Llama-3.3-Nemotron-70B-Feedback/Edit/Select • Mixtral-8x22B-Instruct-v0.1 • DeepSeek-R1 • Qwen-2.5-Math-7B-Instruct • Qwen-2.5-Coder-32B-Instruct • Qwen-2.5-72B-Instruct • Qwen-2.5-32B-Instruct Data Collection for Training Datasets:Hybrid: Automated, Human, Synthetic Data Collection for Training Datasets: Data Labeling for Training Datasets:Hybrid: Automated, Human, Synthetic Data Labeling for Training Datasets: Evaluation Datasets =================== We used the datasets listed below to evaluate Llama-3.3-Nemotron-Super-49B-v1. Data Collection for Evaluation Datasets:Hybrid: Human/Synthetic Data Collection for Evaluation Datasets: Data Labeling for Evaluation Datasets:Hybrid: Human/Synthetic/Automatic Data Labeling for Evaluation Datasets: Evaluation Results ================== These results contain both Reasoning On, and Reasoning Off. We recommend using temperature=0.6, top_p=0.95for Reasoning On mode, and greedy decoding for Reasoning Off mode. All evaluations are done with 32k sequence length. We run the benchmarks up to 16 times and average the scores to be more accurate. NOTE: Where applicable, a Prompt Template will be provided. While completing benchmarks, please ensure that you are parsing for the correct output format as per the provided prompt in order to reproduce the benchmarks seen below. Arena-Hard ---------- MATH500 ------- User Prompt Template: AIME25 ------ User Prompt Template: GPQA ---- User Prompt Template: IFEval ------ BFCL V2 Live ------------ User Prompt Template: MBPP 0-shot ----------- User Prompt Template: MT-Bench -------- Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards, which you can find by clicking the ModelCard++ tab above, next to Overview. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "bevformer",
    "nvidia_url": "https://build.nvidia.com/nvidia/bevformer",
    "scraped_on": "2025-10-15T21:40:11.008413",
    "tags": [
      "automotive",
      "autonomous vehicles",
      "bev",
      "nvidia",
      "perception"
    ],
    "model_card": "BEVFormer Model Overview ======================== Description =========== BEVFormer is a transformer-based model that combines multi-frame camera data into a unified bird's-eye-view (BEV) representation for 3D perception. BEVFormer learns these representations using spatiotemporal transformers. This approach allows the system to exploit both spatial and temporal information by interacting with spatial and temporal spaces through predefined grid-shaped BEV queries. This NIM previews an example of deployingBEVFormerwith explicit quantization withNVIDIA's ModelOpt Toolkit. BEVFormer (https://arxiv.org/abs/2203.17270) NVIDIA's ModelOpt Toolkit (https://github.com/NVIDIA/TensorRT-Model-Optimizer) This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. It has been developed and built to a third-party's requirements for this application and use case; see link toBEVFormer. BEVFormer (https://github.com/fundamentalvision/BEVFormer) License ======= GOVERNING TERMS:The trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:Apache 2.0. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache 2.0 (https://github.com/DerryHub/BEVFormer_tensorrt/blob/main/LICENCE) Deployment Geography ==================== Global Use Case ======== BEVFormer is most suitable for Physical AI developers, especially ADAS and AV developers working on Perception tasks. Release Date ============ 03/18/2025 viahttps://build.nvidia.com/nvidia/bevformer https://build.nvidia.com/nvidia/bevformer Model Architecture ================== • Architecture Type:CNN + Transformer Architecture Type: • Network Architecture:ResNet-101 Backbone + BEVFormer Encoder (six layers with BEV queries, spatial cross-attention, and temporal self-attention) + Dual Heads (3D detection and map segmentation) Network Architecture: Input ===== • Input Types:Multi-view Camera images captured from various angles around the vehicle, Vehicle State Data, Camera extrinsics Input Types: • Input Formats:Images (Red, Green, Blue), Vehicle State Data (json), and Camera extrinsics (json) Input Formats: • Input Parameters:RGB images (2D), Vehicle State Data (1D), and Camera extrinsics (2D) Input Parameters: • Other Properties Related to Input:Image Resolution: 450*800Pre-Processing Requirements: Normalization, Data AugmentationVehicle State Data: Position, Orientation, Velocity, Rotation rateSensor Calibration Data: Camera Intrinsics, Camera Extrinsics Other Properties Related to Input: • Image Resolution: 450*800 • Pre-Processing Requirements: Normalization, Data Augmentation • Vehicle State Data: Position, Orientation, Velocity, Rotation rate • Sensor Calibration Data: Camera Intrinsics, Camera Extrinsics Output ====== • Output Types:Labels Output Types: • Output Formats:Labels: Text String Output Formats: • Output Parameters:1D Output Parameters: • Other Properties Related to Output:Category Label(s) and Bounding boxes of the perceived objects around the vehicle.Object Classes: Vehicles, Pedestrians, Cyclists, Traffic cones, BarriersConfidence Score: Each detected object and trajectory prediction includes confidence level Other Properties Related to Output: • Object Classes: Vehicles, Pedestrians, Cyclists, Traffic cones, Barriers • Confidence Score: Each detected object and trajectory prediction includes confidence level Software Integration ==================== • Runtime Engine(s):TensorRT Runtime Engine(s): • Hardware Compatibility:NVIDIA AmpereNVIDIA Ada Lovelace Hardware Compatibility: • NVIDIA Ampere • NVIDIA Ada Lovelace • Operating System(s):Docker image OS: Ubuntu 22.04.5 LTS (Jammy Jellyfish) Operating System(s): • Docker image OS: Ubuntu 22.04.5 LTS (Jammy Jellyfish) Model Version(s) ================ • Model Name:bevformer_trt_model_int8 Model Name: • Tag/Version:0.1.2 Tag/Version: Training, Testing, and Evaluation Datasets ========================================== Overview -------- nuScenesdataset was used for training, testing and evaluation (see details below). nuScenes (https://www.nuscenes.org/nuscenes) The nuScenes dataset (pronounced /nuːsiːnz/) is a public large-scale dataset for autonomous driving developed by the team atMotional(formerly nuTonomy). Motional is making driverless vehicles a safe, reliable, and accessible reality. Motional (https://www.motional.com/) Data Collection Method: Human ----------------------------- nuScenesdataset collects approximately 15h of driving data in Boston and Singapore. Driving routes are carefully chosen to capture challenging scenarios. nuScenes aims for a diverse set of locations, times and weather conditions. To balance the class frequency distribution, nuScenes includes more scenes with rare classes (such as bicycles). Using these criteria, data was manually selected to include 1000 scenes of 20s duration each. These scenes are carefully annotated using human experts. nuScenes (https://www.nuscenes.org/nuscenes) Labeling Method: Human ---------------------- Annotation partnerScaleis used for annotation. All objects in the nuScenes dataset come with a semantic category, as well as a 3D bounding box and attributes for each frame they occur in. Ground truth labels for 23 object classes are provided. Scale (https://scaleapi.com/) Inference ========= • Engine:TensorRT Engine: • Test Hardware:A30A6000L40L40SL40G Test Hardware: • A30 • A6000 • L40 • L40S • L40G Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "sparsedrive",
    "nvidia_url": "https://build.nvidia.com/nvidia/sparsedrive",
    "scraped_on": "2025-10-15T21:40:10.629564",
    "tags": [
      "automotive",
      "autonomous vehicles",
      "av stack",
      "bev",
      "nvidia"
    ],
    "model_card": "SparseDrive Model Overview ========================== Description =========== SparseDrive is an end-to-end autonomous driving model that performs motion prediction and planning simultaneously, outputting a safe planning trajectory. It first encodes multi-view images into feature maps, then learns sparse scene representation through symmetric sparse perception, and finally performs motion prediction and planning in a parallel manner. This NIM previews an end-to-end example of deployingSparseDrivebased on thepaperwith explicit quantization withNVIDIA's ModelOpt Toolkit. SparseDrive (https://github.com/swc-17/SparseDrive) paper (https://arxiv.org/abs/2405.19620) NVIDIA's ModelOpt Toolkit (https://github.com/NVIDIA/TensorRT-Model-Optimizer) This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. It has been developed and built to a third-party’s requirements for this application and use case; see link toSparseDrive. SparseDrive (https://github.com/swc-17/SparseDrive) License ======= GOVERNING TERMS:The trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:MIT License. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) MIT License (https://github.com/swc-17/SparseDrive/blob/main/LICENSE) Deployment Geography: ===================== Global Use Case ======== Researchers and developers in the field of autonomous driving and motion forecasting, specifically those working with 3D object detection and tracking, would be expected to use this system for tasks such as object detection, tracking, motion prediction, and planning. Release Date ============ 03/18/2025 viahttps://build.nvidia.com/nvidia/sparsedrive https://build.nvidia.com/nvidia/sparsedrive Model Architecture ================== • Architecture Type:CNN backbone + Multiple Transformers Architecture Type: • Network Architecture:SparseDrive uses a Resnet50 backbone for image feature extraction. The multi-head transformer-based architecture consists of:Sparse Perception Transformers: Symmetric Object Detection and Tracking and Online Mapping submodulesMotion Planning Transformer: Parallel Motion Planner for multi-modal trajectory planning and selection Network Architecture: • Sparse Perception Transformers: Symmetric Object Detection and Tracking and Online Mapping submodules • Motion Planning Transformer: Parallel Motion Planner for multi-modal trajectory planning and selection Input/Output Specifications =========================== Input ----- • Input Type(s):Multi-view camera images, cameras’ intrinsics and extrinsics, Ego vehicle state Input Type(s): • Input Format(s):Python Dictionary/JSON Input Format(s): • Input Parameters:6x stacked 2D RGB images, 6x 2D camera projection (4x4) matrix, 6x 2D camera extrinsics (4x4) matrix, 1D (length 12) ego vehicle state vector Input Parameters: • Other Properties Related to Input:Model performs best given a sequence of images from the same “scene”, e.g. a 20 second scene from NuScenes datasetImage Resolution: 256 x 704 (HxW)Pre-Processing Requirements: NoneSensor Calibration Data: Camera Intrinsics, Camera ExtrinsicsEgo Motion Data: Position, Orientation, Velocity Other Properties Related to Input: • Model performs best given a sequence of images from the same “scene”, e.g. a 20 second scene from NuScenes dataset • Image Resolution: 256 x 704 (HxW) • Pre-Processing Requirements: None • Sensor Calibration Data: Camera Intrinsics, Camera Extrinsics • Ego Motion Data: Position, Orientation, Velocity Output ------ • Output Type(s):Ordered arrays of labeled bounding boxes and trajectory predictions Output Type(s): • Output Format:Python Dictionary/JSON Output Format: • Output Parameters:Array of detected 3D object labels (NuScenes dynamic object classes)Array of 3D Bounding Boxes per 3D object instance (each a vector of length 7: translation, size, yaw angle)Array of 2D Predicted Trajectories per 3D object instanceArray of Predicted 2D map elements and labels (NuScenes static object classes, road boundaries)Predicted planned 2D trajectory for ego vehicle Output Parameters: • Array of detected 3D object labels (NuScenes dynamic object classes) • Array of 3D Bounding Boxes per 3D object instance (each a vector of length 7: translation, size, yaw angle) • Array of 2D Predicted Trajectories per 3D object instance • Array of Predicted 2D map elements and labels (NuScenes static object classes, road boundaries) • Predicted planned 2D trajectory for ego vehicle • Other Properties Related to Output:Confidence Score: Each detected object and trajectory prediction includes confidence levelMotion prediction for next 6 sec of scene @2Hz Other Properties Related to Output: • Confidence Score: Each detected object and trajectory prediction includes confidence level • Motion prediction for next 6 sec of scene @2Hz Software Integration ==================== • Runtime Engine(s): TensorRT • Supported Hardware Compatibility:NVIDIA AmpereNVIDIA Ada Lovelace • NVIDIA Ampere • NVIDIA Ada Lovelace • Operating System(s):Docker OS: 24.04.1 LTS (Noble Numbat) • Docker OS: 24.04.1 LTS (Noble Numbat) Model Version(s) ================ • Model Name:sparsedrive_trt_model_fp16 Model Name: • Tag/Version: 0.1.1 Tag/Version Training, Testing, and Evaluation Datasets ========================================== Overview -------- nuScenesdataset was used for training, testing and evaluation (see details below). nuScenes (https://www.nuscenes.org/nuscenes) The nuScenes dataset (pronounced /nuːsiːnz/) is a public large-scale dataset for autonomous driving developed by the team atMotional(formerly nuTonomy). Motional is making driverless vehicles a safe, reliable, and accessible reality. Motional (https://www.motional.com/) Data Collection Method: Human ----------------------------- nuScenesdataset collects approximately 15h of driving data in Boston and Singapore. Driving routes are carefully chosen to capture challenging scenarios. nuScenes aims for a diverse set of locations, times and weather conditions. To balance the class frequency distribution, nuScenes includes more scenes with rare classes (such as bicycles). Using these criteria, data was manually selected to include 1000 scenes of 20s duration each. These scenes are carefully annotated using human experts. nuScenes (https://www.nuscenes.org/nuscenes) Labeling Method: Human ---------------------- Annotation partnerScaleis used for annotation. All objects in the nuScenes dataset come with a semantic category, as well as a 3D bounding box and attributes for each frame they occur in. Ground truth labels for 23 object classes are provided. Scale (https://scaleapi.com/) Inference ========= Engine: Tensor(RT)Test Hardware: • A6000 • L40S Ethical Considerations ====================== Ethical considerations and guidelines. NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "cosmos-predict1-5b",
    "nvidia_url": "https://build.nvidia.com/nvidia/cosmos-predict1-5b",
    "scraped_on": "2025-10-15T21:40:07.397944",
    "tags": [
      "nvidia",
      "physical ai",
      "policy evaluation",
      "robotics",
      "synthetic data generation",
      "video-to-world"
    ],
    "model_card": "Cosmos-Predict1: A Suite of Autoregressive-based World Foundation Models ======================================================================== Cosmos-Predict1 Cosmos|Code|Paper Cosmos (https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6) Cosmos Code (https://github.com/NVIDIA/Cosmos) Code Paper (https://research.nvidia.com/publication/2025-01_cosmos-world-foundation-model-platform-physical-ai) Paper Model Overview ============== Description: ============ Cosmos World Foundation Models: A family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical AI development. Cosmos World Foundation Models The Cosmos autoregressive models are a collection of pre-trained world foundation models that are ideal for predicting and rapidly generating video sequences from video or image inputs for physical AI. They can serve as the building block for various applications or research that are related to world generation. The models are ready for commercial use under NVIDIA Open Model license agreement. Model Developer: NVIDIA Model Developer Model Versions ============== In Cosmos 1.0 release, the Cosmos Autoregressive WFM family includes the following models: • Cosmos-Predict1-4BGiven a 9-frame input video, predicts the future 24 frames.Given an image as the first frame, predicts the future 32 frames. Cosmos-Predict1-4B (https://huggingface.co/nvidia/Cosmos-Predict1-4B) • Given a 9-frame input video, predicts the future 24 frames. • Given an image as the first frame, predicts the future 32 frames. • Cosmos-Predict1-5B-Video2WorldGiven text description and a 9-frame input video, predicts the future 24 frames.Given text description and an image as the first frame, predicts the future 32 frames. Cosmos-Predict1-5B-Video2World (https://huggingface.co/nvidia/Cosmos-Predict1-5B-Video2World) • Given text description and a 9-frame input video, predicts the future 24 frames. • Given text description and an image as the first frame, predicts the future 32 frames. • Cosmos-Predict1-12BGiven a 9-frame input video, predicts the future 24 frames.Given an image as the first frame, predicts the future 32 frames. Cosmos-Predict1-12B (https://huggingface.co/nvidia/Cosmos-Predict1-12B) • Given a 9-frame input video, predicts the future 24 frames. • Given an image as the first frame, predicts the future 32 frames. • Cosmos-Predict1-13B-Video2WorldGiven text description and a 9-frame input video, predicts the future 24 frames.Given text description and an image as the first frame, predicts the future 32 frames. Cosmos-Predict1-13B-Video2World (https://huggingface.co/nvidia/Cosmos-Predict1-13B-Video2World) • Given text description and a 9-frame input video, predicts the future 24 frames. • Given text description and an image as the first frame, predicts the future 32 frames. License: -------- This model is released under theNVIDIA Open Model License. For a custom license, please contactcosmos-license@nvidia.com. NVIDIA Open Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license) cosmos-license@nvidia.com (mailto:cosmos-license@nvidia.com) Under the NVIDIA Open Model License, NVIDIA confirms: • Models are commercially usable. • You are free to create and distribute Derivative Models. • NVIDIA does not claim ownership to any outputs generated using the Models or Derivative Models. Important Note: If you bypass, disable, reduce the efficacy of, or circumvent any technical limitation,safety guardrailor associated safety guardrail hyperparameter, encryption, security, digital rights management, or authentication mechanism contained in the Model, your rights underNVIDIA Open Model License Agreementwill automatically terminate. Important Note safety guardrail NVIDIA Open Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license) • Cosmos-1.0-Guardrailis the safety guardrail for this model. Cosmos-1.0-Guardrail (https://huggingface.co/nvidia/Cosmos-1.0-Guardrail) Model Architecture: =================== Cosmos-Predict1-5B-Video2World is an autoregressive transformer model designed for world generation. The network is composed of interleaved self-attention, cross-attention, and feedforward layers as its building blocks. The cross-attention layers allow the model to condition on input text throughout the decoding process. Input/Output Specifications =========================== • InputInput Type(s): Text+Image, Text+VideoInput Format(s):Text: StringImage: jpg, png, jpeg, webpVideo: mp4Input Parameters:Text: One-dimensional (1D)Image: Two-dimensional (2D)Video: Three-dimensional (3D)Other Properties Related to Input:The input string should contain fewer than 300 words and should provide descriptive content for world generation, such as a scene description, key objects or characters, background, and any specific actions or motions to be depicted within the 1-second duration.The input image and video should be of 1024x640 resolution. Input • Input Type(s): Text+Image, Text+Video Input Type(s) • Input Format(s):Text: StringImage: jpg, png, jpeg, webpVideo: mp4 Input Format(s) • Text: String • Image: jpg, png, jpeg, webp • Video: mp4 • Input Parameters:Text: One-dimensional (1D)Image: Two-dimensional (2D)Video: Three-dimensional (3D) Input Parameters • Text: One-dimensional (1D) • Image: Two-dimensional (2D) • Video: Three-dimensional (3D) • Other Properties Related to Input:The input string should contain fewer than 300 words and should provide descriptive content for world generation, such as a scene description, key objects or characters, background, and any specific actions or motions to be depicted within the 1-second duration.The input image and video should be of 1024x640 resolution. Other Properties Related to Input • The input string should contain fewer than 300 words and should provide descriptive content for world generation, such as a scene description, key objects or characters, background, and any specific actions or motions to be depicted within the 1-second duration. • The input image and video should be of 1024x640 resolution. • OutputOutput Type(s): VideoOutput Format(s): mp4Output Parameters: Three-dimensional (3D)Other Properties Related to Output:For text+image input, the generated video will be a 32-frame clip with a resolution of 1024x640 pixels, conditioned on the input image as the first video frame.For text+video input, the generated video will be a 24-frame clip with a resolution of 1024x640 pixels, conditioned on the first 9 frames of the input video.The content of the video will visualize the input text description as a short animated scene, capturing the main elements mentioned in the input. Output • Output Type(s): Video Output Type(s) • Output Format(s): mp4 Output Format(s) • Output Parameters: Three-dimensional (3D) Output Parameters • Other Properties Related to Output:For text+image input, the generated video will be a 32-frame clip with a resolution of 1024x640 pixels, conditioned on the input image as the first video frame.For text+video input, the generated video will be a 24-frame clip with a resolution of 1024x640 pixels, conditioned on the first 9 frames of the input video.The content of the video will visualize the input text description as a short animated scene, capturing the main elements mentioned in the input. Other Properties Related to Output • For text+image input, the generated video will be a 32-frame clip with a resolution of 1024x640 pixels, conditioned on the input image as the first video frame. • For text+video input, the generated video will be a 24-frame clip with a resolution of 1024x640 pixels, conditioned on the first 9 frames of the input video. • The content of the video will visualize the input text description as a short animated scene, capturing the main elements mentioned in the input. Software Integration: ===================== Runtime Engine(s): • Cosmos Cosmos (https://github.com/NVIDIA/Cosmos) Supported Hardware Microarchitecture Compatibility: • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Ampere Note: We have only tested doing inference with BF16 precision. Note Operating System(s): • Linux (We have not tested on other operating systems.) Usage ===== • SeeCosmosfor details. Cosmos (https://github.com/NVIDIA/Cosmos) Evaluation ========== Please see ourtechnical paperfor detailed evaluations. technical paper (https://research.nvidia.com/publication/2025-01_cosmos-world-foundation-model-platform-physical-ai) Inference Time and GPU Memory Usage =================================== These numbers may vary based on system specifications and are provided for reference only. End-to-end inference runtime on one H100 with no offloading for 5B model and guardrail offloading for 13B, after model initialization: Failure Analysis ================ Our models now support video extension up to 33 frames. Starting from either a single image or a 9-frame video input, it can generate the remaining frames to reach the 33-frame length (generating 32 or 24 frames respectively). We have evaluated all eight possible configurations (4 models × 2 vision input types: image or video) using 100 test videos from physical AI domains. Below are the failure rates for each configuration: We define failure cases as videos with severe distortions, such as: • Sudden appearance of large unexpected objects • Video degrading to a single solid color Note that the following are not considered failures in our analysis: • Static video frames • Minor object distortions or artifacts Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the subcards of Explainability, Bias, Safety & Security, and Privacy below. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "cosmos-predict1-7b",
    "nvidia_url": "https://build.nvidia.com/nvidia/cosmos-predict1-7b",
    "scraped_on": "2025-10-15T21:40:04.079727",
    "tags": [
      "autonomous vehicles",
      "image-to-world",
      "nvidia",
      "physical ai",
      "robotics",
      "synthetic data generation",
      "text-to-world"
    ],
    "model_card": "Cosmos-Predict1: A Suite of Diffusion-Based World Foundation Models =================================================================== Cosmos-Predict1 Cosmos|Code|Paper Cosmos (https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6) Cosmos Code (https://github.com/NVIDIA/Cosmos) Code Paper (https://research.nvidia.com/publication/2025-01_cosmos-world-foundation-model-platform-physical-ai) Paper Model Overview ============== Description: ============ Cosmos World Foundation Models: A family of highly performant pre-trained world foundation models purpose-built for accelerating synthetic data generation. Cosmos World Foundation Models Cosmos world foundation models include: • Cosmos Predict: Generalist model for predictive video generation from text, image, or video prompts that produces future frame sequences based on input context. Trained on 20M hours of physical AI data, the model serves as a strong foundation for post-training into specialized models for autonomous systems. • Cosmos Transfer: Multicontrol model to generate videos conditioned on ground-truth simulations or structured video inputs for physical accuracy, enabling amplification to diverse environments and lighting conditions. • Cosmos Reason: Multimodal reasoning model for planning text response based on spatial and temporal understanding from input video. Model Developer: NVIDIA Model Developer Model Versions ============== The Cosmos Predict 1 release, includes the following models: • Cosmos-Predict1-7B-Text2WorldGiven a text description, predict an output video of 121 frames. Cosmos-Predict1-7B-Text2World (https://huggingface.co/nvidia/Cosmos-Predict1-7B-Text2World) • Given a text description, predict an output video of 121 frames. • Cosmos-Predict1-7B-Video2WorldGiven a text description and an image as the first frame, predict the next 120 frames. Cosmos-Predict1-7B-Video2World (https://huggingface.co/nvidia/Cosmos-Predict1-7B-Video2World) • Given a text description and an image as the first frame, predict the next 120 frames. License: -------- This model is released under theNVIDIA Open Model License. For a custom license, please contactcosmos-license@nvidia.com. NVIDIA Open Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) cosmos-license@nvidia.com (mailto:cosmos-license@nvidia.com) Under the NVIDIA Open Model License, NVIDIA confirms: • Models are commercially usable. • You are free to create and distribute Derivative Models. • NVIDIA does not claim ownership of any outputs generated using the Models or Derivative Models. Important Note: If you bypass, disable, reduce the efficacy of, or circumvent any technical limitation, safety guardrail or associated safety guardrail hyperparameter, encryption, security, digital rights management, or authentication mechanism contained in the Model, your rights underNVIDIA Open Model License Agreementwill automatically terminate. Important Note NVIDIA Open Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Model Architecture: =================== Cosmos-Predict1-7B-Text2WorldandCosmos-Predict1-7B-Video2Worldare diffusion transformer models designed for video denoising in the latent space. The network is composed of interleaved self-attention, cross-attention, and feedforward layers as its building blocks. The cross-attention layers allow the model to condition on input text throughout the denoising process. Before each layer, adaptive layer normalization is applied to embed the time information for denoising. When an image or video is provided as input, their latent frames are concatenated with the generated frames along the temporal dimension. Augment noise is added to conditional latent frames to bridge the training and inference gap. Cosmos-Predict1-7B-Text2World Cosmos-Predict1-7B-Video2World Cosmos-Predict1-7B-Text2World Input/Output Specifications ========================================================= • InputInput Type(s): TextInput Format(s): StringInput Parameters: One-dimensional (1D)Other Properties Related to Input:The input string should contain fewer than 300 words and should provide descriptive content for world generation, such as a scene description, key objects or characters, background, and any specific actions or motions to be depicted within the 5-second duration. Input • Input Type(s): Text Input Type(s) • Input Format(s): String Input Format(s) • Input Parameters: One-dimensional (1D) Input Parameters • Other Properties Related to Input:The input string should contain fewer than 300 words and should provide descriptive content for world generation, such as a scene description, key objects or characters, background, and any specific actions or motions to be depicted within the 5-second duration. Other Properties Related to Input • The input string should contain fewer than 300 words and should provide descriptive content for world generation, such as a scene description, key objects or characters, background, and any specific actions or motions to be depicted within the 5-second duration. • OutputOutput Type(s): VideoOutput Format(s): mp4Output Parameters: Three-dimensional (3D)Other Properties Related to Output: The generated video will be a 5-second clip with a resolution of 1280x704 pixels at 24 frames per second (fps). The content of the video will visualize the input text description as a short animated scene, capturing the main elements mentioned in the input within the time constraints. Output • Output Type(s): Video Output Type(s) • Output Format(s): mp4 Output Format(s) • Output Parameters: Three-dimensional (3D) Output Parameters • Other Properties Related to Output: The generated video will be a 5-second clip with a resolution of 1280x704 pixels at 24 frames per second (fps). The content of the video will visualize the input text description as a short animated scene, capturing the main elements mentioned in the input within the time constraints. Other Properties Related to Output Cosmos-Predict1-7B-Video2World Input/Output Specifications ========================================================== • InputInput Type(s): Text+Image, Text+VideoInput Format(s):Text: StringImage: jpg, png, jpeg, webpVideo: mp4Input Parameters:Text: One-dimensional (1D)Image: Two-dimensional (2D)Video: Three-dimensional (3D)Other Properties Related to Input:The input string should contain fewer than 300 words and should provide descriptive content for world generation, such as a scene description, key objects or characters, background, and any specific actions or motions to be depicted within the 5-second duration.The input image should be of 1280x704 resolution.The input video should be of 1280x704 resolution and 9 input frames. Input • Input Type(s): Text+Image, Text+Video Input Type(s) • Input Format(s):Text: StringImage: jpg, png, jpeg, webpVideo: mp4 Input Format(s) • Text: String • Image: jpg, png, jpeg, webp • Video: mp4 • Input Parameters:Text: One-dimensional (1D)Image: Two-dimensional (2D)Video: Three-dimensional (3D) Input Parameters • Text: One-dimensional (1D) • Image: Two-dimensional (2D) • Video: Three-dimensional (3D) • Other Properties Related to Input:The input string should contain fewer than 300 words and should provide descriptive content for world generation, such as a scene description, key objects or characters, background, and any specific actions or motions to be depicted within the 5-second duration.The input image should be of 1280x704 resolution.The input video should be of 1280x704 resolution and 9 input frames. Other Properties Related to Input • The input string should contain fewer than 300 words and should provide descriptive content for world generation, such as a scene description, key objects or characters, background, and any specific actions or motions to be depicted within the 5-second duration. • The input image should be of 1280x704 resolution. • The input video should be of 1280x704 resolution and 9 input frames. • OutputOutput Type(s): VideoOutput Format(s): mp4Output Parameters: Three-dimensional (3D)Other Properties Related to Output: The generated video will be a 5-second clip with a resolution of 1280x704 pixels at 24 frames per second (fps). The content of the video will use the provided image as the first frame and visualize the input text description as a short animated scene, capturing the main elements mentioned in the input within the time constraints. Output • Output Type(s): Video Output Type(s) • Output Format(s): mp4 Output Format(s) • Output Parameters: Three-dimensional (3D) Output Parameters • Other Properties Related to Output: The generated video will be a 5-second clip with a resolution of 1280x704 pixels at 24 frames per second (fps). The content of the video will use the provided image as the first frame and visualize the input text description as a short animated scene, capturing the main elements mentioned in the input within the time constraints. Other Properties Related to Output Software Integration ==================== Runtime Engine(s): • Cosmos Cosmos (https://github.com/NVIDIA/Cosmos) Supported Hardware Microarchitecture Compatibility: • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Ampere Note: We have only tested inference with BF16 precision. Note Operating System(s): • Linux (We have not tested it on other operating systems.) Usage ===== • SeeCosmosfor details. Cosmos (https://github.com/NVIDIA/Cosmos) Evaluation ========== Please see ourtechnical paperfor detailed evaluations. technical paper (https://research.nvidia.com/publication/2025-01_cosmos-world-foundation-model-platform-physical-ai) Inference Time and GPU Memory Usage =================================== The numbers provided below may vary depending on system specs and are for reference only. We report the maximum observed GPU memory usage during end-to-end inference. Additionally, we offer a series of model offloading strategies to help users manage GPU memory usage effectively. For GPUs with limited memory (e.g., RTX 3090/4090 with 24 GB memory), we recommend fully offloading all models. For higher-end GPUs, users can select the most suitable offloading strategy considering the numbers provided below. Cosmos-Predict1-7B-Text2World ----------------------------- The table below presents the end-to-end inference runtime on a single H100 GPU, excluding model initialization time. Cosmos-Predict1-7B-Video2World ------------------------------ The following table shows the end-to-end inference runtime on a single H100 GPU, excluding model initialization time: Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the subcards of Explainability, Bias, Safety & Security, and Privacy below. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "qwq-32b",
    "nvidia_url": "https://build.nvidia.com/qwen/qwq-32b",
    "scraped_on": "2025-10-15T21:40:03.640056",
    "tags": [
      "advanced reasoning",
      "chat",
      "coding",
      "math",
      "qwen"
    ],
    "model_card": "Model Overview ============== Model Overview Description ----------- Description QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini. This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAQwQ-32B Model Card. QwQ-32B Model Card (https://huggingface.co/Qwen/QwQ-32B) License/Terms of Use ==================== Qwen/QwQ-32B is licensed under theApache 2.0 License Apache 2.0 License (https://huggingface.co/Qwen/QwQ-32B/blob/main/LICENSE) References: =========== Blog,Github,Documentation,Technical Report Blog (https://qwenlm.github.io/blog/qwq-32b) Github (https://github.com/QwenLM/QwQ) Documentation (https://qwen.readthedocs.io/en/latest) Technical Report (https://arxiv.org/abs/2412.15115) Model Architecture: =================== Architecture Type:Transformer with RoPE, SwiGLU, RMSNorm, and Attention QKV biasNetwork Architecture:Qwen2.5 Architecture Type: Network Architecture: This model was developed based on Qwen2.5and has 32.5B of model parameters. This model was developed based on Qwen2.5 Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:1DOther Properties Related to Input:Support up to 131,072 tokens Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:1DOther Properties Related to Output:Generate up to 32,768 tokens Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Model Version(s): ================= QwQ-32B Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= Link:UnknownData Collection Method by dataset:UnknownLabeling Method by dataset:UnknownProperties:Unknown Link: Data Collection Method by dataset: Labeling Method by dataset: Properties: Testing Dataset: ================ Link:UnknownData Collection Method by dataset:UnknownLabeling Method by dataset:UnknownProperties:Unknown Link: Data Collection Method by dataset: Labeling Method by dataset: Properties: Evaluation Dataset: =================== Link:Detailed evaluation results are reported in this blogQwQ-32B: Embracing the Power of Reinforcement LearningData Collection Method by dataset:UnknownLabeling Method by dataset:UnknownProperties:Unknown Link: QwQ-32B: Embracing the Power of Reinforcement Learning (https://qwenlm.github.io/blog/qwq-32b) Data Collection Method by dataset: Labeling Method by dataset: Properties: Inference: ========== Engine:TensorRT-LLMTest Hardware:NVIDIA L40S Engine: Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-4-scout-17b-16e-instruct",
    "nvidia_url": "https://build.nvidia.com/meta/llama-4-scout-17b-16e-instruct",
    "scraped_on": "2025-10-15T21:40:00.486639",
    "tags": [
      "image-to-text",
      "language generation",
      "meta",
      "vision assistant",
      "visual question answering"
    ],
    "model_card": "Model Information ================= The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. These Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to Meta's requirements for this application and use case; see link to Non-NVIDIALlama 4 Scout. Llama 4 Scout (https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct) License/Terms of Use: --------------------- GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service; and the use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:Llama 4 Community Model License. Built with Llama. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Llama 4 Community Model License (https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) Model developer: Meta Model developer Model Architecture:The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality. Model Architecture: Privacy Center (https://www.facebook.com/privacy/guide/genai/) Supported languages:Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Supported languages: Model Release Date:April 5, 2025 Model Release Date: Status:This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback. Status: Where to send questions or comments about the model:Instructions on how to provide feedback or comments on the model can be found in the LlamaREADME. For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please gohere. Where to send questions or comments about the model: README (https://github.com/meta-llama/llama-models/blob/main/README.md) here (https://github.com/meta-llama/llama-cookbook) Intended Use ============ Intended Use Cases:Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases. Intended Use Cases: Out-of-scope: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card**. Out-of-scope **Note: 1. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes200 total languages). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy. Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner. 200 total languages (https://ai.meta.com/research/no-language-left-behind/) 2. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications. Hardware and Software ===================== Training Factors:We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure. Training Factors: Training Energy Use:Model pre-training utilized a cumulative of7.38MGPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. Training Energy Use: 7.38M Training Greenhouse Gas Emissions:Estimated total location-based greenhouse gas emissions were1,999 tonsCO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq. ========================================================================================================================================================================================================================================================================================================================================================================================== Training Greenhouse Gas Emissions: 1,999 tons The methodology used to determine training energy use and greenhouse gas emissions can be foundhere. Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others. ======================================================================================================================================================================================================================================= here (https://arxiv.org/pdf/2204.05149) Training Data ============= Overview:Llama 4 Scout was pretrained on ~40 trillion tokens and Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta’s products and services. This includes publicly shared posts from Instagram and Facebook and people’s interactions with Meta AI. Overview: Data Freshness:The pretraining data has a cutoff of August 2024. Data Freshness: Benchmarks ========== In this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models. Pre-trained models ------------------ Llama 4 Scout Llama 4 Maverick Instruction tuned models ------------------------ Llama 4 Scout Llama 4 Maverick ^reported numbers for MMMU Pro is the average of Standard and Vision tasks Quantization ============ The Llama 4 Scout model is released as BF16 weights, but can fit within a single H100 GPU with on-the-fly int4 quantization; the Llama 4 Maverick model is released as both BF16 and FP8 quantized weights. The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality. We provide code for on-the-fly int4 quantization which minimizes performance degradation as well. Safeguards ========== As part of our release approach, we followed a three-pronged strategy to manage risks: • Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. • Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm. • Provide protections for the community to help prevent the misuse of our models. Llama is a foundational technology designed for use in a variety of use cases; examples on how Meta’s Llama models have been deployed can be found in ourCommunity Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model’s safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in ourDeveloper Use Guide: AI Protections. Community Stories webpage (https://llama.meta.com/community-stories/) Developer Use Guide: AI Protections (https://ai.meta.com/static-resource/developer-use-guide-ai-protections) Model level fine tuning ----------------------- The primary objective of conducting safety fine-tuning is to offer developers a readily available, safe, and powerful model for various applications, reducing the workload needed to deploy safe AI systems. Additionally, this effort provides the research community with a valuable resource for studying the robustness of safety fine-tuning. Fine-tuning dataWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. Fine-tuning data RefusalsBuilding on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. Refusals ToneWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more. Tone To achieve this, we also targeted improvements to system prompt steerability and instruction following, meaning the model is more readily able to take on a specified tone. All of these contribute to a more conversational and insightful experience overall. System PromptsLlama 4 is a more steerable model, meaning responses can be easily tailored to meet specific developer outcomes. Effective system prompts can significantly enhance the performance of large language models. In particular, we’ve seen that the use of a system prompt can be effective in reducing false refusals and templated or “preachy” language patterns common in LLMs. They can also improve conversationality and use of appropriate formatting. System Prompts Consider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models. Llama 4 system protections -------------------------- Large language models, including Llama 4, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional guardrails as required. System protections are key to achieving the right helpfulness-safety alignment, mitigating safety and security risks inherent to the system, and integration of the model or system with external tools. We provide the community with system levelprotections- like Llama Guard, Prompt Guard and Code Shield - that developers should deploy with Llama models or other LLMs. All of ourreference implementationdemos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. protections (https://llama.meta.com/trust-and-safety/) reference implementation (https://github.com/meta-llama/llama-agentic-system) Evaluations ----------- We evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization. Red teamingWe conduct recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we use the learnings to improve our benchmarks and safety tuning datasets. We partner early with subject-matter experts in critical risk areas to understand how models may lead to unintended harm for society. Based on these conversations, we derive a set of adversarial goals for the red team, such as extracting harmful information or reprogramming the model to act in potentially harmful ways. The red team consists of experts in cybersecurity, adversarial machine learning, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. Red teaming Critical Risks -------------- We spend additional focus on the following critical risk areas: --------------------------------------------------------------- 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulnessTo assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area. 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness 2. Child SafetyWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model’s capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We’ve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual. 2. Child Safety 3. Cyber attack enablementOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed. We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. 3. Cyber attack enablement Community --------- Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Trust tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository. Github repository (https://github.com/meta-llama/PurpleLlama) We also set up theLlama Impact Grantsprogram to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be foundhere. Llama Impact Grants (https://llama.meta.com/llama-impact-grants/) here (https://llama.meta.com/llama-impact-grants/#finalists) Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community. output reporting mechanism (https://developers.facebook.com/llama_output_feedback) bug bounty program (https://www.facebook.com/whitehat) Considerations and Limitations ============================== Our AI is anchored on the values of freedom of expression - helping people to explore, debate, and innovate using our technology. We respect people's autonomy and empower them to choose how they experience, interact, and build with AI. Our AI promotes an open exchange of ideas. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 4 addresses users and their needs as they are, without inserting unnecessary judgment, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. Llama 4 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 4’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections,Llama Protectionssolutions, and otherresourcesto learn more. Llama Protections (https://llama.meta.com/trust-and-safety/) resources (https://llama.meta.com/docs/get-started/) Inference: ========== Engine:vLLMTest Hardware: Engine: Test Hardware: • NVIDIA Hopper Ethical Considerations (For NVIDIA Models Only): ================================================ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-4-maverick-17b-128e-instruct",
    "nvidia_url": "https://build.nvidia.com/meta/llama-4-maverick-17b-128e-instruct",
    "scraped_on": "2025-10-15T21:39:57.261815",
    "tags": [
      "image-to-text",
      "language generation",
      "meta",
      "vision assistant",
      "visual question answering"
    ],
    "model_card": "Model Information ================= The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. These Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to Meta's requirements for this application and use case; see link to Non-NVIDIALlama 4 Maverick. Llama 4 Maverick (https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct) License/Terms of Use: --------------------- GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service; and the use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:Llama 4 Community Model License. Built with Llama. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Llama 4 Community Model License (https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) Model developer: Meta Model developer Model Architecture:The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality. Model Architecture: Privacy Center (https://www.facebook.com/privacy/guide/genai/) Supported languages:Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Supported languages: Model Release Date:April 5, 2025 Model Release Date: Status:This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback. Status: Where to send questions or comments about the model:Instructions on how to provide feedback or comments on the model can be found in the LlamaREADME. For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please gohere. Where to send questions or comments about the model: README (https://github.com/meta-llama/llama-models/blob/main/README.md) here (https://github.com/meta-llama/llama-cookbook) Intended Use ============ Intended Use Cases:Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases. Intended Use Cases: Out-of-scope: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card**. Out-of-scope **Note: 1. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes200 total languages). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy. Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner. 200 total languages (https://ai.meta.com/research/no-language-left-behind/) 2. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications. Hardware and Software ===================== Training Factors:We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure. Training Factors: Training Energy Use:Model pre-training utilized a cumulative of7.38MGPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. Training Energy Use: 7.38M Training Greenhouse Gas Emissions:Estimated total location-based greenhouse gas emissions were1,999 tonsCO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq. ========================================================================================================================================================================================================================================================================================================================================================================================== Training Greenhouse Gas Emissions: 1,999 tons The methodology used to determine training energy use and greenhouse gas emissions can be foundhere. Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others. ======================================================================================================================================================================================================================================= here (https://arxiv.org/pdf/2204.05149) Training Data ============= Overview:Llama 4 Scout was pretrained on ~40 trillion tokens and Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta’s products and services. This includes publicly shared posts from Instagram and Facebook and people’s interactions with Meta AI. Overview: Data Freshness:The pretraining data has a cutoff of August 2024. Data Freshness: Benchmarks ========== In this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models. Pre-trained models ------------------ Llama 4 Scout Llama 4 Maverick Instruction tuned models ------------------------ Llama 4 Scout Llama 4 Maverick ^reported numbers for MMMU Pro is the average of Standard and Vision tasks Quantization ============ The Llama 4 Scout model is released as BF16 weights, but can fit within a single H100 GPU with on-the-fly int4 quantization; the Llama 4 Maverick model is released as both BF16 and FP8 quantized weights. The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality. We provide code for on-the-fly int4 quantization which minimizes performance degradation as well. Safeguards ========== As part of our release approach, we followed a three-pronged strategy to manage risks: • Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. • Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm. • Provide protections for the community to help prevent the misuse of our models. Llama is a foundational technology designed for use in a variety of use cases; examples on how Meta’s Llama models have been deployed can be found in ourCommunity Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model’s safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in ourDeveloper Use Guide: AI Protections. Community Stories webpage (https://llama.meta.com/community-stories/) Developer Use Guide: AI Protections (https://ai.meta.com/static-resource/developer-use-guide-ai-protections) Model level fine tuning ----------------------- The primary objective of conducting safety fine-tuning is to offer developers a readily available, safe, and powerful model for various applications, reducing the workload needed to deploy safe AI systems. Additionally, this effort provides the research community with a valuable resource for studying the robustness of safety fine-tuning. Fine-tuning dataWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. Fine-tuning data RefusalsBuilding on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. Refusals ToneWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more. Tone To achieve this, we also targeted improvements to system prompt steerability and instruction following, meaning the model is more readily able to take on a specified tone. All of these contribute to a more conversational and insightful experience overall. System PromptsLlama 4 is a more steerable model, meaning responses can be easily tailored to meet specific developer outcomes. Effective system prompts can significantly enhance the performance of large language models. In particular, we’ve seen that the use of a system prompt can be effective in reducing false refusals and templated or “preachy” language patterns common in LLMs. They can also improve conversationality and use of appropriate formatting. System Prompts Consider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models. Llama 4 system protections -------------------------- Large language models, including Llama 4, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional guardrails as required. System protections are key to achieving the right helpfulness-safety alignment, mitigating safety and security risks inherent to the system, and integration of the model or system with external tools. We provide the community with system levelprotections- like Llama Guard, Prompt Guard and Code Shield - that developers should deploy with Llama models or other LLMs. All of ourreference implementationdemos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. protections (https://llama.meta.com/trust-and-safety/) reference implementation (https://github.com/meta-llama/llama-agentic-system) Evaluations ----------- We evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.Capability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization. Red teamingWe conduct recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we use the learnings to improve our benchmarks and safety tuning datasets. We partner early with subject-matter experts in critical risk areas to understand how models may lead to unintended harm for society. Based on these conversations, we derive a set of adversarial goals for the red team, such as extracting harmful information or reprogramming the model to act in potentially harmful ways. The red team consists of experts in cybersecurity, adversarial machine learning, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. Red teaming Critical Risks -------------- We spend additional focus on the following critical risk areas: --------------------------------------------------------------- 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulnessTo assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area. 1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness 2. Child SafetyWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model’s capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We’ve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual. 2. Child Safety 3. Cyber attack enablementOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed. We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. 3. Cyber attack enablement Community --------- Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Trust tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to ourGithub repository. Github repository (https://github.com/meta-llama/PurpleLlama) We also set up theLlama Impact Grantsprogram to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be foundhere. Llama Impact Grants (https://llama.meta.com/llama-impact-grants/) here (https://llama.meta.com/llama-impact-grants/#finalists) Finally, we put in place a set of resources including anoutput reporting mechanismandbug bounty programto continuously improve the Llama technology with the help of the community. output reporting mechanism (https://developers.facebook.com/llama_output_feedback) bug bounty program (https://www.facebook.com/whitehat) Considerations and Limitations ============================== Our AI is anchored on the values of freedom of expression - helping people to explore, debate, and innovate using our technology. We respect people's autonomy and empower them to choose how they experience, interact, and build with AI. Our AI promotes an open exchange of ideas. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 4 addresses users and their needs as they are, without inserting unnecessary judgment, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. Llama 4 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 4’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections,Llama Protectionssolutions, and otherresourcesto learn more. Llama Protections (https://llama.meta.com/trust-and-safety/) resources (https://llama.meta.com/docs/get-started/) Inference: ========== Engine:vLLMTest Hardware: Engine: Test Hardware: • NVIDIA Hopper Ethical Considerations (For NVIDIA Models Only): ================================================ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.1-nemotron-ultra-253b-v1",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1",
    "scraped_on": "2025-10-15T21:39:54.052761",
    "tags": [
      "advanced reasoning",
      "chat",
      "function calling",
      "instruction following",
      "math",
      "nvidia"
    ],
    "model_card": "Llama-3.1-Nemotron-Ultra-253B-v1 ================================ Model Overview ============== Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) which is a derivative ofMeta Llama-3.1-405B-Instruct(AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling. The model supports a context length of 128K tokens. This model fits on a single 8xH100 node for inference. Meta Llama-3.1-405B-Instruct (https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct) Llama-3.1-Nemotron-Ultra-253B-v1 is a model which offers a great tradeoff between model accuracy and efficiency. Efficiency (throughput) directly translates to savings. Using a novel Neural Architecture Search (NAS) approach, we greatly reduce the model’s memory footprint, enabling larger workloads, as well as reducing the number of GPUs required to run the model in a data center environment. This NAS approach enables the selection of a desired point in the accuracy-efficiency tradeoff. Furthermore, by using a novel method to vertically compress the model (see detailshere), it also offers a significant improvement in latency. here (https://arxiv.org/abs/2503.18908) The model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, Chat, and Tool Calling as well as multiple reinforcement learning (RL) stages using Group Relative Policy Optimization (GRPO) algorithms for reasoning, chat, and instruction-following. This model is ready for commercial use. For more details on how the model was trained, please seethis blog. this blog (https://developer.nvidia.com/blog/build-enterprise-ai-agents-with-advanced-open-nvidia-llama-nemotron-reasoning-models/) This model is part of the Llama Nemotron Collection. You can find the other model(s) in this family here: • Llama-3.1-Nemotron-Nano-8B-v1 Llama-3.1-Nemotron-Nano-8B-v1 (https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1) • Llama-3.3-Nemotron-Nano-49B-v1 Llama-3.3-Nemotron-Nano-49B-v1 (https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1) License/Terms of Use ==================== GOVERNING TERMS: Your use of this model is governed by theNVIDIA Open Model License.Additional Information:Llama 3.1 Community License Agreement. Built with Llama. NVIDIA Open Model License. (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Llama 3.1 Community License Agreement (https://www.llama.com/llama3_1/license/) Model Developer:NVIDIA Model Developer: Model Dates:Trained between November 2024 and April 2025 Model Dates: Data Freshness:The pretraining data has a cutoff of 2023 per Llama-3.1-405B-Instruct Data Freshness: Use Case: --------- Developers designing AI Agent systems, chatbots, RAG systems, and other AI-powered applications. Also suitable for typical instruction-following tasks. Release Date: ------------- 2025-04-07 References ========== • [2502.00203]Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment (https://arxiv.org/abs/2502.00203) • [2411.19146]Puzzle: Distillation-Based NAS for Inference-Optimized LLMs Puzzle: Distillation-Based NAS for Inference-Optimized LLMs (https://arxiv.org/abs/2411.19146) • [2503.18908]FFN Fusion: Rethinking Sequential Computation in Large Language Models FFN Fusion: Rethinking Sequential Computation in Large Language Models (https://arxiv.org/abs/2503.18908) Model Architecture ================== Architecture Type:Dense decoder-only Transformer modelNetwork Architecture:Llama-3.1-405B-Instruct, customized through Neural Architecture Search (NAS) Architecture Type: Network Architecture: **This model was developed based on Llama-3.1-405B-Instruct** This model has 253B model parameters. The model is a derivative of Llama 3.1-405B-Instruct, using Neural Architecture Search (NAS). The NAS algorithm results in non-standard and non-repetitive blocks. This includes the following: • Skip attention: In some blocks, the attention is skipped entirely, or replaced with a single linear layer. • Variable FFN: The expansion/compression ratio in the FFN layer is different between blocks. • FFN Fusion: When several consecutive attention layers are skipped, which can result in a sequence of multiple FFNs, that sequence of FFNs are fused into a smaller number of wider FFN layers. For each block of the reference model, we create multiple variants providing different tradeoffs of quality vs. computational complexity, discussed in more depth below. We then search over the blocks to create a model which meets the required throughput and memory while minimizing the quality degradation. To recover performance, the model initially undergoes knowledge distillation (KD) for 65 billion tokens. This is followed by a continual pretraining (CPT) phase for 88 billion tokens. Intended use ============ Llama-3.1-Nemotron-Ultra-253B-v1 is a general purpose reasoning and chat model intended to be used in English and coding languages. Other non-English languages (German, French, Italian, Portuguese, Hindi, Spanish, and Thai) are also supported. Input ===== • Input Type:Text Input Type: • Input Format:String Input Format: • Input Parameters:One-Dimensional (1D) Input Parameters: • Other Properties Related to Input:Context length up to 131,072 tokens Other Properties Related to Input: Output ====== • Output Type:Text Output Type: • Output Format:String Output Format: • Output Parameters:One-Dimensional (1D) Output Parameters: • Other Properties Related to Output:Context length up to 131,072 tokens Other Properties Related to Output: Software Integration ==================== • Runtime Engine:TRT-LLM Runtime Engine: • Recommended Hardware Microarchitecture Compatibility: Recommended Hardware Microarchitecture Compatibility: • NVIDIA Hopper • NVIDIA Ampere -[Preferred/Supported] Operating System(s):Linux [Preferred/Supported] Operating System(s): Model Version ============= 1.0 (3/18/2025) Quick Start and Usage Recommendations: ====================================== • Reasoning mode (ON/OFF) is controlled via the system prompt, which must be set as shown in the example below. All instructions should be contained within the user prompt • We recommend setting temperature to0.6, and Top P to0.95for Reasoning ON mode • We recommend using greedy decoding (temperature 0) for Reasoning OFF mode • We do not recommend to add additional system prompts besides the control prompt, all instructions should be put into user query • We have provided a list of prompts to use for evaluation for each benchmark where a specific template is required • The model will include<think></think>if no reasoning was necessary in Reasoning ON model, this is expected behaviour You can try this model out through the preview API, using this link:Llama-3_1-Nemotron-Ultra-253B-v1. Llama-3_1-Nemotron-Ultra-253B-v1 (https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1) Inference: ========== Engine:TRT-LLMTest Hardware:- BF16: Engine: Test Hardware: • 8x NVIDIA H100-80GB • 4x NVIDIA B100FP 8 • FP 8 • 4x NVIDIA H100-80GB Training and Evaluation Datasets ================================ Training Datasets ================= A large variety of training data was used for the knowledge distillation phase before post-training pipeline, 3 of which included: FineWeb, Buzz-V1.2, and Dolma. The data for the multi-stage post-training phases is a compilation of SFT and RL data that supports improvements of math, code, general reasoning, and instruction following capabilities of the original Llama instruct model. Prompts have been sourced from either public and open corpus or synthetically generated. Responses were synthetically generated by a variety of models, with some prompts containing responses for both reasoning on and off modes, to train the model to distinguish between two modes. This model was improved with Qwen. We have released ourLlama-Nemotron-Post-Training-Datasetto promote openness and transparency in model development and improvement. Llama-Nemotron-Post-Training-Dataset (https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset) Data Collection for Training Datasets:Hybrid: Automated, Human, Synthetic Data Collection for Training Datasets: Data Labeling for Training Datasets:Hybrid: Automated, Human, Synthetic Data Labeling for Training Datasets: Evaluation Datasets =================== We used the datasets listed in the next section to evaluate Llama-3.1-Nemotron-Ultra-253B-v1. Data Collection for Evaluation Datasets: Hybrid: Human/Synthetic Data Labeling for Evaluation Datasets: Hybrid: Human/Synthetic/Automatic Evaluation Results ================== These results contain both Reasoning On, and Reasoning Off. We recommend using temperature=0.6, top_p=0.95for Reasoning On mode, and greedy decoding for Reasoning Off mode. All evaluations are done with 32k sequence length. We run the benchmarks up to 16 times and average the scores to be more accurate. NOTE: Where applicable, a Prompt Template will be provided. While completing benchmarks, please ensure that you are parsing for the correct output format as per the provided prompt in order to reproduce the benchmarks seen below. GPQA ---- User Prompt Template: AIME25 ------ User Prompt Template: BFCL V2 Live ------------ User Prompt Template: LiveCodeBench (20240801-20250201) --------------------------------- User Prompt Template (without starter code): User Prompt Template (with starter code): IFEval ------ MATH500 ------- User Prompt Template: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++Explainability,Bias,Safety & Security, andPrivacySubcards. Explainability (./explainability.md) Bias (./bias.md) Safety & Security (./safety.md) Privacy (./privacy.md) Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "FLUX.1-dev",
    "nvidia_url": "https://build.nvidia.com/black-forest-labs/flux_1-dev",
    "scraped_on": "2025-10-15T21:39:53.677690",
    "tags": [
      "black-forest-labs",
      "image generation",
      "run-on-rtx",
      "text-to-image"
    ],
    "model_card": "Overview ======== Description: ============ FLUX.1 is a collection of generative image AI models creating high quality, realistic images: • FLUX.1-dev generates images from simple text prompts. • FLUX.1-Canny-dev combines the text prompt with an image input processed to canny edges to guide the output image structure. • FLUX.1-Depth-dev combines the text prompt with an image input processed to depth map leveraging LiheYoung/Depth-anything-large-hf model to guide the output image structure. This model is ready for non-commercial use. Contactsales@blackforestlabs.aifor commercial terms. sales@blackforestlabs.ai (mailto:sales@blackforestlabs.ai) Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to: • black-forest-labs/FLUX.1-dev Model Card black-forest-labs/FLUX.1-dev Model Card (https://huggingface.co/black-forest-labs/FLUX.1-dev) • black-forest-labs/FLUX.1-Canny-dev Model Card black-forest-labs/FLUX.1-Canny-dev Model Card (https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev) • black-forest-labs/FLUX.1-Depth-dev Model Card black-forest-labs/FLUX.1-Depth-dev Model Card (https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev) • LiheYoung/Depth-anything-large-hf Model Card LiheYoung/Depth-anything-large-hf Model Card (https://huggingface.co/LiheYoung/depth-anything-large-hf) Terms of use ------------ GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service. Contactsales@blackforestlabs.aifor commercial terms to use the Flux.1-dev model. ADDITIONAL INFORMATION:Apache 2.0,NVIDIA Community Model License AgreementandLlama 2 Community Model License Agreement. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) sales@blackforestlabs.ai (mailto:sales@blackforestlabs.ai) Apache 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) NVIDIA Community Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Llama 2 Community Model License Agreement (https://github.com/meta-llama/llama-models/blob/main/models/llama2/LICENSE) Deployment Geography: --------------------- Global Use Case: ========= Creators and professionals can use this model to generate high-quality images from text prompts, simplifying visual communication. Release Date: ------------- August 1, 2024 References ---------- • Flux on Black Forest Labs Flux on Black Forest Labs (https://blackforestlabs.ai/) • Flux blog post Flux blog post (https://blackforestlabs.ai/announcing-black-forest-labs/) Model Architecture: =================== Architecture Type: Transformer and Convolutional Neural Network (CNN)Network Architecture: Diffusion TransformerLiheYoung/Depth-anything-large-hf leverages the DPT architecture with a DINOv2 backbone. Architecture Type Network Architecture Input: ====== Input Type: Text, Image (optional)Input Parameters: Text: 1D. Image: 2DInput Format: Text: String. Image: Red, Green, Blue (RGB)Other Properties Related to Input: Steps, Classifier-Free Guidance Scale, Output Image Aspect Ratio, and Seed per theAPI Reference Page Input Type Input Parameters Input Format Other Properties Related to Input API Reference Page (https://docs.api.nvidia.com/nim/reference/black-forest-labs-flux_1-dev) Output: ======= Output Type: ImageOutput Parameters: 2DOutput Format: Red, Green, Blue (RGB)Other Properties Related to Output: 1024x1024, 768x1344, 1344x768, 1344x768, 1344x768, 1344x768, 1216x832 Output Type Output Parameters Output Format Other Properties Related to Output Software Integration: ===================== Runtime Engines: • TensorRT Supported Hardware Platforms: Supported Hardware Platforms • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace Supported Operating Systems: Linux, Windows Subsystem for Linux Supported Operating Systems Model Version(s): ================= • FLUX.1-dev • FLUX.1-Canny-dev • FLUX.1-Depth-dev • LiheYoung/Depth-anything-large-hf Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= • Data Collection Method by Dataset: Undisclosed • Labeling Method by Dataset: Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Testing Dataset: ================ • Data Collection Method by Dataset: Undisclosed • Labeling Method by Dataset: Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: =================== • Data Collection Method by Dataset: Undisclosed • Labeling Method by Dataset: Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference: ========== Engine: TensorRTTest Hardware: H100 Engine Test Hardware Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "parakeet-1.1b-rnnt-multilingual-asr",
    "nvidia_url": "https://build.nvidia.com/nvidia/parakeet-1_1b-rnnt-multilingual-asr",
    "scraped_on": "2025-10-15T21:39:50.528556",
    "tags": [
      "asr",
      "multilingual",
      "nvidia",
      "nvidia nim",
      "speech-to-text",
      "streaming"
    ],
    "model_card": "Speech Recognition: Parakeet ============================ Description =========== RIVA Parakeet-RNNT-XXL-1.1B ASR Multilingual with Universal Tokenizer (around 1.1B parameters) [1], [2] is trained on ASR Set with over 90,000 hours of speech. Individual tokenizers are trained for each language and then merged into a single tokenizer. The model transcribes 25 languages (English(en-US, en-GB), Spanish (es-US, es-ES), German (de-DE), French (fr-FR, fr-CA), Italian (it-IT), Arabic (ar-AR), Japanese (ja-JP), Korean (ko-KR), Portuguese (pt-BR, pt-PT), Russian (ru-RU), Hindi (hi-IN), Dutch (nl-NL), Danish (da-DK), Norwegian Nynorsk (nn-NO), Norwegian Bokmal (nb-NO), Czech (cs-CZ), Polish (pl-PL), Swedish (sv-SE), Thai (th-TH), Turkish (tr-TR), and Hebrew (he-IL)) in upper case and lower case alphabets along with punctuations, spaces, and apostrophes. This model is ready for commercial use. License/Terms of Use ==================== GOVERNING TERMS: The use of this model is governed by the NVIDIA Community Model License (found athttps://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/). https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/ References ========== [1]Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition[2]Fast-Conformer-RNNT Model[3]Conformer: Convolution-augmented Transformer for Speech Recognition Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition (https://arxiv.org/abs/2305.05084) Fast-Conformer-RNNT Model (https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html) Conformer: Convolution-augmented Transformer for Speech Recognition (https://arxiv.org/abs/2005.08100) Model Architecture ================== Architecture Type:Parakeet-RNNT (also known as FastConformer-RNNT) [1], [2] which is an optimized version of Conformer model [3] with 8x depthwise-separable convolutional downsampling with Hybrid lossNetwork Architecture:Parakeet-RNNT-XXL-1.1B Architecture Type: Network Architecture: Input ===== Input Type(s):AudioInput Format(s):wavInput Parameters:1-DimensionOther Properties Related to Input:Maximum Length in seconds specific to GPU Memory, No Pre-Processing Needed, Mono channel is required Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output ====== Output Type(s):TextOutput Format:StringOutput Parameters:1-DimensionOther Properties Related to Output:No Maximum Character Length, Does not handle special characters Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration ==================== Supported Hardware Microarchitecture Compatibility: • NVIDIA Blackwell • NVIDIA Ampere • NVIDIA Hopper • NVIDIA Jetson • NVIDIA Turing [Preferred/Supported] Operating System(s): • Linux Model Version(s): ================= Parakeet-RNNT-XXL-1.1b_universal_spe8.5k_1.0 Inference ========= Engine:TritonTest Hardware: Engine: Test Hardware: • NVIDIA A10 • NVIDIA A100 • NVIDIA A30 • NVIDIA H100 • NVIDIA L4 • NVIDIA L40 • NVIDIA Turing T4 Ethical Considerations (For NVIDIA Models Only): ================================================ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "mistral-medium-3-instruct",
    "nvidia_url": "https://build.nvidia.com/mistralai/mistral-medium-3-instruct",
    "scraped_on": "2025-10-15T21:39:47.405191",
    "tags": [
      "image-to-text",
      "language generation",
      "mistralai",
      "multimodal",
      "visual question answering"
    ],
    "model_card": "Mistral Medium 3 Overview ========================= Description: ============ Mistral Medium 3is a frontier-class dense language model optimized for enterprise use. It delivers state-of-the-art performance at significantly lower cost—up to 8× cheaper than leading alternatives—while maintaining high usability, adaptability, and deployability in enterprise environments. Designed to excel in professional workloads like coding, STEM reasoning, and multimodal understanding, it supports hybrid and self-hosted deployment, full model customization, and seamless integration into enterprise systems. Mistral Medium 3 This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIAMistral Medium 3 Model Card. Mistral Medium 3 Model Card (https://mistral.ai/news/mistral-medium-3) License and Terms of Use ------------------------ GOVERNING TERMS:The trial service is governed by theNVIDIA API Trial Terms of Service. To deploy and customize the model in your environment, pleasecontact Mistral. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) contact Mistral (https://mistral.ai/contact) Deployment Geography: --------------------- Global Use Case: --------- Enterprise and research users leveraging high-performance LLMs for reasoning, multilingual understanding, and coding tasks. • Hybrid or on-premises / in-VPC deployment • Custom post-training • Integration into enterprise tools and systems Release Date: ------------- • May of 2025 Reference(s): ============= **https://mistral.ai/news/mistral-medium-3 https://mistral.ai/news/mistral-medium-3 Model Architecture: =================== Architecture Type:Transformer-based dense decoder-only autoregressive LLMThis model was developed based on:Proprietary Mistral architecture Architecture Type: This model was developed based on: Input: ====== Input Type(s):Text, Text+ImageInput Format(s):StringInput Parameters:2D token sequencesOther Properties Related to Input: Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: • Up to 128k tokens context length • Pre-tokenized usingmistral-tokenizer Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:2D token sequencesOther Properties Related to Output: Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: • Output is in plain text, produced autoregressively • Post-processing required to decode tokens to readable text Supported Hardware Microarchitecture Compatibility: =================================================== [Preferred/Supported] Operating System(s): • Linux • Windows Model Version(s): ================= Mistral Medium 3 (2505) Training, Testing, and Evaluation Datasets: =========================================== Training Dataset : ================== Data Collection Method by dataset: UndisclosedLabeling Method by dataset: UndisclosedProperties: Undisclosed Testing Dataset: ================ Data Collection Method by dataset: UndisclosedLabeling Method by dataset: UndisclosedProperties: Undisclosed Evaluation Dataset: =================== Top-tier performance. --------------------- Mistral Medium 3 is designed to be frontier-class, particularly in categories of professional use. In the evaluations below, we use numbers reported previously by other providers wherever available, otherwise we use our own evaluation harness. Performance accuracy on all benchmarks were obtained through the same internal evaluation pipeline. Mistral Medium 3 particular stands out in coding and STEM tasks where it comes close to its very large and much slower competitors. Mistral Medium Benchmarking --------------------------- CODING INSTRUCTION FOLLOWING MATH KNOWLEDGE LONG CONTEXT MULTIMODAL *Performance accuracy on all benchmarks were obtained through the same internal evaluation pipeline. Human Evals ----------- In addition to academic benchmarks we report third-party human evaluations that are more representative of real-world use cases. Mistral Medium 3 continues to shine in the coding domain and delivers much better performance, across the board, than some of its much larger competitors. Inference: ========== Engine:Compatible with open-source inference engines like vLLMTest Hardware: Engine: Test Hardware: • NVIDIA H100 Ethical Considerations ---------------------- NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "mistral-small-3.1-24b-instruct-2503",
    "nvidia_url": "https://build.nvidia.com/mistralai/mistral-small-3_1-24b-instruct-2503",
    "scraped_on": "2025-10-15T21:39:46.961894",
    "tags": [
      "image understanding",
      "language generation",
      "mistralai",
      "multimodal"
    ],
    "model_card": "Mistral-Small-3.1-24B-Instruct-2503 Overview ============================================ Model Overview ============== Description ----------- Mistral Small 3.1 (2503) builds upon Mistral Small 3 (2501) by adding state-of-the-art vision understanding and enhancing long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks. Key features include vision capabilities, multilingual support, agent-centric design with native function calling and JSON outputting, advanced reasoning, a 128k context window, strong adherence to system prompts, and utilization of a Tekken tokenizer with a 131k vocabulary size. This model is ready for commercial and non-commercial use. Multilingual Capabilities: English, French, German, Japanese, Korean, Chinese, and more. Multilingual Capabilities Third-Party Community Consideration ----------------------------------- This model is not owned or developed by NVIDIA. It has been developed by Mistral AI and built to a third-party’s requirements. For more details, see the Mistral-Small-3.1-24B-Instruct-2503Model Card. Model Card (https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503) License and Terms of Use ------------------------ GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Apache 2.0. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) References ---------- • Mistral-Small-3.1-24B-Base-2503 Mistral-Small-3.1-24B-Base-2503 (https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503) • Mistral AI Blog Post Mistral AI Blog Post (https://mistral.ai/news/mistral-small-3-1/) Deployment Geography -------------------- • Global Use Cases --------- • Fast-response conversational agents • Low-latency function calling • Subject matter expertise via fine-tuning • Local inference for hobbyists and organizations handling sensitive data • Programming and mathematical reasoning • Long document understanding • Visual understanding Release Date ------------ • Huggingface:March 2025 (Link) Huggingface: Link (https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503) • Build.Nvidia.com:April 2025 (Link) Build.Nvidia.com: Link (https://build.nvidia.com/mistralai/mistral-small-3_1-24b-instruct-2503) Model Architecture ------------------ • Architecture Type:Transformer-based Language Model Architecture Type: • Network Architecture:Instruction-tuned, multimodal, Transformer-based Network Architecture: • Base Model:Mistral-Small-3.1-24B-Base-2503 Base Model: • Model Parameters:24 billion Model Parameters: Input ----- • Types:Text, Image Types: • Formats:Text: String. Image: Red, Green, Blue (RGB) Formats: • Parameters:Image: Two-Dimensional (2D). Text: One-Dimensional (1D) Parameters: • Additional Properties:Minimum resolution and pre-processing required for images; token limitation per context window up to 128k. Additional Properties: Output ------ • Types:Text Types: • Formats:String, JSON (function calling) Formats: • Parameters:One-Dimensional (1D) Parameters: • Additional Properties:Post-processing recommended (text formatting, JSON parsing) Additional Properties: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Supported Hardware Microarchitecture Compatibility -------------------------------------------------- • NVIDIA Ampere • NVIDIA Lovelace (e.g., RTX 4090) Preferred/Supported Operating Systems ------------------------------------- • Linux • Windows • macOS Model Versions -------------- • Mistral-Small-3.1-24B-Instruct-2503 v1.0 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset : ================== Data Collection Method by dataset: UndisclosedLabeling Method by dataset: UndisclosedProperties: Undisclosed Testing Dataset: ================ Data Collection Method by dataset: UndisclosedLabeling Method by dataset: UndisclosedProperties: Undisclosed Evaluation Benchmark Results ============================ When available, we report numbers previously published by other model providers, otherwise we re-evaluate them using our own evaluation harness. Pretrain Evals -------------- Small 3.1 24B Base 81.01% 56.03% 37.50% 59.27% 81.30% Instruction Evals ----------------- Text ---- Small 3.1 24B Instruct 44.42% 45.96% 88.41% 10.43% 67.50% 89.00% 82.00% 85.60% Vision ------ Small 3.1 24B Instruct 49.25% 68.91% 94.08% 93.72% 7.3 64.90% 87.20% Multilingual Evals ------------------ Small 3.1 24B Instruct 71.18% 75.30% 69.17% 70.90% Long Context Evals ------------------ Small 3.1 24B Instruct 37.18% 93.96% 91.90% Basic Instruct Template (V7-Tekken) =================================== <system_prompt>,<user message>and<assistant response>are placeholders. Please make sure to usemistral-commonas the source of truth mistral-common (https://github.com/mistralai/mistral-common) Usage ===== The model can be used with the following frameworks; • vllm (recommended): Seehere vllm (recommended) (https://github.com/vllm-project/vllm) here (#vllm) Note 1: We recommend using a relatively low temperature, such astemperature=0.15. Note 1 Note 2: Make sure to add a system prompt to the model to best tailor it for your needs. If you want to use the model as a general assistant, we recommend the following system prompt: Note 2 Inference --------- Engine:vLLM (recommended)Test Hardware: Engine: Test Hardware: • RTX 4090 or equivalent GPU (minimum 55 GB GPU RAM) • NVIDIA L40S Ethical Considerations ---------------------- NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "gemma-2-9b-cpt-sahabatai-instruct",
    "nvidia_url": "https://build.nvidia.com/gotocompany/gemma-2-9b-cpt-sahabatai-instruct",
    "scraped_on": "2025-10-15T21:39:43.838329",
    "tags": [
      "chat",
      "gotocompany",
      "indonesian",
      "regional language generation",
      "sovereign ai",
      "text-to-text"
    ],
    "model_card": "Gemma2 9B CPT Sahabat-AI v1 Instruct Overview ============================================= Description =========== Sahabat-AI(Indonesian language for “close friends”) is a collection of Large Language Models (LLMs) which has been pretrained and instruct-tuned for Indonesian language and its various dialects. Sahabat-AI ecosystem is co-initiated by Indonesian tech and telecommunication companies: GoTo Group and Indosat Ooredoo Hutchison. Sahabat-AI Gemma2 9B CPT Sahabat-AI v1 Instruct is a decoder-based large language model fine-tuned for instruction-following tasks in Indonesian, Javanese, Sundanese, and English. It builds upon the Gemma2 architecture with a context length of 8192 tokens and utilizes the default tokenizer from Gemma-2-9B. (Gemma2 9B CPT Sahabat-AI v1 Instruct Model Card) Gemma2 9B CPT Sahabat-AI v1 Instruct Model Card (https://huggingface.co/GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct?utm_source=chatgpt.com) This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. It has been developed and built to a third-party’s requirements for this application and use case; see theGemma2 9B CPT Sahabat-AI v1 Instruct Model Card. Gemma2 9B CPT Sahabat-AI v1 Instruct Model Card (https://huggingface.co/GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct) License/Terms of Use -------------------- GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service; and the use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:Gemma Terms of Use. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://ai.google.dev/gemma/terms) Gemma Terms of Use (https://ai.google.dev/gemma/terms) Deployment Geography -------------------- Global Use Case -------- This model is intended for developers and researchers focusing on natural language processing tasks in Indonesian and its dialects, including instruction-following, question answering, sentiment analysis, and translation. Release Date ------------ • Hugging Face: November 6, 2024 vialink link (https://huggingface.co/GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct) • Build.NVIDIA.com: 05/14/2025 vialink link (https://build.nvidia.com/gotocompany/gemma-2-9b-cpt-sahabatai-instruct/modelcard) References ========== • SEA HELM (BHASA) Evaluation Benchmark (arxiv.org) arxiv.org (https://arxiv.org/abs/2408.00118?utm_source=chatgpt.com) • IndoMMLU Benchmark (arxiv.org) arxiv.org (https://arxiv.org/abs/2403.08295?utm_source=chatgpt.com) Model Architecture ================== • Architecture Type:Transformer-based Decoder Architecture Type: • Network Architecture:Gemma2 Network Architecture: • Base Model:Gemma2 9B CPT Sahabat-AI v1 Base Model: • Model Parameters:9 billion Model Parameters: Input ===== • Input Type(s):Text Input Type(s): • Input Format(s):String Input Format(s): • Input Parameters:1D Input Parameters: • Other Properties Related to Input:Supports a context length of up to 8192 tokens. Other Properties Related to Input: Output ====== • Output Type(s):Text Output Type(s): • Output Format:String Output Format: • Output Parameters:1D Output Parameters: • Other Properties Related to Output:Generates text sequences based on input prompts. Other Properties Related to Output: Supported Hardware Microarchitecture Compatibility ================================================== • NVIDIA Ampere • NVIDIA Hopper • NVIDIA Turing • NVIDIA Volta • NVIDIA Lovelace Preferred/Supported Operating System(s) ======================================= • Linux • Windows Model Version(s) ================ • Gemma2 9B CPT Sahabat-AI v1 Instruct Training, Testing, and Evaluation Datasets ========================================== Data ==== Gemma2 9B CPT Sahabat-AI v1 Instruct was trained on a wide range of synthetic instructions, alongside publicly available instructions hand-curated by the team with the assistance of native speakers. Training Dataset ================ • Data Collection Method:Hybrid: Automated, Synthetic, Human Data Collection Method: • Labeling Method:Human Labeling Method: • Properties:Approximately 448,000 Indonesian instruction-completion pairs, 96,000 in Javanese, 98,000 in Sundanese, and 129,000 in English. Properties: Testing Dataset =============== • Data Collection Method:Hybrid: Automated, Synthetic, Human Data Collection Method: • Labeling Method:Human Labeling Method: • Properties:Undisclosed Properties: Evaluation Dataset ================== • Benchmark Score:Evaluated on SEA HELM (BHASA), IndoMMLU, and HuggingFace LLM Leaderboard tasks. Benchmark Score: • Data Collection Method:Undisclosed Data Collection Method: Inference ========= • Engine:vLLM Engine: • Test Hardware:NVIDIA L40S Test Hardware: Additional Details: =================== Benchmark Performance --------------------- We evaluated Gemma2 9B CPT Sahabat-AI V1 Instruct on both general language capabilities and instruction-following capabilities. General Language Capabilities ----------------------------- For the evaluation of general language capabilities, we employed the • SEA HELM (also known as BHASA) evaluation benchmarkacross a variety of tasks.These tasks include Question Answering (QA), Sentiment Analysis (Sentiment), Toxicity Detection (Toxicity), Translation in both directions (Eng>Lang & Lang>Eng), Abstractive Summarization (Summ), Causal Reasoning (Causal) and Natural Language Inference (NLI).We also added support for Javanese and Sundanese for the BHASA tasks whenever applicable SEA HELM (also known as BHASA) evaluation benchmark (https://arxiv.org/abs/2309.06085v2) • These tasks include Question Answering (QA), Sentiment Analysis (Sentiment), Toxicity Detection (Toxicity), Translation in both directions (Eng>Lang & Lang>Eng), Abstractive Summarization (Summ), Causal Reasoning (Causal) and Natural Language Inference (NLI). • We also added support for Javanese and Sundanese for the BHASA tasks whenever applicable • IndoMMLUThese tasks include examination questions on Humanities, Indonesian language, Local languages and cultures, Social science and STEM across primary, middle, and high school levels. IndoMMLU (https://arxiv.org/pdf/2310.04928) • These tasks include examination questions on Humanities, Indonesian language, Local languages and cultures, Social science and STEM across primary, middle, and high school levels. • and the common English tasks from theHuggingFace LLM Leaderboard.These tasks consist ofIFEval, BBH, Math Lvl 5, GPQA, MuSR, and MMLU-PRO.Caveat: Our results differ from the HuggingFace LLM Leaderboard because we have usedVLLMas our inference platform. VLLM caps the context size at4096 tokenswhile HuggingFace was set to8192 tokens. HuggingFace LLM Leaderboard (https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) • These tasks consist ofIFEval, BBH, Math Lvl 5, GPQA, MuSR, and MMLU-PRO. IFEval, BBH, Math Lvl 5, GPQA, MuSR, and MMLU-PRO. (https://huggingface.co/docs/leaderboards/open_llm_leaderboard/about) • Caveat: Our results differ from the HuggingFace LLM Leaderboard because we have usedVLLMas our inference platform. VLLM caps the context size at4096 tokenswhile HuggingFace was set to8192 tokens. Caveat VLLM (https://docs.vllm.ai/en/latest/) 4096 tokens 8192 tokens Note: SEA HELM is implemented using prompts to elicit answers in a strict format. For all tasks, the model is expected to provide an answer tag from which the answer is automatically extracted. For tasks where options are provided, the answer should comprise one of the pre-defined options. The scores for each task is normalised to account for baseline performance due to random chance. The evaluation was donezero-shotwith native prompts on a sample of 100-1000 instances for each dataset. zero-shot Instruction-following Capabilities ---------------------------------- Since Gemma2 9B CPT Sahabat-AI v1 Instruct is an instruction-following model, we also evaluated it on instruction-following capabilities with theIFEvaldataset. IFEval (https://arxiv.org/abs/2311.07911) As this dataset was in English, the linguists and native speakers in the team worked together to filter, localize and translate the dataset into the respective target languages to ensure that the examples remained reasonable, meaningful and natural. IFEval IFEval evaluates a model's ability to adhere to constraints provided in the prompt, for example beginning a response with a specific word/phrase or answering with a certain number of sections. Additionally, accuracy is normalized by the proportion of responses in the correct language (if the model performs the task correctly but responds in the wrong language, it is judged to have failed the task). Note: IFEval was only used on Bahasa Indonesia. We are currently working on adding it for Javanese and Sundanese for our upcoming releases. Note Results ------- Indonesian Results ------------------ SEA HELM (also known as BHASA) ------------------------------ IndoMMLU -------- English Results --------------- Gemma2 9B CPT Sahabat-AI v1 Instruct can be run using the 🤗 Transformers library Caveats ------- It is important for users to be aware that the model exhibits certain limitations that warrant consideration. Like many LLMs, the model can generate incorrect responses and occasionally generates irrelevant content, introducing fictional elements that are not grounded in the provided context. Users should also exercise caution in interpreting and validating the model's responses due to the potential inconsistencies in its reasoning. Limitations ----------- Safety ------ Current Sahabat-AI models, including this commercially permissive release, have not been aligned for safety. Developers and users should perform their own safety fine-tuning and related security measures. In no event shall the authors be held liable for any claim, damages, or other liability arising from the use of the released weights and codes. Technical Specifications ------------------------ Fine-Tuning Details ------------------- Gemma2 9B CPT Sahabat-AI v1 Instruct was built using a combination of a full parameter fine-tune, on-policy alignment, and model merges of the best performing checkpoints. The training process for fine-tuning was approximately 4 hours, with alignment taking 2 hours, both on 8x H100-80GB GPUs. Call for Collaboration ====================== Sahabat-AI (Indonesian language for “close friends”) alocal open source Large Language Model (LLM) ecosystem in Indonesian language, co-initiated by Indonesian tech and telecommunication companies: GoTo Group and Indosat Ooredoo Hutchison. Sahabat-AI ecosystem aims to empower Indonesians who want to develop AI-based services and applications using Bahasa Indonesia and its various local dialects. local open source Large Language Model (LLM) ecosystem in Indonesian language We are supported by research centers and global tech experts such as AI Singapore and Tech Mahendra to train the model to gain general language understanding. We also collaborate with key top Indonesia universities such as University of Indonesia, Gadjah Mada University, Bogor Institute of Agriculture, Bandung Institute of Technology, including top Indonesia media groups, such as Kompas Gramedia Group and Republika to train and enrich the model in Bahasa Indonesia, ensuring optimum provision of local context and cultural relevance. We would like to inviteresearchers, developers, and language enthusiaststo actively contribute to the enhancement and expansion of Sahabat-AI. Your collaborations can involve: researchers, developers, and language enthusiasts • Identifying and reporting technical issues • Sharing pre-training, instruction, and preference data • Improving documentation usability • Proposing and implementing new model evaluation tasks and metrics Join us in shaping the future of Sahabat-AI by sharing your expertise and insights to make these models more accessible, accurate, and versatile. You can contribute your ideas throughthis form. this form. (https://docs.google.com/forms/d/1_us969eQtEooYOn4XkvGkdP5VHOyCbO6L_sd9kTMnaA/edit) The Development Team (in ascending alphabetical order) ====================================================== AI Singapore ------------ Chan AdwinCheng NicholasChoa EstherHuang YuliLau WayneLee Chwan RenLeong Wai YiLeong Wei QiLimkonchotiwat PeeratLiu Bing Jie DariusMontalan Jann RaileyNg Boon Cheong RaymondNgui Jian GangNguyen Thanh NganOng BrandonOng Tat-Wee DavidOng Zhi HaoRengarajan HamsawardhiniSiow BryanSusanto YosephineTai Ngee ChiaTan Choon MengTeng WalterTeo Eng Sipp LeslieTeo Wei YiTjhi WilliamYeo Yeow TongYong Xianbin PT GoTo Gojek Tokopedia Tbk --------------------------- Anissa DinintaChau Shiau ChingChoiri Hendra HadhilGoel PriyankSaini Ajay KumarShalev OfirTan DarylTep Kilian RithiTiwari AnupamWidjojo Daniel Acknowledgements ================ AI Singaporeis a national programme supported by the National Research Foundation, Singapore and hosted by the National University of Singapore. AI Singapore Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the National Research Foundation or the National University of Singapore. Contact ======= For more info, please contact us using thisSahabat-AI Inquiry Form. Sahabat-AI Inquiry Form. (https://docs.google.com/forms/d/1_us969eQtEooYOn4XkvGkdP5VHOyCbO6L_sd9kTMnaA/edit) Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and has established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "eurollm-9b-instruct",
    "nvidia_url": "https://build.nvidia.com/utter-project/eurollm-9b-instruct",
    "scraped_on": "2025-10-15T21:39:40.672118",
    "tags": [
      "chat",
      "european",
      "multilingual",
      "regional language generation",
      "sovereign ai",
      "text-to-text",
      "utter-project"
    ],
    "model_card": "EuroLLM-9B-Instruct Overview ============================ Description: ============ The EuroLLM project has the goal of creating a suite of LLMs capable of understanding and generating text in all European Union languages as well as some additional relevant languages. EuroLLM-9B-Instruct is a 9.154 billion parameter multilingual transformer language model developed to understand and generate text across 35 languages, including all 24 official European Union languages and 11 additional languages. It is instruction-tuned on the EuroBlocks dataset, focusing on general instruction-following and machine translation tasks. This model is ready for commercial and non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see theEuroLLM-9B-Instruct Model Card. EuroLLM-9B-Instruct Model Card (https://huggingface.co/utter-project/EuroLLM-9B-Instruct) License and Terms of Use: ------------------------- GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service; and the use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:Apache License Version 2.0. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache License Version 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) Deployment Geography: --------------------- Global Use Case: --------- Designed for multilingual applications such as machine translation, conversational AI, and general-purpose instruction-following tasks across diverse languages. Release Date: ------------- • Hugging Face: December 2024 vialink link (https://huggingface.co/utter-project/EuroLLM-9B-Instruct) • Build.NVIDIA.com: 05/14/2025 vialink link (https://build.nvidia.com/utter-project/eurollm-9b-instruct) Reference(s): ============= • arXiv:2202.03799 arXiv:2202.03799 (https://arxiv.org/abs/2202.03799) • arXiv:2402.17733 arXiv:2402.17733 (https://arxiv.org/abs/2402.17733) • arXiv:2506.04079 arXiv:2506.04079 (https://arxiv.org/abs/2506.04079) Model Architecture: =================== • Architecture Type:Transformer Architecture Type: • Network Architecture:Dense Transformer with Grouped Query Attention (GQA) Network Architecture: • Base Model:EuroLLM-9B Base Model: • Model Parameters:9.154 billion Model Parameters: Input: ====== • Input Type(s):Text Input Type(s): • Input Format(s):String Input Format(s): • Input Parameters:1D Input Parameters: • Other Properties Related to Input:Maximum sequence length of 4,096 tokens; tokenized using a custom tokenizer designed for multilingual support. Other Properties Related to Input: Output: ======= • Output Type(s):Text Output Type(s): • Output Format:String Output Format: • Output Parameters:1D Output Parameters: • Other Properties Related to Output:NA Other Properties Related to Output: Software Integration: ===================== Supported Hardware Microarchitecture Compatibility: =================================================== • NVIDIA Ampere • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace • NVIDIA Pascal Operating System(s): ==================== • Linux Model Version(s): ================= • EuroLLM-9B-Instruct v1.0 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human Labeling Method by dataset: • Properties:Trained on 4 trillion tokens across 35 languages. Properties: Testing Dataset: ================ • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human Labeling Method by dataset: • Properties:Undisclosed Properties: Evaluation Dataset: =================== • Benchmark Score:EuroLLM-9B-Instruct demonstrates competitive performance on multilingual benchmarks, surpassing many European-developed models and matching the performance of models like Mistral-7B. Benchmark Score: • Data Collection Method by dataset:Undisclosed Data Collection Method by dataset: Inference: ========== • Engine:TensorRT-LLM Engine: • Test Hardware:NVIDIA Lovelace L40S Test Hardware: Additional Details: =================== For pre-training, we use 400 Nvidia H100 GPUs of the Marenostrum 5 supercomputer, training the model with a constant batch size of 2,800 sequences, which corresponds to approximately 12 million tokens, using the Adam optimizer, and BF16 precision. Here is a summary of the model hyper-parameters: Run the model ------------- Results ------- EU Languages ------------ Table 1:Comparison of open-weight LLMs on multilingual benchmarks. The borda count corresponds to the average ranking of the models (see (Colombo et al., 2022)). For Arc-challenge, Hellaswag, and MMLU we are using Okapi datasets (Lai et al., 2023) which include 11 languages. For MMLU-Pro and MUSR we translate the English version with Tower (Alves et al., 2024) to 6 EU languages.* As there are no public versions of the pre-trained models, we evaluated them using the post-trained versions. Table 1: Colombo et al., 2022 (https://arxiv.org/abs/2202.03799) Lai et al., 2023 (https://aclanthology.org/2023.emnlp-demo.28/) Alves et al., 2024 (https://arxiv.org/abs/2402.17733) The results in Table 1 highlight EuroLLM-9B's superior performance on multilingual tasks compared to other European-developed models (as shown by the Borda count of 1.0), as well as its strong competitiveness with non-European models, achieving results comparable to Gemma-2-9B and outperforming the rest on most benchmarks. English ------- Table 2:Comparison of open-weight LLMs on English general benchmarks.* As there are no public versions of the pre-trained models, we evaluated them using the post-trained versions. Table 2: The results in Table 2 demonstrate EuroLLM's strong performance on English tasks, surpassing most European-developed models and matching the performance of Mistral-7B (obtaining the same Borda count). Bias, Risks, and Limitations ---------------------------- This model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive. Developers should implement appropriate safety measures and conduct thorough evaluations before deploying the model in production environments. Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "FLUX.1-schnell",
    "nvidia_url": "https://build.nvidia.com/black-forest-labs/flux_1-schnell",
    "scraped_on": "2025-10-15T21:39:40.232281",
    "tags": [
      "black-forest-labs",
      "image generation",
      "run-on-rtx",
      "text-to-image"
    ],
    "model_card": "Overview ======== Description: ============ FLUX.1-schnell is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions at fast speeds. Trained using latent adversarial diffusion distillation, FLUX.1-schnell can generate high-quality images in only 1 to 4 steps. This model is ready for commercial use. Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to: • black-forest-labs/FLUX.1-schnell Model Card black-forest-labs/FLUX.1-schnell Model Card (https://huggingface.co/black-forest-labs/FLUX.1-schnell) Terms of use ------------ GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service. The Flux.1 Schnell model is available athttps://huggingface.co/black-forest-labs/FLUX.1-schnell. Use of the NVIDIA Cosmos-1.0 Guardrail is governed by theNVIDIA Open Model License Agreement. ADDITIONAL INFORMATION:Llama 2 Community License Agreement. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) https://huggingface.co/black-forest-labs/FLUX.1-schnell NVIDIA Open Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Llama 2 Community License Agreement (https://github.com/meta-llama/llama-models/blob/main/models/llama2/LICENSE) Deployment Geography: --------------------- Global Use Case: ========= Creators and professionals can use this model to generate high-quality images from text prompts, simplifying visual communication. Release Date: ------------- August 1, 2024 References ---------- • Flux on Black Forest Labs Flux on Black Forest Labs (https://blackforestlabs.ai/) • Flux blog post Flux blog post (https://blackforestlabs.ai/announcing-black-forest-labs/) Model Architecture: =================== Architecture Type: Transformer and Convolutional Neural Network (CNN)Network Architecture: Diffusion TransformerLiheYoung/Depth-anything-large-hf leverages the DPT architecture with a DINOv2 backbone. Architecture Type Network Architecture Input: ====== Input Type: TextInput Parameters: Text: 1D.Input Format: Text: String.Other Properties Related to Input: Steps, Output Image Aspect Ratio, and Seed per theAPI Reference Page Input Type Input Parameters Input Format Other Properties Related to Input API Reference Page (https://docs.api.nvidia.com/nim/reference/black-forest-labs-flux_1-schnell) Output: ======= Output Type: ImageOutput Parameters: 2DOutput Format: Red, Green, Blue (RGB)Other Properties Related to Output: Supported resolutions 1024x1024, 768x1344, 1344x768, 896x1152, 1152x896, 832x1216, 1216x832 Output Type Output Parameters Output Format Other Properties Related to Output Software Integration: ===================== Runtime Engines: • TensorRT Supported Hardware Platforms: Supported Hardware Platforms • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace Supported Operating Systems: Linux, Windows Subsystem for Linux Supported Operating Systems Model Version(s): ================= • FLUX.1-schnell Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= • Data Collection Method by Dataset: Undisclosed • Labeling Method by Dataset: Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Testing Dataset: ================ • Data Collection Method by Dataset: Undisclosed • Labeling Method by Dataset: Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: =================== • Data Collection Method by Dataset: Undisclosed • Labeling Method by Dataset: Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference: ========== Engine: TensorRTTest Hardware: H100 Engine Test Hardware Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "qwen3-235b-a22b",
    "nvidia_url": "https://build.nvidia.com/qwen/qwen3-235b-a22b",
    "scraped_on": "2025-10-15T21:39:37.085502",
    "tags": [
      "advanced reasoning",
      "chat",
      "complex math",
      "instruction following",
      "qwen"
    ],
    "model_card": "Qwen3-235B-A22B Overview ======================== Description: ============ Qwen3-235B-A22B is the latest generation of large language models in the Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support. It uniquely supports seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within a single model, ensuring optimal performance across various scenarios. The model shows significant enhancement in its reasoning capabilities, surpassing previous versions on mathematics, code generation, and commonsense logical reasoning. It also demonstrates superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Furthermore, it has expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. Qwen3-235B-A22B supports over 100 languages and dialects with strong capabilities for multilingual instruction following and translation. This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed by Qwen (Alibaba Cloud). This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIAQwen3-235B-A22B Model Card. Qwen3-235B-A22B Model Card (https://huggingface.co/Qwen/Qwen3-235B-A22B) License/Terms of Use: --------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Apache 2.0. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) Deployment Geography: --------------------- Global Use Case: --------- This model is expected to be used for a wide range of tasks including: • Complex logical reasoning, mathematics, and code generation (thinking mode). • Efficient, general-purpose dialogue (non-thinking mode). • Instruction-following. • Agent capabilities requiring integration with external tools. • Multilingual applications, including instruction following and translation across 100+ languages. • Creative writing and role-playing. • Multi-turn dialogues. Release Date: ------------- May 2025 May 2025 (https://huggingface.co/Qwen/Qwen3-235B-A22B) Reference(s): ============= • Qwen3-235B-A22B Hugging Face Model Card:https://huggingface.co/Qwen/Qwen3-235B-A22B https://huggingface.co/Qwen/Qwen3-235B-A22B • Qwen3: Think Deeper, Act Faster (blog):https://qwenlm.github.io/blog/qwen3/ https://qwenlm.github.io/blog/qwen3/ • Qwen3: Github:https://github.com/QwenLM/Qwen3 https://github.com/QwenLM/Qwen3 • Qwen3: Documentation:https://qwen.readthedocs.io/en/latest/ https://qwen.readthedocs.io/en/latest/ Model Architecture: =================== Architecture Type:Causal Language Model, Mixture-of-Experts (MoE)Network Architecture:Qwen3 Architecture Type: Network Architecture: • This model was developed based on the Qwen series architecture. • This model has 235B total parameters and 22B activated parameters. • Number of Parameters (Non-Embedding): 234B • Number of Layers: 94 • Number of Attention Heads (GQA): 64 for Query (Q) and 4 for Key/Value (KV) • Number of Experts: 128 • Number of Activated Experts: 8 Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:One-Dimensional (1D) for text.Other Properties Related to Input: Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: • Context Length: 32,768 tokens natively. Can be extended to 131,072 tokens with YaRN. • Pre-Processing Needed: Tokenization using the Qwen3 tokenizer. Input can be formatted for \"thinking mode\" (enable_thinking=True) or \"non-thinking mode\" (enable_thinking=False). The tokenizer by default hasenable_thinking=True. Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:One-Dimensional (1D) for text.Other Properties Related to Output: Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: • Recommended output length: 32,768 tokens for most queries. • For benchmarking on highly complex problems (e.g., math, programming competitions), a max output length of 38,912 tokens is suggested. • Post-Processing Needed: Detokenization. Parsing of thinking content might be needed ifenable_thinking=True. Software Integration: ===================== Runtime Engine(s): • Hugging Face Transformers (version >= 4.51.0 recommended) • vLLM (version >= 0.8.5 for OpenAI-compatible API endpoint with reasoning parser) • SGLang (version >= 0.4.6.post1 for OpenAI-compatible API endpoint with reasoning parser) • Other platforms supporting Qwen3 include: Ollama, LMStudio, MLX-LM, llama.cpp, KTransformers. Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere or newer GPUs are highly recommended for optimal performance due to the model size.-This model can be run on various GPU setups, potentially including NVIDIA Hopper, NVIDIA Blackwell, NVIDIA Lovelace, NVIDIA Turing, and NVIDIA Volta, depending on the quantization (e.g., FP8 version mentioned as Qwen3-235B-A22B-FP8) and distributed inference setup. Specific requirements depend on the inference framework and desired throughput/latency. [Preferred/Supported] Operating System(s): • Linux • Windows (via Python environments) • macOS (via Python environments)(Generally OS independent as long as the Python environment and necessary drivers are supported). Model Version(s): ================= Qwen3-235B-A22B v1.0 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset : ================== Data Collection Method by dataset: UndisclosedLabeling Method by dataset: UndisclosedProperties: Undisclosed Testing Dataset: ================ Data Collection Method by dataset: UndisclosedLabeling Method by dataset: UndisclosedProperties: Undisclosed Evaluation Benchmark Results ============================ Data Collection Method by dataset: UndisclosedLabeling Method by dataset: UndisclosedProperties: Undisclosed For more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to Qwen3blog,GitHub, andDocumentation. blog (https://qwenlm.github.io/blog/qwen3/) GitHub (https://github.com/QwenLM/Qwen3) Documentation (https://qwen.readthedocs.io/en/latest/) Inference: ========== Engine:SGLangTest Hardware: Engine: Test Hardware: • NVIDIA Hopper Additional content ================== • Qwen3 Highlights:Qwen3 is the latest generation of LLMs in the Qwen series, offering significant advancements in reasoning, instruction-following, agent capabilities, and multilingual support. Qwen3 Highlights: • Thinking Mode:The model supports a \"thinking mode\" (enable_thinking=True) for complex tasks, which enhances response quality through internal reasoning steps. It can be switched to a \"non-thinking mode\" (enable_thinking=False) for general dialogue. Thinking Mode: • Quickstart:Code snippets are available on the Hugging Face model card to demonstrate how to load the tokenizer and model, prepare inputs, and generate text. Quickstart: • Context Extension (YaRN):The model supports YaRN (Yet another RoPE extensioN method) to extend its context window from 32,768 tokens to 131,072 tokens. Configuration details are provided in the model card. Context Extension (YaRN): • Recommended Sampling Parameters:For thinking mode (enable_thinking=True): Temperature=0.6, TopP=0.95, TopK=20, MinP=0. Greedy decoding is not recommended.For non-thinking mode (enable_thinking=False): Temperature=0.7, TopP=0.8, TopK=20, MinP=0.presence_penaltycan be adjusted (0 to 2) to reduce repetition. Recommended Sampling Parameters: • For thinking mode (enable_thinking=True): Temperature=0.6, TopP=0.95, TopK=20, MinP=0. Greedy decoding is not recommended. • For non-thinking mode (enable_thinking=False): Temperature=0.7, TopP=0.8, TopK=20, MinP=0. • presence_penaltycan be adjusted (0 to 2) to reduce repetition. • Output Length:Recommended output length is 32,768 tokens, extendable to 38,912 tokens for complex problems. Output Length: • Standardized Output Format for Benchmarking:Prompts can be used to standardize model outputs, e.g., \"Please reason step by step, and put your final answer within \\boxed{}.\", or using JSON for multiple-choice questions. Standardized Output Format for Benchmarking: • Multi-turn Conversation History:In multi-turn conversations, the historical model output should only include the final output part, not the thinking content. This is handled by the provided Jinja2 chat template. Multi-turn Conversation History: • Qwen-Agent:Qwen-Agent capabilities are integrated, simplifying tool-calling and integration with external tools. Qwen-Agent: Ethical Considerations: ======================= NVIDIA believes Trustworthy Al is a shared responsibility and we have established policies and practices to enable development for a wide array of Al applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "granite-3.3-8b-instruct",
    "nvidia_url": "https://build.nvidia.com/ibm/granite-3_3-8b-instruct",
    "scraped_on": "2025-10-15T21:39:36.702913",
    "tags": [
      "chat",
      "coding",
      "ibm",
      "instruction following",
      "reasoning"
    ],
    "model_card": "Granite-3.3-8B-Instruct Overview ================================ Description =========== Granite-3.3-8B-Instruct is an 8-billion parameter language model with a 128K context length, fine-tuned for enhanced reasoning and instruction-following capabilities. Built upon Granite-3.3-8B-Base, it demonstrates significant improvements on benchmarks such as AlpacaEval-2.0 and Arena-Hard, as well as in tasks involving mathematics, coding, and instruction adherence. The model supports structured reasoning through the use of<think>and<response>tags, enabling a clear separation between internal thoughts and final outputs. It has been trained on a carefully balanced combination of permissively licensed data and curated synthetic tasks. This model is ready for commercial and non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. It has been developed and built to IBM's requirements for this application and use case; see theGranite-3.3-8B-Instruct Model Card Granite-3.3-8B-Instruct Model Card (https://huggingface.co/ibm-granite/granite-3.3-8b-instruct) License and Terms of Use ------------------------ GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Apache 2.0. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache 2.0 (https://www.apache.org/licenses/LICENSE-2.0) Deployment Geography -------------------- Global Use Case -------- Designed to handle general instruction-following tasks, Granite-3.3-8B-Instruct can be integrated into AI assistants across various domains, including business applications. Supported Languages:English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. However, users may finetune this Granite model for languages beyond these 12 languages. Supported Languages: Capabilities • Thinking • Summarization • Text classification • Text extraction • Question-answering • Retrieval Augmented Generation (RAG) • Code related tasks • Function-calling tasks • Multilingual dialog use cases • Long-context tasks including long document/meeting summarization, long document QA, etc. Release Date ------------ • Hugging Face: April 16, 2025 • Build.nvidia.com: May, 2025 References ========== • Granite-3.3-8B-Instruct on Hugging Face Granite-3.3-8B-Instruct on Hugging Face (https://huggingface.co/ibm-granite/granite-3.3-8b-instruct) • Website:Granite Docs Granite Docs (https://www.ibm.com/granite/docs/) • ⭐️ Learn about the latest updates with Granite:https://www.ibm.com/granite https://www.ibm.com/granite • 📄 Get started with tutorials, best practices, and prompt engineering advice:https://www.ibm.com/granite/docs/ https://www.ibm.com/granite/docs/ • 💡 Learn about the latest Granite learning resources:https://github.com/ibm-granite-community/ https://github.com/ibm-granite-community/ Model Architecture ================== • Architecture Type:Transformer (Decoder-only) Architecture Type: • Network Architecture:Granite Network Architecture: • Base Model:Granite-3.3-8B-Base Base Model: • Model Parameters:8 billion Model Parameters: Input ===== • Input Type:Text Input Type: • Input Format:String Input Format: • Input Parameters:One-Dimensional (1D) Input Parameters: • Other Properties Related to Input:Supports up to 128K tokens context length. Other Properties Related to Input: Output ====== • Output Type:Text Output Type: • Output Format:String Output Format: • Output Parameters:One-Dimensional (1D) Output Parameters: • Other Properties Related to Output:Capable of generating structured responses using<think>and<response>tags. Other Properties Related to Output: Preferred/Supported Operating System(s) ======================================= • Linux • Windows Model Version(s) ================ • Granite-3.3-8B-Instruct v1.0 Training, Testing, and Evaluation Datasets ========================================== Training Dataset ---------------- • Link:Undisclosed Link: • Data Collection Method:Undisclosed Data Collection Method: • Labeling Method:Undisclosed Labeling Method: • Properties:Undisclosed Properties: Testing Dataset --------------- • Link:Undisclosed Link: • Data Collection Method:Undisclosed Data Collection Method: • Labeling Method:Undisclosed Labeling Method: • Properties:Undisclosed Properties: Evaluation Dataset ------------------ • Benchmark Score:Undisclosed Benchmark Score: • Data Collection Method:Undisclosed Data Collection Method: • Labeling Method:Undisclosed Labeling Method: • Properties:Undisclosed Properties: Evaluation Results: 1 (#fn1) 2 (#fn2) 3 (#fn3) Inference ========= • Engine:vLLM Engine: • Test Hardware:NVIDIA Ada Lovelace Test Hardware: Additional Details ================== Generation:This is a simple example of how to use Granite-3.3-8B-Instruct model. Generation: Install the following libraries: Then, copy the snippet from the section that is relevant for your use case. Example Outputs • thinking=True • thinking=False Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and has established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "magpie-tts-zeroshot",
    "nvidia_url": "https://build.nvidia.com/nvidia/magpie-tts-zeroshot",
    "scraped_on": "2025-10-15T21:39:36.327658",
    "tags": [
      "nvidia",
      "nvidia nim",
      "nvidia riva",
      "text-to-speech",
      "tts"
    ],
    "model_card": "Overview Speech Synthesis: Magpie TTS Zeroshot Model Overview ==================================================== Description: ============ The Magpie TTS Zeroshot model converts text into audio (speech). Magpie TTS is a generative model, designed to be used as the first part of a neural text-to-speech system in conjunction with an audio codec model. This model uses the International Phonetic Alphabet (IPA) for inference and training, and it can output a female or a male voice for English-US and European-Spanish. In addition, it uses character-based encoding for French. Audio Codec is a neural codec model for speech applications. It is the second part of a two-stage speech synthesis pipeline. This model is ready for commercial use. License/Terms of Use: --------------------- NVIDIA AI Foundation Models Community License Agreement NVIDIA AI Foundation Models Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) References: =========== TTS model papers:Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free GuidanceImproving Robustness of LLM-based Speech Synthesis by Learning Monotonic AlignmentLow Frame-rate Speech Codec: A Codec Designed for Fast High-quality Speech LLM Training and Inference Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance (https://arxiv.org/pdf/2502.05236) Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment (https://arxiv.org/pdf/2406.17957) Low Frame-rate Speech Codec: A Codec Designed for Fast High-quality Speech LLM Training and Inference (https://arxiv.org/abs/2409.12117) Model Architecture: =================== Network Architecture: T5-TTS + Audio codec T5-TTS is an encoder-decoder transformer model for text-to-speech synthesis that improves robustness by learning monotonic alignment between text and speech tokens. The model takes text tokens and reference audio codes as input and autoregressively predicts acoustic tokens of the target speech. Low Frame Rate Speech Codec - 21Hz is a neural audio compression model that quantizes speech or audio signals into discrete tokens at a low temporal rate of 21 frames per second. The model typically employs a multi-stage encoding process to compress the input audio into a sequence of discrete codes while preserving essential acoustic characteristics despite the aggressive temporal compression. During encoding, it analyzes longer windows of audio to capture relevant acoustic features before downsampling to 21Hz, and during decoding, it uses neural upsampling techniques to reconstruct high-fidelity audio at the original sampling rate. This lower frame rate allows for efficient storage and transmission while still maintaining reasonable audio quality for applications like speech synthesis and audio compression. Input: ====== Input Type:TextInput Format:Strings (Graphemes in US English)Input Parameters:One-Dimensional (1D) Input Type: Input Format: Input Parameters: Output: ======= Output Type:AudioOutput Format:Audio of shape (batch x time) in wav formatOutput Parameters:Mono, PCM-encoded 16 bit audio; sampling rate of 22.05 kHz; 20 Second Maximum Length; Depending on input, this model can output a female or a male voice for English US with two (2) emotions for the female voice and six (6) emotions for male voices. The female voice is classified as “neutral” and “calm.” The male voice is classified as “neutral,” “calm,” “happy,” and “fearful”, “sad”, and “angry.” Output Type: Output Format: Output Parameters: Software Integration: ===================== Runtime Engine(s):Riva 2.19.0 or greater Runtime Engine(s): Supported Hardware Platform(s): • NVIDIA Turing T4 • NVIDIA A100 GPU • NVIDIA A30 GPU • NVIDIA A10 GPU • NVIDIA H100 GPU • NVIDIA L4 GPU • NVIDIA L40 GPU Supported Operating System(s): • Linux Inference ========= Engine:TritonTest Hardware: Engine: Test Hardware: • NVIDIA Turing T4 • NVIDIA A100 GPU • NVIDIA A30 GPU • NVIDIA A10 GPU • NVIDIA H100 GPU • NVIDIA L4 GPU • NVIDIA L40 GPU Model Version(s): ================= magpie-tts-zeroshot v1 Ethical Considerations (For NVIDIA Models Only): ================================================ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) GOVERNING TERMS: ================ This trial is governed by theNVIDIA API Trial Terms of Service. The use of this model is governed by theAI Foundation Models Community License Agreement NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) AI Foundation Models Community License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/)"
  },
  {
    "name": "marin-8b-instruct",
    "nvidia_url": "https://build.nvidia.com/marin/marin-8b-instruct",
    "scraped_on": "2025-10-15T21:39:35.835320",
    "tags": [
      "chat",
      "marin",
      "math",
      "open model",
      "reasoning",
      "science"
    ],
    "model_card": "Marin 8B Instruct Overview ========================== Description: ============ Marin 8B Instruct is a Transformer-style autoregressive language model, fine-tuned from marin-8b-base, designed to follow instructions and engage in dialogue. This model is intended for tasks such as question answering, summarization, code generation, and dialogue. • Developed by:The Marin team at Stanford CRFM. Developed by: • Model type:a Transformer style autoregressive language model. Model type: • Knowledge Cutoff:~July 2024 Knowledge Cutoff: • Language(s) (NLP):English Language(s) (NLP): • License:The code and model are released under Apache 2.0. License: • Contact:dlwh at stanford.edu Contact: This model is ready for non-commercial/research use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIAmarin-8b-instruct Model Card. marin-8b-instruct Model Card (https://huggingface.co/marin-community/marin-8b-instruct) License and Terms of use: ------------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Apache 2.0. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache 2.0 (https://www.apache.org/licenses/LICENSE-2.0) Deployment Geography: --------------------- Global Use Case: --------- The Marin 8B Instruct model is designed for tasks requiring instruction comprehension and generation, such as question answering, summarization, code generation, and dialogue. It is positioned as a research artifact or a foundational instruct model upon which others can build and implement their own safety protocols. Release Date: ------------- • Build.nvidia.com: May 2025 • Huggingface: May 2025 Reference(s): ============= • Marin Community Hugging Face:https://huggingface.co/marin-community Marin Community Hugging Face: https://huggingface.co/marin-community • marin-8b-instruct Hugging Face:https://huggingface.co/marin-community/marin-8b-instruct marin-8b-instruct Hugging Face: https://huggingface.co/marin-community/marin-8b-instruct • Stanford CRFM:https://crfm.stanford.edu/ Stanford CRFM: https://crfm.stanford.edu/ • Levanter GitHub:https://github.com/stanford-crfm/levanter Levanter GitHub: https://github.com/stanford-crfm/levanter Model Architecture: =================== • Architecture Type:Transformer (Autoregressive Language Model) Architecture Type: • Network Architecture:Llama 3 8B Network Architecture: • This model was developed based onmarin-8b-base. This model was developed based on • This model has8.03 billionmodel parameters.Hidden Size: 4096Feedforward Size: 14336Number of Layers: 32Number of Attention Heads: 32Number of Key-Value (KV) Heads: 8 (Grouped-Query Attention) This model has model parameters. • Hidden Size: 4096 • Feedforward Size: 14336 • Number of Layers: 32 • Number of Attention Heads: 32 • Number of Key-Value (KV) Heads: 8 (Grouped-Query Attention) Input: ====== • Input Type(s):Text Input Type(s): • Input Format(s):String Input Format(s): • Input Parameters:1D Input Parameters: • Other Properties Related to Input:4K Context Window Length Other Properties Related to Input: Output: ======= • Output Type(s):Text Output Type(s): • Output Format:String Output Format: • Output Parameters:1D Output Parameters: • Other Properties Related to Output:Generates text based on input instructions. Knowledge cutoff is around July 2024. Other Properties Related to Output: Our Al models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. [Preferred/Supported] Operating System(s): • Linux • Windows • MacOS (via Hugging Face Transformers library compatibility) Model Version(s): ================= marin-8b-instruct v1.0 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ----------------- The Marin-8b-Instruct model was adapted from marin-8b-base through Supervised Fine-Tuning (SFT) for an additional 5.3 billion tokens. • Data Collection Method by dataset:Hybrid: Automated, Human Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human Labeling Method by dataset: • Properties:Undisclosed Properties: Datasets used in Marin 8B Base ------------------------------ Nemotron-CC Nemotron-CC (https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html) • DCLM Baseline DCLM Baseline (https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0) • Starcoder Data Starcoder Data (https://huggingface.co/datasets/bigcode/starcoderdata) • Proofpile 2 Proofpile 2 (https://huggingface.co/datasets/EleutherAI/proof-pile-2) • FineMath3+ FineMath (https://huggingface.co/datasets/HuggingFaceTB/finemath) • Dolma, including their versions of:MegaWikapeS2o(And most of the rest of it) Dolma (https://huggingface.co/datasets/allenai/dolma) • MegaWika MegaWika (https://huggingface.co/datasets/hltcoe/megawika) • peS2o peS2o (https://huggingface.co/datasets/allenai/peS2o) • (And most of the rest of it) • Dolmino-Mix-1124, including their versions of:FLANCodeSearchNet(with OWM Filter)GSM8KMetaMathMathCoder2 Synthetic Dolmino-Mix-1124 (https://huggingface.co/datasets/allenai/dolmino-mix-1124) • FLAN FLAN (https://arxiv.org/abs/2109.01652) • CodeSearchNet(with OWM Filter) CodeSearchNet (https://arxiv.org/abs/1909.09436) • GSM8K GSM8K (https://arxiv.org/pdf/2110.14168v1) • MetaMath MetaMath (https://arxiv.org/abs/2309.12284) • MathCoder2 Synthetic MathCoder2 Synthetic (https://arxiv.org/abs/2310.03731) A full report is available onour ReadTheDocs site. our ReadTheDocs site (https://marin.readthedocs.org/en/latest/reports/marin-8b-retro.html) Datasets used in Marin 8B Instruct ---------------------------------- Marin 8B Instruct is currently an SFT-only model. It was trained on the following datasets: • TIGER-Lab/AceCode-89K TIGER-Lab/AceCode-89K (https://huggingface.co/datasets/TIGER-Lab/AceCode-89K) • bespokelabs/Bespoke-Stratos-17k bespokelabs/Bespoke-Stratos-17k (https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k) • cognitivecomputations/dolphin-r1(includes both nonreasoning and reasoning subsets) cognitivecomputations/dolphin-r1 (https://huggingface.co/datasets/cognitivecomputations/dolphin-r1) • tuenguyen/dolphin_r1_reasoning tuenguyen/dolphin_r1_reasoning (https://huggingface.co/datasets/tuenguyen/dolphin_r1_reasoning) • facebook/natural_reasoning facebook/natural_reasoning (https://huggingface.co/datasets/facebook/natural_reasoning) • open-r1/OpenThoughts-114k-math open-r1/OpenThoughts-114k-math (https://huggingface.co/datasets/open-r1/OpenThoughts-114k-math) • HuggingFaceTB/smoltalk HuggingFaceTB/smoltalk (https://huggingface.co/datasets/HuggingFaceTB/smoltalk) • allenai/tulu-3-sft-mixture allenai/tulu-3-sft-mixture (https://huggingface.co/datasets/allenai/tulu-3-sft-mixture) • PrimeIntellect/verifiable-math-problems PrimeIntellect/verifiable-math-problems (https://huggingface.co/datasets/PrimeIntellect/verifiable-math-problems) Testing Dataset: ---------------- • Data Collection Method:Undisclosed Data Collection Method: • Labeling Method:Undisclosed Labeling Method: • Properties:Undisclosed Properties: Evaluation Dataset: ------------------- • Data Collection Method:Undisclosed Data Collection Method: • Labeling Method:Undisclosed Labeling Method: • Properties:Undisclosed Properties: Base Model Evaluation Results ----------------------------- We ran a suite of standard benchmarks to compare our model withLlama 3.1 8B, and the open source 7-8B modelsOlmo 2 7B, andMAP NEO 7B. For all benchmarks, we usedLM Eval Harnesswith the default setup for each task. (These numbers may differ from reported results due to differences in setup. LM Eval Harness is usually somewhat stricter than other harnesses.) Llama 3.1 8B (https://huggingface.co/meta-llama/Meta-Llama-3.1-8B) Olmo 2 7B (https://huggingface.co/allenai/OLMo-2-1124-7B) MAP NEO 7B (https://huggingface.co/m-a-p/neo_7b) LM Eval Harness (https://github.com/EleutherAI/lm-evaluation-harness) 68.3 86.5 63.1 50.6 85.9 92.0 82.3 83.6 74.7 67.6 65.9 36.5 84.4 74.5 92.0 32.3 74.7 46.2 86.1 23.0 81.7 Marin 8B Base fares well on most tasks. Inference: ========== • Engine:TensorRT-LLM Engine: • Test Hardware:L40s Test Hardware: Additional Information ====================== • Developed by:The Marin Project / Marin Community, closely associated with Stanford University's Center for Research on Foundation Models (CRFM). Developed by: • Primary Contact:David Hall (dlwh at stanford.edu) is listed as the primary contact for the Marin 8B models on their Hugging Face model cards. Primary Contact: • Training Framework:Developed using thestanford-crfm/levantertraining framework, which uses JAX and Named Tensors. Training Framework: • Training Logs:Public Weights & Biases (W&B) logs are available for the Marin 8B training runs. Training Logs: • Tokenizer:stanford-crfm/marin-tokenizer(variant of Llama 3 tokenizer). Tokenizer: • Philosophy:The Marin Community operates as \"an open lab for building foundation models collaboratively,\" emphasizing open sharing of source code, datasets, experimental methodologies, and mistakes. Philosophy: • Distinction:Marin Community (AI research project) is distinct from Marin Software (digital advertising company). Distinction: • Training Checkpoints (for base model):Kestrel, Ocelot, Jellyfish, Phoenix, Starling, and deeper-starling (13.7T tokens). Training Checkpoints (for base model): Bias, Risks, and Limitations ============================ Like any base language model or fine-tuned model without safety filtering, these models can easily be prompted by users to generate harmful and sensitive content. Such content may also be produced unintentionally, especially in cases involving bias, so we recommend that users consider the risks when applying this technology. Additionally, many statements from Marin or any LLM are often inaccurate, so responses should be verified. Marin 8B has not undergone any safety tuning or evaluation. We strongly recommend that users use this model with caution and consider the risks when applying this technology. In particular, this model is not intended for fully autonomous use. Ethical Considerations: ======================= NVIDIA believes Trustworthy Al is a shared responsibility and we have established policies and practices to enable development for a wide array of Al applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.1-nemotron-nano-4b-v1.1",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-4b-v1_1",
    "scraped_on": "2025-10-15T21:39:35.350241",
    "tags": [
      "chat",
      "edge",
      "math",
      "nvidia",
      "reasoning",
      "tool calling"
    ],
    "model_card": "Llama-3.1-Nemotron-Nano-4B-v1.1 =============================== Model Overview ============== Llama-3.1-Nemotron-Nano-4B-v1.1 is a large language model (LLM) which is a derivative ofnvidia/Llama-3.1-Minitron-4B-Width-Base, which is created from Llama 3.1 8B usingour LLM compression techniqueand offers improvements in model accuracy and efficiency. It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling. nvidia/Llama-3.1-Minitron-4B-Width-Base (https://huggingface.co/nvidia/Llama-3.1-Minitron-4B-Width-Base) our LLM compression technique (https://arxiv.org/abs/2408.11796) Llama-3.1-Nemotron-Nano-4B-v1.1 is a model which offers a great tradeoff between model accuracy and efficiency. The model fits on a single RTX GPU and can be used locally. The model supports a context length of 128K. This model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, and Tool Calling as well as multiple reinforcement learning (RL) stages using Reward-aware Preference Optimization (RPO) algorithms for both chat and instruction-following. The final model checkpoint is obtained after merging the final SFT and RPO checkpoints This model is part of the Llama Nemotron Collection. You can find the other model(s) in this family here: • Llama-3.3-Nemotron-Ultra-253B-v1 Llama-3.3-Nemotron-Ultra-253B-v1 (https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1) • Llama-3.3-Nemotron-Super-49B-v1 Llama-3.3-Nemotron-Super-49B-v1 (https://huggingface.co/nvidia/Llama-3.3-Nemotron-Super-49B-v1) • Llama-3.1-Nemotron-Nano-8B-v1 Llama-3.1-Nemotron-Nano-8B-v1 (https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1) This model is ready for commercial use. License/Terms of Use ==================== GOVERNING TERMS: Your use of this model is governed by theNVIDIA Open Model License. Additional Information:Llama 3.1 Community License Agreement. Built with Llama. NVIDIA Open Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Llama 3.1 Community License Agreement (https://www.llama.com/llama3_1/license/) Model Developer:NVIDIA Model Developer: Model Dates:Trained between August 2024 and April 2025 Model Dates: Data Freshness:The pretraining data has a cutoff of 2023 per Meta Llama 3.1 8B Data Freshness: Use Case: ========= Developers designing AI Agent systems, chatbots, RAG systems, and other AI-powered applications. Also suitable for typical instruction-following tasks. Balance of model accuracy and compute efficiency (the model fits on a single RTX GPU and can be used locally). Release Date: ============= 5/20/2025 References ========== • [2502.00203] Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment [2502.00203] Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment (https://arxiv.org/abs/2502.00203) Model Architecture ================== Architecture Type:Dense decoder-only Transformer model Architecture Type: Network Architecture:Llama 3.1 Minitron Width 4B Base Network Architecture: Intended use ============ Llama-3.1-Nemotron-Nano-4B-v1.1 is a general purpose reasoning and chat model intended to be used in English and coding languages. Other non-English languages (German, French, Italian, Portuguese, Hindi, Spanish, and Thai) are also supported. Input: ====== • Input Type:Text Input Type: • Input Format:String Input Format: • Input Parameters:One-Dimensional (1D) Input Parameters: • Other Properties Related to Input:Context length up to 131,072 tokens Other Properties Related to Input: Output: ======= • Output Type:Text Output Type: • Output Format:String Output Format: • Output Parameters:One-Dimensional (1D) Output Parameters: • Other Properties Related to Output:Context length up to 131,072 tokens Other Properties Related to Output: Model Version: ============== 1.1 (5/20/2025) Software Integration ==================== • Runtime Engine:NeMo 24.12 Runtime Engine: • Recommended Hardware Microarchitecture Compatibility:NVIDIA HopperNVIDIA Ampere Recommended Hardware Microarchitecture Compatibility: • NVIDIA Hopper • NVIDIA Ampere Quick Start and Usage Recommendations: ====================================== • Reasoning mode (ON/OFF) is controlled via the system prompt, which must be set as shown in the example below. All instructions should be contained within the user prompt • We recommend setting temperature to0.6, and Top P to0.95for Reasoning ON mode • We recommend using greedy decoding for Reasoning OFF mode • We have provided a list of prompts to use for evaluation for each benchmark where a specific template is required You can try this model out through the preview API, using this link:Llama-3_1-Nemotron-Nano-4B-v1.1. Llama-3_1-Nemotron-Nano-4B-v1.1 (https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-4b-v1_1) Inference: ========== Engine:TRT-LLMTest Hardware: Engine: Test Hardware: • A100 • A10g • H100 • L40s Preferred/Supported Operating System(s):Linux Preferred/Supported Operating System(s): Training Datasets ================= A large variety of training data was used for the post-training pipeline, including manually annotated data and synthetic data. The data for the multi-stage post-training phases for improvements in Code, Math, and Reasoning is a compilation of SFT and RL data that supports improvements of math, code, general reasoning, and instruction following capabilities of the original Llama instruct model. Prompts have been sourced from either public and open corpus or synthetically generated. Responses were synthetically generated by a variety of models, with some prompts containing responses for both Reasoning On and Off modes, to train the model to distinguish between two modes. Data Collection for Training Datasets: • Hybrid: Automated, Human, Synthetic Data Labeling for Training Datasets: • N/A Evaluation Datasets =================== We used the datasets listed below to evaluate Llama-3.1-Nemotron-Nano-8B-v1. Data Collection for Evaluation Datasets:Hybrid: Human/Synthetic Data Collection for Evaluation Datasets: Data Labeling for Evaluation Datasets:Hybrid: Human/Synthetic/Automatic Data Labeling for Evaluation Datasets: Evaluation Results ================== These results contain both “Reasoning On”, and “Reasoning Off”. We recommend using temperature=0.6, top_p=0.95for “Reasoning On” mode, and greedy decoding for “Reasoning Off” mode. All evaluations are done with 32k sequence length. We run the benchmarks up to 16 times and average the scores to be more accurate. NOTE: Where applicable, a Prompt Template will be provided. While completing benchmarks, please ensure that you are parsing for the correct output format as per the provided prompt in order to reproduce the benchmarks seen below. MT-Bench -------- MATH500 ------- User Prompt Template: AIME25 ------ User Prompt Template: GPQA-D ------ User Prompt Template: IFEval ------ BFCL v2 Live ------------ User Prompt Template: MBPP 0-shot ----------- User Prompt Template: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards, which you can find by clicking the ModelCard++ tab above, next to Overview. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "bielik-11b-v2.3-instruct",
    "nvidia_url": "https://build.nvidia.com/speakleash/bielik-11b-v2_3-instruct",
    "scraped_on": "2025-10-15T21:39:32.225203",
    "tags": [
      "chat",
      "chatbots",
      "polish",
      "sovereign ai",
      "speakleash",
      "summarization"
    ],
    "model_card": "Bielik-11B-v2.3-Instruct Overview ================================= Description: ============ Bielik-11B-v2.3-Instruct is a generative text model featuring 11 billion parameters, designed to process and understand the Polish language with high precision. It is a linear merge of the Bielik-11B-v2.0-Instruct, Bielik-11B-v2.1-Instruct, and Bielik-11B-v2.2-Instruct models, which are instruct fine-tuned versions of the Bielik-11B-v2. As a result, the model exhibits an exceptional ability to understand and process the Polish language, providing accurate responses and performing a variety of linguistic tasks with high precision. This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIABielik-11B-v2.3-Instruct Model Card. Bielik-11B-v2.3-Instruct Model Card (https://huggingface.co/speakleash/Bielik-11B-v2.3-Instruct) License/Terms of Use: --------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Apache 2.0;Bielik Terms of Use. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) Bielik Terms of Use (https://bielik.ai/terms/) Deployment Geography: --------------------- Global (primarily Polish language, with English capabilities). Use Case: --------- This model can be utilized for a variety of Polish language processing tasks, such as text generation, question answering, language modeling, and chatbots. Its strong performance on benchmarks suggests it could be a valuable resource for natural language processing projects and applications targeting the Polish market or requiring high-quality Polish language understanding and generation. It can also be used for English language tasks. Release Date: ------------- • Build.Nvidia.com 05/28/2025 viahttps://build.nvidia.com/speakleash/bielik-11b-v2_3-instruct https://build.nvidia.com/speakleash/bielik-11b-v2_3-instruct • Huggingface 08/30/2024 viahttps://huggingface.co/speakleash/Bielik-11B-v2.3-Instruct https://huggingface.co/speakleash/Bielik-11B-v2.3-Instruct Reference(s): ============= • Technical report:https://arxiv.org/abs/2505.02410 Technical report: https://arxiv.org/abs/2505.02410 • Chat Arena:https://arena.speakleash.org.pl/ Chat Arena: https://arena.speakleash.org.pl/ • ALLaMo framework:Implemented by Krzysztof Ociepa for training LLaMA and Mistral-like models. ALLaMo framework: Model Architecture: =================== • Architecture Type:Causal decoder-only (Transformer-based) Architecture Type: • Network Architecture:Based on Mistral, LLaMA-like architecture. Network Architecture: • This model was developed based ona linear merge of Bielik-11B-v2.0-Instruct, Bielik-11B-v2.1-Instruct, and Bielik-11B-v2.2-Instruct, which are fine-tuned versions of Bielik-11B-v2. Bielik-11B-v2 was initialized from Mistral-7B-v0.2. This model was developed based on • This model has11 billionmodel parameters. This model has model parameters. Input: ====== • Input Type(s):Text Input Type(s): • Input Format(s):String Input Format(s): • Input Parameters:One-Dimensional (1D) for text. Input Parameters: • Other Properties Related to Input:Context Length: 32,768 tokens natively. Other Properties Related to Input: • Context Length: 32,768 tokens natively. Output: ======= • Output Type(s):Text Output Type(s): • Output Format:String. Output Format: • Output Parameters:One-Dimensional (1D) for text. Output Parameters: • Other Properties Related to Output:Languages: Polish, English. Other Properties Related to Output: • Languages: Polish, English. Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration: ===================== Runtime Engine(s): Runtime Engine(s) • TensorRT-LLM 0.17.2 Runtime Engine(s): Runtime Engine(s) • NVIDIA Lovelace • NVIDIA Hopper [Preferred/Supported] Operating System(s): ========================================== • [Linux] • [Windows] • [macOS] (especially with GGUF and llama.cpp support) Model Version(s): ================= Bielik-11B-v2.3-Instruct Other related versions include GGUF, GPTQ, and FP8 quantized versions. Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= • The model was fine-tuned on a dataset comprising over 20 million instructions (over 10 billion tokens). The DPO-Positive (DPO-P) fine-tuning used a dataset of over 66,000 examples. The base model Bielik-11B-v2 was trained on 400 billion tokens from Polish text corpora (SpeakLeash project) and English texts (SlimPajama dataset). • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Hybrid: Automated, Human, Synthetic • Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset: • Hybrid: Automated, Human, Synthetic • Properties (Quantity, Dataset Descriptions, Sensor(s)):Instruction fine-tuning: >20 million instructions, >10 billion tokens.DPO-P fine-tuning: >66,000 examples.Pre-training (Bielik-11B-v2): 400 billion tokens. The SpeakLeash dataset for Bielik v3 (related models) comprised 237 billion Polish tokens and was supplemented with English texts from SlimPajama, totaling 292 billion tokens from 303 million documents. Properties (Quantity, Dataset Descriptions, Sensor(s)): • Instruction fine-tuning: >20 million instructions, >10 billion tokens. • DPO-P fine-tuning: >66,000 examples. • Pre-training (Bielik-11B-v2): 400 billion tokens. The SpeakLeash dataset for Bielik v3 (related models) comprised 237 billion Polish tokens and was supplemented with English texts from SlimPajama, totaling 292 billion tokens from 303 million documents. Testing Dataset: ================ • Data Collection Method by dataset: Undisclosed Data Collection Method by dataset • Labeling Method by dataset: Undisclosed Labeling Method by dataset • Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: =================== • Benchmark Score:Open PL LLM Leaderboard (5-shot):65.71 (outperforms models <70B parameters, competitive with 70B models).Polish MT-Bench:8.556250 (outperforms GPT-3.5-turbo).Polish EQ-Bench:70.86.MixEval:Competitive score, methodology correlates highly with Chatbot Arena. Benchmark Score: • Open PL LLM Leaderboard (5-shot):65.71 (outperforms models <70B parameters, competitive with 70B models). Open PL LLM Leaderboard (5-shot): • Polish MT-Bench:8.556250 (outperforms GPT-3.5-turbo). Polish MT-Bench: • Polish EQ-Bench:70.86. Polish EQ-Bench: • MixEval:Competitive score, methodology correlates highly with Chatbot Arena. MixEval: • Data Collection Method by dataset:Hybrid: Automated, Human, Undisclosed Data Collection Method by dataset: • Hybrid: Automated, Human, Undisclosed • Labeling Method by datasetHybrid: Automated, Human, Undisclosed Labeling Method by dataset • Hybrid: Automated, Human, Undisclosed Inference: ========== • Acceleration Engine:TRT-LLM Acceleration Engine: • Test Hardware:NVIDIA Ada Lovelace Test Hardware: • NVIDIA Ada Lovelace Additional content ================== • Quantized Models:Available in GGUF (Q4_K_M, Q5_K_M, Q6_K, Q8_0, and experimental IQ imatrix versions), GPTQ (4bit), and FP8 (for vLLM, SGLang - Ada Lovelace, Hopper optimized). Quantized models may offer lower quality compared to full-sized variants. Quantized Models: • Training Improvements:Weighted tokens level loss.Adaptive learning rate.Masked prompt tokens.DPO-Positive (DPO-P) methodology with multi-turn conversation introduction. Training Improvements: • Weighted tokens level loss. • Adaptive learning rate. • Masked prompt tokens. • DPO-Positive (DPO-P) methodology with multi-turn conversation introduction. • Framework:Trained using an original open-source framework called ALLaMo. Framework: • Contact:SpeakLeash team via Discord or Hugging Face discussion tab. Contact: • Responsible for training the model:SpeakLeash & ACK Cyfronet AGH. Computational grant PLG/2024/016951 on Athena and Helios supercomputers (part of PLGrid environment). Responsible for training the model: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.1-nemotron-nano-vl-8b-v1",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3.1-nemotron-nano-vl-8b-v1",
    "scraped_on": "2025-10-15T21:39:31.709252",
    "tags": [
      "doc intelligence",
      "multiple image understanding",
      "nvidia",
      "ocr"
    ],
    "model_card": "Model Overview ============== Description: ============ Llama Nemotron Nano VL is a leading document intelligence vision language model (VLMs) that enables the ability to query and summarize images from the physical or virtual world. Llama Nemotron Nano VL is deployable in the data center, cloud and at the edge. We find: (1) image-text pairs are not enough, interleaved image-text is essential; (2) unfreezing LLM during interleaved image-text pre-training enables in-context learning; (3)re-blending text-only instruction data is crucial to boost both VLM and text-only performance. This model was trained on commercial images for all three stages of training and supports single image inference. License/Terms of Use -------------------- Governing Terms: Governing Terms: Your use of the service is governed by theNVIDIA API Catalog Terms of Service. Your use of the model is governed by theNVIDIA Open License Agreement. NVIDIA API Catalog Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Open License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) The NIM container is governed by theNVIDIA Software License Agreementand theProduct-Specific Terms for NVIDIA AI Products. Your use of this model is governed by the NVIDIA Community Model License. NVIDIA Software License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) Product-Specific Terms for NVIDIA AI Products (https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/) Additional Information:Llama 3.1 Community Model License; Built with Llama. Additional Information: Llama 3.1 Community Model License (https://www.llama.com/llama3_1/license/) Deployment Geography: --------------------- Global Use Case: --------- Customers: AI foundry enterprise customersUse Cases: Image summarization. Text-image analysis, Optical Character Recognition, Interactive Q&A on images.. Release Date: ============= Build.Nvidia.com [May 28th] via [URL] Hugging Face [May 28th] Model Architecture: =================== Network Type:TransformerNetwork Architecture:Vision Encoder: CRadioV2-H Language Encoder: LLAMA-3.1-8B-Instruct Network Type: Network Architecture: Input ----- Input Type(s): Image, Text • Input Images • Language Supported: English only Input Format(s): Image (Red, Green, Blue (RGB)), and Text (String) Input Parameters: Image (2D), Text (1D) Other Properties Related to Input: • Input + Output Token: 16K • Minimum Resolution: 32 × 32 pixels • Maximum Resolution: Determined by a 12-tile layout constraint, with each tile being 512 × 512 pixels. This supports aspect ratios such as:4 × 3 layout: up to 2048 × 1536 pixels3 × 4 layout: up to 1536 × 2048 pixels2 × 6 layout: up to 1024 × 3072 pixels6 × 2 layout: up to 3072 × 1024 pixelsOther configurations allowed, provided total tiles ≤ 12 • 4 × 3 layout: up to 2048 × 1536 pixels • 3 × 4 layout: up to 1536 × 2048 pixels • 2 × 6 layout: up to 1024 × 3072 pixels • 6 × 2 layout: up to 3072 × 1024 pixels • Other configurations allowed, provided total tiles ≤ 12 • Channel Count: 3 channels (RGB) • Alpha Channel: Not supported (no transparency) Output ------ Output Type(s): Text Output Formats: String Output Parameters: 1D Other Properties Related to Output: Input + Output Token: 16K Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration -------------------- Runtime Engine(s): TensorRT-LLMSupported Hardware Microarchitecture Compatibility: H100 SXM 80GBSupported Operating System(s): Linux Model Versions: --------------- Llama-3.1-Nemotron-Nano-VL-8B-V1 Training/Evaluation Dataset: ============================ NV-Pretraining and NV-CosmosNemotron-SFT were used for training and evaluation Data Collection Method by dataset (Training and Evaluation): • Hybrid: Human, Synthetic Labeling Method by dataset (Training and Evaluation): • Hybrid: Human, Synthetic Additionally, the dataset collection (for training and evaluation) consists of a mix of internal and public datasets designed for training and evaluation across various tasks. It includes:• Internal datasets built with public commercial images and internal labels, supporting tasks like conversation modeling and document analysis.• Public datasets sourced from publicly available images and annotations, adapted for tasks such as image captioning and visual question answering.• Synthetic datasets generated programmatically for specific tasks like tabular data understanding.• Specialized datasets for safety alignment, function calling, and domain-specific tasks (e.g., science diagrams, financial question answering). Evaluation Benchmarks: ====================== Inference: ========== Engine:TTensorRT-LLMTest Hardware: Engine: Test Hardware: • 1x NVIDIA H100 SXM 80GB Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards [Insert Link to Model Card++ here]. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Users are responsible for model inputs and outputs. Users are responsible for ensuring safe integration of this model, including implementing guardrails as well as other safety mechanisms, prior to deployment. Outputs generated by these models may contain political content or other potentially misleading information, issues with content security and safety, or unwanted bias that is independent of our oversight."
  },
  {
    "name": "Boltz-2",
    "nvidia_url": "https://build.nvidia.com/mit/boltz2",
    "scraped_on": "2025-10-15T21:39:31.319667",
    "tags": [
      "biology",
      "bionemo",
      "drug discovery",
      "mit",
      "nim",
      "protein folding"
    ],
    "model_card": "Model Overview ============== Description: ------------ Boltz-2 NIM is a next-generation structural biology foundation model that shows strong performance for both structure and affinity prediction. Boltz-2 is the first deep learning model to approach the accuracy of free energy perturbation (FEP) methods in predicting binding affinities of small molecules and proteins—achieving strong correlations on benchmarks while being nearly 1000× more computationally efficient.Key Features:Trunk optimizations:Mixed-precision (bfloat16) and trifast triangle attention cut runtime/memory; enables training with 768-token crops (as in AlphaFold3).Physical quality:Integrates Boltz-steering at inference (Boltz-2x) to reduce steric clashes and stereochemistry errors without losing accuracy.Controllability: Trunk optimizations: Physical quality: Controllability: • Method conditioning:Steers predictions to resemble X-ray, NMR, or MD-style structures. Method conditioning: • Template conditioning + steering:Uses single or multimeric templates; supports strict template enforcement or soft guidance. Template conditioning + steering: • Contact/pocket conditioning:Accepts distance constraints from experiments or expert priors. Contact/pocket conditioning: Affinity module:PairFormer refines protein–ligand and intra-ligand interactions; predicts both binding likelihood and a continuous affinity on log µM scale (trained on mixed Ki, Kd, IC50). Output is an IC50-like measure suitable for ranking.Key advances vs Boltz-1/1x:Faster/more memory-efficient trunk, improved physical plausibility via integrated steering, markedly enhanced controllability, and added affinity prediction head. Affinity module: Key advances vs Boltz-1/1x: This NIM is ready for commercial use. Third-Party Community Consideration ----------------------------------- This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. License / Terms of Use ---------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:MIT. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) MIT (https://github.com/jwohlwend/boltz?tab=MIT-1-ov-file#readme) You are responsible for ensuring that your use of NVIDIA provided models complies with all applicable laws. Deployment Geography -------------------- Global Use Case -------- Boltz-2 NIM enables researchers and commercial entities in the Drug Discovery, Life Sciences, and Digital Biology fields to predict the three-dimensional structure of biomolecular complexes and predict small-molecule binding affinities. Trained on millions of curated experimental datapoints with a novel training strategy tailored for noisy biochemical assay data, Boltz-2 demonstrates robust performance across hit-discovery, hit-to-lead, and lead optimization. Release Date ------------ Build.nvidia.com September 30, 2025 viabuild.nvidia.com build.nvidia.com (https://build.nvidia.com/mit/boltz2) NGC September 30, 2025 viahttps://registry.ngc.nvidia.com/ https://registry.ngc.nvidia.com/ (https://registry.ngc.nvidia.com/orgs/nvstaging/teams/nim/models/boltz2) References: ----------- Model Architecture: ------------------- Architecture Type:Four components — trunk, denoising module (with steering), confidence module, and a new affinity moduleNetwork Architecture:PairFormer Architecture Type: Network Architecture: Input Type(s):Biomolecular sequences (protein, DNA, RNA), ligand SMILES or CCD strings, molecular modifications, structural constraints, conditioning parameters, optional booleansInput Format(s):Dictionary containing sequence strings, modification records, and constraint parametersInput Parameters:Sequences (strings), predict_affinity(boolean), modifications (list of residue-specific changes), constraints (dictionary of structural parameters)Other Properties Related to Input:Maximum sequence length of 4096 residues per chain. Maximum of 12 input polymers. Maximum of 20 input ligands. Passing boolean options such as predict_affinity will increase the runtime of the request.Model Parameters:Tables 1 and 2 record some of the hyperparameters of Boltz-2’s architecture, training and inference procedures that differ from Boltz-1’s and were not previously mentioned in the manuscript. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Model Parameters: Table 1:Extra model architecture and training hyperparametersthat differ from Boltz-1 and were not previously mentioned in the manuscript. Table 2:Diffusion process hyperparametersthat differ from Boltz-1, with the exception of sigma_min we opted for AlphaFold3’s default hyperparameters, see Abramson et al. (2024) for more details. Output: ------- Output Type(s):Structure prediction in mmcif format; scores in numeric arrays; runtime metrics as a dictionaryOutput Format:mmcif (text file); numeric arrays; scalar numeric valuesOutput Parameters:3D atomic coordinates, predicted scores, and metadataOther Properties Related to Output:All Boltz-2 scores are returned by default. Runtime metrics are optional.Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Software Integration: --------------------- Runtime Engine(s): • PyTorch, TensorRT Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere, NVIDIA Hopper, NVIDIA Lovelace [Preferred/Supported] Operating System(s): • [Linux] • The integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment. Model Version(s): ----------------- Boltz2 version 1.3 Training & Evaluation: ====================== Training Dataset: ----------------- ** Data Modality • [Text] Link:Protein Data Bank as used by AlphaFold3** Data Collection Method by dataset Link: Protein Data Bank as used by AlphaFold3 (https://github.com/jwohlwend/boltz/blob/main/docs/training.md) • Human ** Labeling Method by dataset • Human Properties:All Protein Data Bank structures before 2021-09-30 with a resolution of at least 9 Angstroms, preprocessed to match each structure to its sequence. Ligands were processed similarly. All data was cleaned as described in AlphaFold3. Properties: Evaluation Dataset: ------------------- Link:Boltz Evaluation Performed on 744 Structures from the Protein Data Bank** Data Collection Method by dataset Link: Boltz Evaluation Performed on 744 Structures from the Protein Data Bank (https://github.com/jwohlwend/boltz/blob/main/docs/evaluation.md) • Human ** Labeling Method by dataset • Hybrid: Human and Automated Properties:The test and validation datasets were generated by extensive filtering of PDB sequences deposited between 2021-09-31 and 2023-01-13. In total, 593 structures passed filters and were used for validation. Properties: Inference: ---------- Acceleration Engine:PyTorch, TensorRTTest Hardware: Acceleration Engine: Test Hardware: • NVIDIA B200 • NVIDIA H100 • NVIDIA L40 • NVIDIA A100 • NVIDIA RTX6000/NVIDIA RTX6000-Ada • NVIDIA GB200 Ethical Considerations: ----------------------- NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report model quality, risk, security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) You are responsible for ensuring for ensuring the physical properties of model-generated molecules are appropriately evaluated, and comply with applicable safety regulations and ethical standards."
  },
  {
    "name": "deepseek-r1-0528",
    "nvidia_url": "https://build.nvidia.com/deepseek-ai/deepseek-r1-0528",
    "scraped_on": "2025-10-15T21:39:30.888865",
    "tags": [
      "advanced reasoning",
      "chat",
      "coding",
      "deepseek-ai",
      "math"
    ],
    "model_card": "DeepSeek-R1-0528 Overview ========================= Description =========== DeepSeek-R1-0528 is an enhanced version of the DeepSeek R1 model, featuring improved reasoning and inference capabilities achieved through increased computational resources and post-training algorithmic optimizations. This upgrade significantly boosts performance in mathematics, programming, and general logic tasks, with reduced hallucination rates and better support for function calling. Notably, the model's accuracy on the AIME 2025 test increased from 70% to 87.5%. This model is ready for commercial use. Third‑Party Community Consideration =================================== This model is not owned or developed by NVIDIA. It has been produced to a third‑party’s requirements for this application and use‑case; see link to Non-NVIDIADeepSeek-R1-0528 Model Card. DeepSeek-R1-0528 Model Card (https://huggingface.co/deepseek-ai/DeepSeek-R1-0528) License and Terms of Use: ------------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:MIT License. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) MIT License (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md) Deployment Geography: ===================== Global Use Case: ========= Advanced reasoning tasks, including mathematics, programming, and general logic. Release Date: ============= Build.NVIDIA.com:2025-06-09Hugging Face:2025-05-28 (link) Build.NVIDIA.com: Hugging Face: link (https://huggingface.co/deepseek-ai/DeepSeek-R1-0528) References: =========== • DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 (arxiv.org) arxiv.org (https://arxiv.org/abs/2501.12948) Model Architecture: =================== • Architecture Type: Sparse Mixture of Experts (MoE) • Network Architecture: DeepSeek-V3-Base • Parameter Count: 671 billion total / 37 billion activated per forward pass • Notable Architectural Features: Incorporates chain-of-thought reasoning and reinforcement learning techniques. • Base Model: DeepSeek-V3-Base Input ===== Input Type(s):TextInput Formats:StringInput Parameters:1DOther Properties Related to Input:Maximum context length of 128K tokens. Input Type(s): Input Formats: Input Parameters: Other Properties Related to Input: Output ====== Output Type(s):TextOutput Formats:StringOutput Parameters:1DOther Properties Related to Output:Enhanced reasoning depth, averaging 23K tokens per question in AIME test set. Output Type(s): Output Formats: Output Parameters: Other Properties Related to Output: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA’s hardware (e.g., GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration ==================== Runtime Engine(s): • vLLM • SGLang Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Jetson • NVIDIA Lovelace • NVIDIA Pascal • NVIDIA Turing • NVIDIA Volta Supported Operating System(s):Linux Supported Operating System(s): Model Version: ============== DeepSeek-R1-0528 v1.0 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= Link:UndisclosedData Collection Method by dataset Link: Data Collection Method by dataset • Hybrid: Human, Automated Labeling Method by dataset • Hybrid: Human, Automated Properties:Undisclosed Properties: Testing Dataset: ================ Link:UndisclosedData Collection Method by dataset Link: Data Collection Method by dataset • Hybrid: Human, Automated Labeling Method by dataset: • Hybrid: Human, Automated Properties:Undisclosed Properties: Evaluation Dataset: =================== Benchmark Score:Various benchmarks, including AIME 2025 with 87.5% accuracy.Data Collection Method by dataset Benchmark Score: Data Collection Method by dataset • Hybrid: Human, Automated Labeling Method by dataset • Hybrid: Human, Automated Properties:Undisclosed Properties: Evaluation Results ------------------ DeepSeek-R1-0528 ---------------- For all our models, the maximum generation length is set to 64K tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 16 responses per query to estimate pass@1. Inference: ========== Acceleration Engine:SGLangTest Hardware:NVIDIA Blackwell Acceleration Engine: Test Hardware: Additional Details: =================== DeepSeek-R1-0528 demonstrates competitive performance on various benchmarks, including MMLU-Redux, GPQA-Diamond, and LiveCodeBench. It also introduces a distilled version, DeepSeek-R1-0528-Qwen3-8B, which achieves state-of-the-art performance among open-source models on the AIME 2024 benchmark. How to Run Locally ------------------ Please visitDeepSeek-R1repository for more information about running DeepSeek-R1-0528 locally. DeepSeek-R1 (https://github.com/deepseek-ai/DeepSeek-R1) Compared to previous versions of DeepSeek-R1, the usage recommendations for DeepSeek-R1-0528 have the following changes: • System prompt is supported now. • It is not required to add \"<think>\\n\" at the beginning of the output to force the model into thinking pattern. The model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528. This model can be run in the same manner as Qwen3-8B. System Prompt ------------- In the official DeepSeek web/app, we use the same system prompt with a specific date. For example, Temperature ----------- In our web and application environments, the temperature parameter $T_{model}$ is set to 0.6. Prompts for File Uploading and Web Search ----------------------------------------- For file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments. For Web Search, {search_results}, {cur_date}, and {question} are arguments. For Chinese query, we use the prompt: For English query, we use the prompt: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "mistral-nemotron",
    "nvidia_url": "https://build.nvidia.com/mistralai/mistral-nemotron",
    "scraped_on": "2025-10-15T21:39:27.761533",
    "tags": [
      "chat",
      "function calling",
      "instruction following",
      "language generation",
      "mistralai"
    ],
    "model_card": "Mistral-Nemotron Overview ========================= Description: ============ Mistral-Nemotron is a large language model produced by Mistral and optimised by NVIDIA that generates human-like text and can be used for a variety of natural language processing tasks, such as text generation, language translation, and text summarization. It is also suitable for Agentic workflows due to its tool calling capabilities. This model is ready for commercial usage. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; more information available on the modelhere(scroll past animation). here (https://mistral.ai/models) License/Terms of Use: ===================== Access to this model is governed by the NVIDIAAPI Trial Terms of Service. API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) To deploy and customize Mistral models with NIMs in your environment, please contact Mistral AI to acquire necessary licenses athttps://mistral.ai/license-mistral-models-for-nvidia-inference-microservices. https://mistral.ai/license-mistral-models-for-nvidia-inference-microservices Use of this model is subject to compliance with all applicable laws, and users are responsible for ensuring such compliance.\" Deployment Geography: ===================== Global Use Case: ========= Researchers and developers in the field of natural language processing (NLP) and artificial intelligence (AI) may use the Mistral-Nemotron model for tasks such as language translation, text summarization, and conversational AI applications. Release Date: ============= Release Date on Build.NVIDIA.com:\"06/11/2025,https://build.nvidia.com/mistralai/mistral-nemotron\" https://build.nvidia.com/mistralai/mistral-nemotron Model Architecture: =================== • Architecture Type:Transformer • Transformer • Network Architecture:Modified Transformer • Modified Transformer Input: ====== • Input Type(s):Text • Text • Input Format(s):String • String • Input Parameters:One-Dimensional (1D) • One-Dimensional (1D) • Other Properties Related to Input:128K Maximum Context Length • 128K Maximum Context Length Output: ======= • Output Type:Text • Text • Output Format:String • String • Output Parameters:One-Dimensional (1D) • One-Dimensional (1D) • Other Properties Related to Output:Maximum Context Length 128K • Maximum Context Length 128K Software Integration:Runtime Engine(s):['TensorRT-LLM', 'vLLM'] Supported Hardware Microarchitecture Compatibility:['NVIDIA Hopper'] [Preferred/Supported] Operating System(s):['Linux'] Model Version(s):{'v1'} Training, Testing, and Evaluation Datasets: Benchmark Score: Coding & Programming ==================== Instruction Following ===================== Mathematics =========== General Knowledge & Reasoning ============================= MMLU by Language ================ Data Collection Method by dataset • Hybrid: Automated, Human, Synthetic Labeling Method by dataset • Hybrid: Automated, Human, Synthetic Inference: • Engine:TensorRT-LLM, vLLM • TensorRT-LLM, vLLM Test Hardware : • H100 Ethical Considerations:NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.2-nemoretriever-1b-vlm-embed-v1",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_2-nemoretriever-1b-vlm-embed-v1",
    "scraped_on": "2025-10-15T21:39:24.649348",
    "tags": [
      "embedding",
      "nemo retriever",
      "nvidia",
      "retrieval augmented generation",
      "text-to-embedding"
    ],
    "model_card": "Model Overview ============== Model Overview Description ----------- Description The Llama 3.2 NeMo Retriever Multimodal Embedding 1B model is optimized formultimodalquestion-answering retrieval. The model can embed 'documents' in the form of image, text, or image and text combined. Documents can be retrieved given a user query in text form. The model supports images containing text, tables, charts, and infographics. This model was evaluated onViDoRe V1and two internal multimodal retrieval benchmarks. multimodal ViDoRe V1 (https://huggingface.co/spaces/vidore/vidore-leaderboard) An embedding model is a crucial component of a retrieval system, because it transforms information into dense vector representations. An embedding model is typically a transformer encoder that processes tokens of input (text or image) (for example: question, passage) to output an embedding. The Llama 3.2 NeMo Retriever Multimodal Embedding 1B model is a combined language model and vision model. The Llama 3.2 NeMo Retriever Multimodal Embedding 1B model is a part of the NVIDIA NeMo Retriever collection of NIM, which provides state-of-the-art, commercially-ready models and microservices optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can readily customize them for domain-specific use cases, such as information technology, human resource help assistants, and research & development research assistants. This model is ready for commercial use. License/Terms of use -------------------- License/Terms of use GOVERNING TERMS: Access to this trial is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) ADDITIONAL INFORMATION:Llama 3.2 Community License Agreement. Built with Llama. Llama 3.2 Community License Agreement (https://www.llama.com/llama3_2/license/) Deployment Geography: --------------------- Global Use Case: --------- The Llama 3.2 NeMo Retriever Multimodal Embedding 1B model is most suitable for users who want to build a multimodal question-and-answer application over a large corpus, leveraging the latest dense retrieval technologies. Release Date: ------------- Build.Nvidia.com:May 20, 2025 viahttps://build.nvidia.com/nvidia/llama-3_2-nemoretriever-1b-vlm-embed-v1NGC:May 20, 2025 Build.Nvidia.com: https://build.nvidia.com/nvidia/llama-3_2-nemoretriever-1b-vlm-embed-v1 NGC: You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. Model Architecture ------------------ Model Architecture Architecture Type:TransformerNetwork Architecture:Fine-tuned MultiModal Llama 3.2 1B Retriever Architecture Type: Network Architecture: This NeMo Retriever embedding model is a transformer encoder. It is a fine-tuned version of Llama 3.2 1B with SigLip2 400M, with 16 layers and an embedding size of 2048, which is trained on public datasets. Embedding models for text retrieval are typically trained using a bi-encoder architecture. This involves encoding a pair of query and document independently using the embedding model. Contrastive learning is used in this model to maximize the similarity between the query and the document that contains the answer, while minimizing the similarity between the query and sampled negative documents not useful to answer the question. The vision-language model encoder incorporates key innovations from NVIDIA, includingEagle 2 workandnemoretriever-parse, which use a tiling-based VLM architecture. This architecture, available onHugging Face, significantly enhances multimodal understanding through its dynamic tiling and mixture of vision encoders design. It particularly improves performance on tasks that involve high-resolution images and complex visual content. Eagle 2 work (https://arxiv.org/abs/2501.14818) nemoretriever-parse (https://build.nvidia.com/nvidia/nemoretriever-parse) Hugging Face (https://huggingface.co/collections/nvidia/eagle-2-6764ba887fa1ef387f7df067) Input ----- Input Output ------ Output Output Type:FloatsOutput Format:List of float arraysOutput:Model outputs embedding vectors of maximum dimension 2048 for each input.Other Properties Related to Output:N/A Output Type: Output Format: Output: Other Properties Related to Output: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (such as GPU cores) and software frameworks (such as CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration -------------------- Software Integration Runtime Engine:NeMo Retriever Embedding NIMSupported Hardware Microarchitecture Compatibility: NVIDIA Ampere, NVIDIA Blackwell, NVIDIA Hopper, NVIDIA LovelaceSupported Operating System(s):Linux Runtime Engine: Supported Hardware Microarchitecture Compatibility Supported Operating System(s): Model Version(s) ---------------- Model Version(s) Llama 3.2 NeMo Retriever Multimodal Embedding 1BShort Name:llama-3.2-nemoretriever-1b-vlm-embed-v1 Training Dataset & Evaluation ============================= Training Dataset & Evaluation Training Dataset ---------------- Training Dataset The development of large-scale, public, open-QA datasets has enabled tremendous progress in powerful embedding models. However, the following issues limit the use of these models in commercial settings. • One popular dataset, named MS MARCO, restricts ‌commercial licensing. • Many multimodal datasets use synthetic data generation with proprietary models. To address these issues, NVIDIA created its own training dataset. NVIDIA's training dataset is based on public QA datasets, and only includes datasets that have a license for commercial applications. Data Collection Method by dataset: Hybrid: Automated, Human, SyntheticLabeling Method by dataset: Hybrid: Automated, Human, SyntheticProperties:The text component is comprised of semi-supervised pre-training on 12M samples from public datasets and fine-tuning on 1.5M samples from public datasets. The VLM component uses only commercially-viable data from theEagle2training data. Data Collection Method by dataset Labeling Method by dataset Properties: Eagle2 (https://github.com/NVlabs/EAGLE) Evaluation Datasets ------------------- Evaluation Datasets We evaluated the NeMo Retriever Multimodal Embedding Model against both published literature and existing open-source and commercial retriever models. Our evaluation used three benchmark datasets for question-answering tasks: the publicViDoRe V1benchmark and two internal multimodal retrieval benchmarks. For those interested in reproducing our results, one of our internal datasets (DigitalCorpora-767) can be created by following instructions inthis notebookfrom the NeMo Retriever Extraction GitHub repository. ViDoRe V1 (https://huggingface.co/spaces/vidore/vidore-leaderboard) this notebook (https://github.com/NVIDIA/nv-ingest/blob/main/evaluation/digital_corpora_download.ipynb) Data Collection Method by dataset: Hybrid: Automated, Human, SyntheticLabeling Method by dataset: Hybrid: Automated, Human, SyntheticProperties:More details onViDoRe V1can be found on their leaderboard.DigitalCorpora-767is a set of 767 PDFs that have a good mixture of text, tables, and charts. Data Collection Method by dataset Labeling Method by dataset Properties: ViDoRe V1 (https://huggingface.co/spaces/vidore/vidore-leaderboard) DigitalCorpora-767 (https://github.com/NVIDIA/nv-ingest/blob/main/evaluation/digital_corpora_download.ipynb) Evaluation Results ------------------ Evaluation Results We do not compare to col-style embedding (late interaction) models because late interaction embeddings require a significant embedding store. Detailed Performance Analysis ----------------------------- The model's performance was evaluated across different modalities and compared with other models using various pipelines. The following table contains the detailed results for the DigitalCorpora-767 dataset: InferenceEngine:TensorRTTest Hardware:H100 PCIe/SXM, A100 PCIe/SXM, L40s, L4, and A10G Inference Engine: Test Hardware: Ethical Considerations ====================== Ethical Considerations NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case, and address unforeseen product misuse. For more detailed information on ethical considerations for this model, see the Model Card++ tab for the Explainability, Bias, Safety & Security, and Privacy subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "Background Noise Removal",
    "nvidia_url": "https://build.nvidia.com/nvidia/bnr",
    "scraped_on": "2025-10-15T21:39:21.457805",
    "tags": [
      "digital human",
      "nvidia",
      "nvidia maxine",
      "speech enhancement",
      "speech-to-speech"
    ],
    "model_card": "Model Overview ============== Description =========== Maxine Background Noise Removal (BNR) model is an audio background noise removal model from NVIDIA. It removes a variety of background noises from audio recordings. It also retains emotive tones in speech, such as happy, sad, excited and angry tones BNR is available under NVIDIA Maxine — a developer platform for deploying AI features that enhance audio, video, and creating new experiences in real-time audio-video communication. Maxine's state-of-the-art models create high-quality AI effects using standard microphones and cameras without additional special equipments. NVIDIA Maxine is exclusively part of NVIDIA AI Enterprise for production workflows — an extensive library of full-stack software, including AI solution workflows, frameworks, pre-trained models, and infrastructure optimization. Terms of use ============ The use of NVIDIA Maxine's BNR is available as a demonstration of the input and output of the BNR generative model. As such the user may upload an audio file or select one of the sample inputs and download the generated audio for evaluation under the terms of theNVIDIA MAXINE EVALUATION LICENSE AGREEMENT. NVIDIA MAXINE EVALUATION LICENSE AGREEMENT (https://developer.download.nvidia.com/maxine/nvidia-maxine-evaluation-license-24oct2023.pdf) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws Reference(s): ============= • NVIDIA Maxine NVIDIA Maxine (https://developer.nvidia.com/maxine) Model Architecture: =================== Architecture Type:Residual Convolutional Recurrent Neural Network (CRNN) Architecture Type: Network Architecture:SEASR Network Architecture: Model Version:1.0 Model Version: Input: ====== Input Type(s):Ordered List (audio samples) Input Type(s): Input Format(s):FP32 (-1.0 to 1.0) Input Format(s): Other Properties Related to Input:Pulse Code Modulation (PCM) audio samples Other Properties Related to Input: with no encoding or pre-processing; 16kHz or 48kHz sampling rate required. Output: ======= Output Type(s):Ordered List (audio samples) Output Type(s): Output Format:FP32 (-1.0 to 1.0) Output Format: Other Properties Related to Output:PCM audio samples at input sampling rate Other Properties Related to Output: with no encoding or post-processing. Software Integration ==================== Supported Hardware Microarchitecture Compatibility: • Volta • Turing • Ada • Ampere • Hopper • Blackwell Supported Operating System(s): • Linux Training & Evaluation ===================== Datasets ======== NVIDIA models are trained on a diverse set of public and proprietary datasets. The BNR model is trained on a wide range of English language accents, some European and Asian languanges, and 29 different noise profiles that are commonly audible in real world. Data Collection Method by dataset:[Hybrid: Human, Synthetic] Data Collection Method by dataset: Labeling Method by dataset:[Hybrid: Human, Synthetic] Labeling Method by dataset: Link:AudioSetProperties (Quantity, Dataset Descriptions, Sensor(s)):AudioSet consists of an expanding ontology of 632 audio event classes and a collection of 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos. Link: AudioSet (https://research.google.com/audioset/) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:CREMA-DProperties (Quantity, Dataset Descriptions, Sensor(s)):CREMA-D is a data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified). Actors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified). Link: CREMA-D (https://www.kaggle.com/ejlok1/cremad) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:Crowdsourced high-quality UK and Ireland English Dialect speech data setProperties (Quantity, Dataset Descriptions, Sensor(s)):The dataset contains male and female high quality recordings of English from various dialects of the UK and Ireland for a total of 17,877 lines. Link: Crowdsourced high-quality UK and Ireland English Dialect speech data set (https://www.openslr.org/83/) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:CSR-I WSJ0Properties (Quantity, Dataset Descriptions, Sensor(s)):Corpus by the DARPA Spoken Language Program to support research on large- vocabulary Continuous Speech Recognition (CSR) systems. The first two CSR Corpora consist primarily of read speech with texts drawn from a machine- readable corpus of Wall Street Journal news text and are thus often known as WSJ0 and WSJ1. WSJ0 consists of 123 speakers. Link: CSR-I WSJ0 (https://catalog.ldc.upenn.edu/LDC93S6A) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:CSTR VCTKProperties (Quantity, Dataset Descriptions, Sensor(s)):This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. Link: CSTR VCTK (https://datashare.ed.ac.uk/handle/10283/3443) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:DAPSProperties (Quantity, Dataset Descriptions, Sensor(s)):The DAPS dataset has 15 versions of audio (3 professional versions and 12 consumer device/real-world environment combinations). Each version consists of about 4.5 hours of data (about 14 minutes from each of 20 speakers). Link: DAPS (https://ccrma.stanford.edu/~gautham/Site/daps.html) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:DEMANDProperties (Quantity, Dataset Descriptions, Sensor(s)):The DEMAND (Diverse Environments Multichannel Acoustic Noise Database) contains a set of recordings that allow testing of algorithms using real-world noise in a variety of settings. This version provides 15 recordings. All recordings are made with a 16-channel array, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm. It is a collection of multichannel recordings of accoustic noise in diverse environments. Link: DEMAND (https://zenodo.org/records/1227121) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:Edinburgh 56 speaker datasetProperties (Quantity, Dataset Descriptions, Sensor(s)):Clean and noisy parallel speech database from 56 speakers designed to train and test speech enhancement methods that operate at 48kHz. Link: Edinburgh 56 speaker dataset (https://datashare.ed.ac.uk/handle/10283/2791) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:FreeFieldProperties (Quantity, Dataset Descriptions, Sensor(s)):A dataset of standardised 7690 10-second excerpts from Freesound field recordings. Link: FreeField (https://archive.org/details/freefield1010) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:FreesoundProperties (Quantity, Dataset Descriptions, Sensor(s)):Freesound is a collaborative collection of 620,291 free sounds which contains speakers speaking in different emotions as well as female speakers speaking in high pitched voices. The audio data also contains few noise profiles too. Link: Freesound (https://freesound.org/) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:GTC DatasetProperties (Quantity, Dataset Descriptions, Sensor(s)):A collection of talks from Nvidia GTC Conferences with a total of 103 speakers. Link: Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:HiFi-TTSProperties (Quantity, Dataset Descriptions, Sensor(s)):A multi-speaker English dataset for training text-to-speech models. The HiFi-TTS dataset contains about 291.6 hours of speech from 10 speakers with at least 17 hours per speaker sampled at 44.1 kHz. Link: HiFi-TTS (https://www.openslr.org/109/) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:LibriTTSProperties (Quantity, Dataset Descriptions, Sensor(s)):LibriTTS is a multi-speaker English corpus of approximately 585 hours of read English speech by 200 speakers, which is resampled at 16kHZ. Link: LibriTTS (https://www.openslr.org/60/) Properties (Quantity, Dataset Descriptions, Sensor(s)): Link:Vocal set datasetProperties (Quantity, Dataset Descriptions, Sensor(s)):VocalSet is a singing voice dataset consisting of 10.1 hours of monophonic recorded audio of professional singers demonstrating both standard and extended vocal techniques on all 5 vowels. Existing singing voice datasets aim to capture a focused subset of singing voice characteristics, and generally consist of just a few singers. VocalSet contains recordings from 20 different singers (9 male, 11 female) and a range of voice types. VocalSet aims to improve the state of existing singing voice datasets and singing voice research by capturing not only a range of vowels, but also a diverse set of voices on many different vocal techniques, sung in contexts of scales, arpeggios, long tones, and excerpts. Link: Vocal set dataset (https://zenodo.org/record/1193957#.Xt9uVUUzaUl) Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference ========= Engine:TritonTest Hardware:A10, A100, A16, A2, A30, A40, H100, L4, L40, RTX 4080, RTX 4090, RTX 5070, RTX 5080, RTX 5090, T4, V100 Engine: Triton (https://developer.nvidia.com/triton-inference-server) Test Hardware: Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++Explainability,Bias,Safety & Security, andPrivacySubcards. Please report security vulnerabilities or NVIDIA AI Concernshere. Explainability (explainability) Bias (bias) Safety & Security (safety-and-security) Privacy (privacy) here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "cosmos-transfer1-7b",
    "nvidia_url": "https://build.nvidia.com/nvidia/cosmos-transfer1-7b",
    "scraped_on": "2025-10-15T21:39:18.067601",
    "tags": [
      "autonomous vehicles",
      "nvidia",
      "physical ai",
      "robotics",
      "synthetic data generation",
      "video-to-world"
    ],
    "model_card": "Cosmos-Transfer1-7B Overview ============================ Description: ============ Cosmos-Transfer1-7B is a family of highly performant pre-trained world foundation models that are designed to generate videos aligned with the input control conditions. The Cosmos Transfer1 models are a collection of diffusion-based world foundation models that generate dynamic, high quality videos from text and control video inputs. It can serve as the building block for various applications or research that are related to world generation. This model is ready for commercial use. License/Terms of Use: --------------------- The trial service is governed by theNVIDIA API Trial Terms of Service; and use of the Cosmos-Transfer1-7B model is governed by theNVIDIA Open Model License NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Open Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Deployment Geography: --------------------- Global Use Case: --------- Physical AI developers for world generation. Release Date: ------------- Github 03/02/2025 via [https://github.com/nvidia-cosmos/cosmos-transfer1]Huggingface 03/02/2025 via [https://huggingface.co/nvidia/Cosmos-Transfer1-7B] https://github.com/nvidia-cosmos/cosmos-transfer1 https://huggingface.co/nvidia/Cosmos-Transfer1-7B Reference(s): ============= Cosmos-Transfer1: A Suite of Diffusion-based World-to-World Transfer ModelsGithb repo Cosmos-Transfer1: A Suite of Diffusion-based World-to-World Transfer Models (https://arxiv.org/abs/2503.14492) Cosmos-Transfer1: A Suite of Diffusion-based World-to-World Transfer Models Githb repo (https://github.com/nvidia-cosmos/cosmos-transfer1) Githb repo Model Architecture: =================== Cosmos-Transfer1-7B is a diffusion transformer model designed for video denoising in the latent space, modulated by multiple control branches with a spatiotemporal control map. The diffusion transformer network (\"the base model\") is composed of interleaved self-attention, cross-attention, and feedforward layers as its building blocks. The cross-attention layers allow the model to condition on input text throughout the denoising process. Before each layers, adaptive layer normalization is applied to embed the time information for denoising. The control branch is formed by replicating the first few transformer blocks of the base model. It processes the control input video to extract control signals, which are then injected into the corresponding transformer blocks of the base model, guiding the denoising process with structured control. When multiple control input videos are provided, each is processed by a dedicated control branch, trained to extract modality-specific control signals. These signals are then combined through a weighted sum and injected into the corresponding transformer blocks in the base model. Input: ====== Input Type(s):Text+VideoInput Format(s):* Text: String* Video: mp4* Control Input Video: mp4Input Parameters:* Text: One-dimensional (1D)* Video: Three-dimensional (3D)* Control Input video: Three-dimensional (3D)Other Properties Related to Input:* The input string should contain fewer than 300 words and should provide descriptive content for world generation, such as a scene description, key objects or characters, background, and any specific actions or motions to be depicted within the 5-second duration.* The model supports control input videos of varying lengths, but a length which is multiples of 120 frames (e.g., 120, 240, or 360 frames) performs the best.* The model supports four types of control input videos: blurred video, Canny edge, depth map, segmentation mask, and human keypoints. When multiple control inputs are provided, they must be derived from the same source video, representing different modalities of the same content while maintaining identical spatio-temporal dimensions. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):VideoOutput Format:mp4Output Parameters:Three-dimensional (3D)Other Properties Related to Output:By default, the generated video is the same length, height, width and frames-per-second (fps) as the input. The video content transforms the input video into a photorealistic scene, following the input text description. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster inference times compared to CPU-only solutions. Software Integration: ===================== Runtime Engine(s): • Cosmos-Transfer1 Cosmos-Transfer1 (https://github.com/nvidia-cosmos/cosmos-transfer1) Supported Hardware Microarchitecture Compatibility: • NVIDIA Hopper H200 • NVIDIA Hopper H100 SXM • NVIDIA Hopper H100 NVL • NVIDIA Hopper H100 PCIe • NVIDIA Ampere A100 • NVIDIA Ada Lovelace L40S • NVIDIA Hopper H20 • NVIDIA Blackwell Preferred/Supported Operating System(s): • Linux Model Version(s): ================= Cosmos-Transfer1-7B Evaluation Dataset : ==================== Please see our technicalpaperfor detailed evaluations of the base model. The control models are built upon the base foundation model. paper (https://arxiv.org/abs/2503.14492) ** Data Collection Method by dataset • Hybrid: Human, Synthetic, Automated Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Users are responsible for model inputs and outputs. Users are responsible for ensuring safe integration of this model, including implementing guardrails as well as other safety mechanisms, prior to deployment. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.2-nemoretriever-500m-rerank-v2",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_2-nemoretriever-500m-rerank-v2",
    "scraped_on": "2025-10-15T21:39:17.594119",
    "tags": [
      "nemo retriever",
      "nvidia",
      "reranking",
      "retrieval augmented generation"
    ],
    "model_card": "Model Overview ============== Model Overview Description ----------- Description ​​The Llama 3.2 NeMo Retriever Reranking 500M model is optimized for providing a logit score that represents how relevant a document(s) is to a given query. The model was fine-tuned for multilingual, cross-lingual text question-answering retrieval, with support for long documents (up to 8192 tokens). This model was evaluated on 26 languages: English, Arabic, Bengali, Chinese, Czech, Danish, Dutch, Finnish, French, German, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, and Turkish. The reranking model is a component in a text retrieval system to improve the overall accuracy. A text retrieval system often uses an embedding model (dense) or a lexical search (sparse) index to return relevant text passages given the input. A reranking model can be used to rerank the potential candidates into a final order. The reranking model has the question-passage pairs as an input and therefore, can process cross attention between the words. It’s not feasible to apply a Ranking model on all documents in the knowledge base, therefore, ranking models are often deployed in combination with embedding models. This 500m version is pruned from the 1B version - it shares the same architecture overall, but is smaller and faster. Users should expect 90-95% of the accuracy of the 1B version, but with lower latency (as much as 2- 3x improvement) and reduced memory usage. This model is ready for commercial use. The Llama 3.2 NeMo Retriever Reranking 500M model is a part of the NeMo Retriever collection of NIM, which provides state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can also readily customize them for their domain-specific use cases, such as information technology, human resource help assistants, and research & development assistants. License/Terms of use -------------------- License/Terms of use GOVERNING TERMS: The NIM container is governed by theNVIDIA Software License Agreementand theProduct-Specific Terms for NVIDIA AI Products; except for the model which is governed by theNVIDIA Community Model License Agreement. NVIDIA Software License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) Product-Specific Terms for NVIDIA AI Products (https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/) NVIDIA Community Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) ADDITIONAL INFORMATION:Llama 3.2 Community License Agreement. Built with Llama. Llama 3.2 Community License Agreement (https://www.llama.com/llama3_2/license/) Intended use ------------ Intended use The Llama 3.2 NeMo Retriever Reranking 500M model is most suitable for users who are focused on performance and latency, and want to improve their multilingual retrieval tasks by reranking a set of candidates for a given question. Model Architecture: Llama-3.2 500M Ranker ----------------------------------------- Model Architecture: Llama-3.2 500M Ranker Architecture Type:TransformerNetwork Architecture:Fine-tuned meta-llama/Llama-3.2-1B Architecture Type: Network Architecture: The Llama 3.2 NeMo Retriever Reranking 500M model is a transformer encoder fine-tuned for contrastive learning. We employ bi-directional attention when fine-tuning for higher accuracy. The last embedding output by the decoder model is used with a mean pooling strategy, and a binary classification head is fine-tuned for the ranking task. Ranking models for text ranking are typically trained as a cross-encoder for sentence classification. This involves predicting the relevancy of a sentence pair (for example, question and chunked passages). The CrossEntropy loss is used to maximize the likelihood of passages containing information to answer the question and minimize the likelihood for (negative) passages that do not contain information to answer the question. We train the model on public datasets described in the Dataset and Training section. Input ----- Input Input Type:Pair of TextsInput Format:List of text pairsInput Parameters:1DOther Properties Related to Input:The model was trained on question and answering over text documents from multiple languages. It was evaluated to work successfully with up to a sequence length of 8192 tokens. Longer texts are recommended to be either chunked or truncated. Input Type: Input Format: Input Parameters: Other Properties Related to Input: OutputOutput Type:FloatsOutput Format:List of floatsOutput Parameters:1DOther Properties Related to Output:Each the probability score (or raw logits). Users can decide to implement a Sigmoid activation function applied to the logits in their usage of the model. Output Output Type: Output Format: Output Parameters: Other Properties Related to Output: Software Integration -------------------- Software Integration Runtime:Llama 3.2 NeMo Retriever Reranking 500MNIMSupported Hardware Microarchitecture Compatibility:NVIDIA Ampere, NVIDIA Hopper, NVIDIA LovelaceSupported Operating System(s):Linux Runtime: Llama 3.2 NeMo Retriever Reranking 500M Supported Hardware Microarchitecture Compatibility: Supported Operating System(s): Model Version(s) ---------------- Model Version(s) Llama 3.2 NeMo Retriever Reranking 500MShort Name: llama-3-2-nemoretriever-rerankqa-500m Training Dataset & Evaluation ============================= Training Dataset & Evaluation Training Dataset ---------------- Training Dataset The development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset namedMSMARCOrestricts ‌commercial licensing, limiting the use of these models in commercial settings. To address this, NVIDIA created its own training dataset blend based on public QA datasets, which each have a license for commercial applications. MSMARCO (https://microsoft.github.io/msmarco/) Data Collection Method by dataset: Automated, Unknown Data Collection Method by dataset Labeling Method by dataset:Automated, Unknown Labeling Method by dataset: Properties:This model was trained on 800k samples from public datasets. Properties: Evaluation Results ------------------ Evaluation Results We evaluate the pipelines on a set of evaluation benchmarks. We applied the ranking model to the candidates retrieved from a retrieval embedding model. Overall, the pipeline llama-3.2-nv-embedqa-1b-v2 + llama-3-2-nemoretriever-rerankqa-500m provides high BEIR+TechQA accuracy with multilingual and crosslingual support. The llama-3-2-nemoretriever-rerankqa-500m ranking model is 3.5x smaller than the nv-rerankqa-mistral-4b-v3 model. Data Collection Method by Dataset Labeling Method by Dataset We evaluated the NVIDIA Retrieval QA Embedding Model in comparison to literature open & commercial retriever models on academic benchmarks for question-answering -NQ,HotpotQAandFiQA (Finance Q&A)from BeIR benchmark and TechQA dataset. In this benchmark, the metric used was Recall@5. As described, we need to apply the ranking model on the output of an embedding model. NQ (https://huggingface.co/datasets/BeIR/nq) HotpotQA (https://huggingface.co/datasets/hotpot_qa) FiQA (Finance Q&A) (https://huggingface.co/datasets/BeIR/fiqa) llama-3-2-nemoretriever-rerankqa-500m 72.03% We evaluated the model’s multilingual capabilities on theMIRACLacademic benchmark - a multilingual retrieval dataset, across 15 languages, and on an additional 11 languages that were translated from the English and Spanish versions of MIRACL. The reported scores are based on a custom subsampled version by selecting hard negatives for each query to reduce the corpus size. MIRACL (https://github.com/project-miracl/miracl) llama-3-2-nemoretriever-rerankqa-500m 64.24% We evaluated the cross-lingual capabilities on the academic benchmarkMLQAbased on 7 languages (Arabic, Chinese, English, German, Hindi, Spanish, Vietnamese). We consider only evaluation datasets when the query and documents are in different languages. We calculate the average Recall@5 across the 42 different language pairs. MLQA (https://github.com/facebookresearch/MLQA/) llama-3-2-nemoretriever-rerankqa-500m 82.27% We evaluated the support of long documents on the academic benchmarkMultilingual Long-Document Retrieval (MLDR)built on Wikipedia and mC4, covering 12 typologically diverse languages . The English version has a median length of 2399 tokens and 90th percentile of 7483 tokens using the llama 3.2 tokenizer. Multilingual Long-Document Retrieval (MLDR) (https://huggingface.co/datasets/Shitao/MLDR) llama-3-2-nemoretriever-rerankqa-500m 65.39% PropertiesThe evaluation datasets are based on threeMTEB/BEIRTextQA datasets, the TechQA dataset, and MIRACL multilingual retrieval datasets, which are all public datasets. The sizes range between 10,000s up to 5M depending on the dataset. Properties MTEB/BEIR (https://github.com/beir-cellar/beir) Inference --------- Engine:TensorRTTest Hardware:A100 PCIe/SXM, and A10G Engine: Test Hardware: Ethical Considerations ====================== Ethical Considerations NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ subcards. For more detailed information on ethical considerations for this model Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "gemma-3n-e2b-it",
    "nvidia_url": "https://build.nvidia.com/google/gemma-3n-e2b-it",
    "scraped_on": "2025-10-15T21:39:17.243766",
    "tags": [
      "chat",
      "google",
      "language generation",
      "speech recognition",
      "visual qa"
    ],
    "model_card": "Gemma 3n e2b-it Overview ======================== Description =========== Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for pre-trained and instruction-tuned variants. These models were trained with data in over 140 spoken languages. This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. It has been produced to a third-party's requirements for this application and use-case. See the external card:Gemma 3n e2b-it Model Card. Gemma 3n e2b-it Model Card (https://huggingface.co/google/gemma-3n-e2b-it) License and Terms of Use: ------------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Gemma Terms of Use NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Gemma Terms of Use (https://ai.google.dev/gemma/terms) Deployment Geography: --------------------- Global Use Case: --------- Content Creation and Communication (Text Generation, Chatbots, Summarization, Image/Audio Data Extraction), Research and Education (NLP Research, Language Learning, Knowledge Exploration) Intended Usage ============== Open generative models have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development. • Content Creation and CommunicationText Generation: Generate creative text formats such as poems, scripts, code, marketing copy, and email drafts.Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications.Text Summarization: Generate concise summaries of a text corpus, research papers, or reports.Image Data Extraction: Extract, interpret, and summarize visual data for text communications.Audio Data Extraction: Transcribe spoken language, translate speech to text in other languages, and analyze sound-based data. • Text Generation: Generate creative text formats such as poems, scripts, code, marketing copy, and email drafts. Text Generation • Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications. Chatbots and Conversational AI • Text Summarization: Generate concise summaries of a text corpus, research papers, or reports. Text Summarization • Image Data Extraction: Extract, interpret, and summarize visual data for text communications. Image Data Extraction • Audio Data Extraction: Transcribe spoken language, translate speech to text in other languages, and analyze sound-based data. Audio Data Extraction • Research and EducationNatural Language Processing (NLP) and generative model Research: These models can serve as a foundation for researchers to experiment with generative models and NLP techniques, develop algorithms, and contribute to the advancement of the field.Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice.Knowledge Exploration: Assist researchers in exploring large bodies of data by generating summaries or answering questions about specific topics. • Natural Language Processing (NLP) and generative model Research: These models can serve as a foundation for researchers to experiment with generative models and NLP techniques, develop algorithms, and contribute to the advancement of the field. Natural Language Processing (NLP) and generative model Research • Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice. Language Learning Tools • Knowledge Exploration: Assist researchers in exploring large bodies of data by generating summaries or answering questions about specific topics. Knowledge Exploration Release Date: ------------- Build.NVIDIA.com:06/26/2025 via (link)Hugging Face:06/26/2025 via (link) Build.NVIDIA.com: link (https://build.nvidia.com/google/gemma-3n-e2b-it) Hugging Face: link (https://huggingface.co/google/gemma-3n-e2b-it) References: ----------- • Gemma 3n Model Overview Gemma 3n Model Overview (https://ai.google.dev/gemma/docs/gemma-3n) • Gemma 3n's Efficient Parameter Management Technology Gemma 3n's Efficient Parameter Management Technology (https://ai.google.dev/gemma/docs/gemma-3n#parameters) • Responsible Generative AI Toolkit Responsible Generative AI Toolkit (https://ai.google.dev/responsible) • Gemma on Kaggle Gemma on Kaggle (https://www.kaggle.com/models/google/gemma-3n) • Gemma on HuggingFace Gemma on HuggingFace (https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4) • Gemma on Vertex Model Garden Gemma on Vertex Model Garden (https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n) Model Architecture: =================== • Architecture Type: Matryoshka Transformer • Network Architecture: Matryoshka Transformer (MatFormer) • Parameter Count: 2B (base model), ~4.4B (with Per-Layer Embeddings) • Number of Layers: 30 • Notable Architectural Features: Selective parameter activation technology • Base Model: google/gemma-3n-e2b • Additional Notes: The model's full parameter count is higher than its base model size due to Per-Layer Embeddings (PLE). Standard implementations will load all parameters, including PLE, into VRAM. Input ----- • Input Type(s):Text, Image, Audio Input Type(s): • Input Formats:Text string, Images (normalized to 256x256, 512x512, or 768x768), Audio data (single channel) Input Formats: • Input Parameters:One Dimensional (1D), Two Dimensional (2D), Three Dimensional (3D) Input Parameters: • Other Properties Related to Input:Total input context of 32K tokens. Images are encoded to 256 tokens each. Audio data is encoded to 6.25 tokens per second from a single channel. Other Properties Related to Input: Output ------ • Output Type(s):Text Output Type(s): • Output Formats:Text Output Formats: • Output Parameters:1D Output Parameters: • Other Properties Related to Output:Total output length up to 32K tokens, subtracting the request input tokens. Other Properties Related to Output: Software Integration ==================== Supported Hardware Microarchitecture Compatibility: NVIDIA GPU Micro-architectures Suitable for ServingGemma 3nin Production ------------------------------------------------------------------------ Gemma 3n (≥ 16 GB VRAM + Tensor-core /mixed-precision support) Blackwell Hopper Ada Lovelace Ampere Turing Volta Pascal (edge case) No Recommendation:Start withAmpere or newerfor production workloads that demand real-time multimodal responses or higher concurrency. Turing/Volta can host smaller replica pools; Pascal is generallynotadvised for new deployments. Recommendation: Ampere or newer not Model Version: ============== gemma-3n-e2b-it v1.0 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ----------------- • Data Collection Method: Hybrid: Automated, Synthetic, Human • Labeling Method: Undisclosed • Properties: A diverse collection of web text in over 140 languages, code, mathematics, images, and audio, totalling approximately 11 trillion tokens. Knowledge cutoff date is June 2024. Data Preprocessing ------------------ Here are the key data cleaning and filtering methods applied to the training data: • CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content. CSAM Filtering • Sensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets. Sensitive Data Filtering • Additional methods: Filtering based on content quality and safety in line withour policies. Additional methods our policies (https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf) Testing Dataset: ---------------- • Data Collection Method by dataset: Hybrid: Automated, Synthetic, Human • Labeling Method by dataset: Undisclosed • Properties: Undisclosed Evaluation Benchmark Results: ----------------------------- • Data Collection Method: Undisclosed • Labeling Method: Undisclosed • Properties: Benchmarks include: HellaSwag, BoolQ, PIQA, SocialIQA, TriviaQA, Natural Questions, ARC-c, ARC-e, WinoGrande, BIG-Bench Hard, DROP, MGSM, WMT24++, Include, MMLU, GPQA Diamond, LiveCodeBench, Codegolf, AIME 2025, MBPP, HumanEval, HiddenMath, Global-MMLU-Lite. Model evaluation metrics and results. Benchmark Results ----------------- These models were evaluated at full precision (float32) against a large collection of different datasets and metrics to cover different aspects of content generation. Evaluation results marked withITare for instruction-tuned models. Evaluation results marked withPTare for pre-trained models. IT PT Reasoning and factuality HellaSwag (https://arxiv.org/abs/1905.07830) BoolQ (https://arxiv.org/abs/1905.10044) PIQA (https://arxiv.org/abs/1911.11641) SocialIQA (https://arxiv.org/abs/1904.09728) TriviaQA (https://arxiv.org/abs/1705.03551) Natural Questions (https://github.com/google-research-datasets/natural-questions) ARC-c (https://arxiv.org/abs/1911.01547) ARC-e (https://arxiv.org/abs/1911.01547) WinoGrande (https://arxiv.org/abs/1907.10641) BIG-Bench Hard (https://paperswithcode.com/dataset/bbh) DROP (https://arxiv.org/abs/1903.00161) Multilingual ------------ MGSM (https://arxiv.org/abs/2210.03057) WMT24++ (https://arxiv.org/abs/2502.12404v1) Include (https://arxiv.org/abs/2411.19799) MMLU (https://arxiv.org/abs/2009.03300) OpenAI MMLU (https://huggingface.co/datasets/openai/MMMLU) Global-MMLU (https://huggingface.co/datasets/CohereLabs/Global-MMLU) ECLeKTic (https://arxiv.org/abs/2502.21228) STEM and code ------------- GPQA (https://arxiv.org/abs/2311.12022) LiveCodeBench (https://arxiv.org/abs/2403.07974) AIME 2025 (https://www.vals.ai/benchmarks/aime-2025-05-09) Additional benchmarks --------------------- MMLU (https://arxiv.org/abs/2009.03300) MBPP (https://arxiv.org/abs/2108.07732) HumanEval (https://arxiv.org/abs/2107.03374) LiveCodeBench (https://arxiv.org/abs/2403.07974) Global-MMLU-Lite (https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite) MMLU (https://arxiv.org/abs/2009.03300) Evaluation Approach ------------------- Our evaluation methods include structured evaluations and internal red-teaming testing of relevant content policies. Red-teaming was conducted by a number of different teams, each with different goals and human evaluation metrics. These models were evaluated against a number of different categories relevant to ethics and safety, including: • Child Safety: Evaluation of text-to-text and image to text prompts covering child safety policies, including child sexual abuse and exploitation. Child Safety • Content Safety:Evaluation of text-to-text and image to text prompts covering safety policies including, harassment, violence and gore, and hate speech. Content Safety: • Representational Harms: Evaluation of text-to-text and image to text prompts covering safety policies including bias, stereotyping, and harmful associations or inaccuracies. Representational Harms In addition to development level evaluations, we conduct \"assurance evaluations\" which are our 'arms-length' internal evaluations for responsibility governance decision making. They are conducted separately from the model development team, to inform decision making about release. High level findings are fed back to the model team, but prompt sets are held-out to prevent overfitting and preserve the results' ability to inform decision making. Notable assurance evaluation results are reported to our Responsibility & Safety Council as part of release review. Evaluation Results ------------------ For all areas of safety testing, we saw safe levels of performance across the categories of child safety, content safety, and representational harms relative to previous Gemma models. All testing was conducted without safety filters to evaluate the model capabilities and behaviors. For text-to-text, image-to-text, and audio-to-text, and across all model sizes, the model produced minimal policy violations, and showed significant improvements over previous Gemma models' performance with respect to high severity violations. A limitation of our evaluations was they included primarily English language prompts. Inference: ========== Acceleration Engine:vLLMTest Hardware:L40s Acceleration Engine: Test Hardware: Usage ----- Below, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3n is supported starting from transformers 4.53.0. Then, copy the snippet from the section that is relevant for your use case. Running with thepipelineAPI --------------------------- You can initialize the model and processor for inference withpipelineas follows. Running the model on a single GPU --------------------------------- Additional Details: =================== Usage and Limitations --------------------- These models have certain limitations that users should be aware of. Limitations ----------- • Training DataThe quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses.The scope of the training dataset determines the subject areas the model can handle effectively. • The quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses. • The scope of the training dataset determines the subject areas the model can handle effectively. • Context and Task ComplexityModels are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging.A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • Models are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging. • A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • Language Ambiguity and NuanceNatural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language. • Natural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language. • Factual AccuracyModels generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • Models generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • Common SenseModels rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. • Models rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. Risks identified and mitigations: • Perpetuation of biases: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases. Perpetuation of biases • Generation of harmful content: Mechanisms and guidelines for content safety are essential. Developers are encouraged to exercise caution and implement appropriate content safety safeguards based on their specific product policies and application use cases. Generation of harmful content • Misuse for malicious purposes: Technical limitations and developer and end-user education can help mitigate against malicious applications of generative models. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in theGemma Prohibited Use Policy. Misuse for malicious purposes Gemma Prohibited Use Policy (https://ai.google.dev/gemma/prohibited_use_policy) • Privacy violations: Models were trained on data filtered for removal of certain personal information and other sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques. Privacy violations Benefits -------- At the time of release, this family of models provides high-performance open generative model implementations designed from the ground up for responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown to provide superior performance to other, comparably-sized open model alternatives. Ethical Considerations: ======================= NVIDIA believes Trustworthy Al is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Citation ========"
  },
  {
    "name": "gemma-3n-e4b-it",
    "nvidia_url": "https://build.nvidia.com/google/gemma-3n-e4b-it",
    "scraped_on": "2025-10-15T21:39:16.892503",
    "tags": [
      "chat",
      "google",
      "language generation",
      "speech recognition",
      "visual qa"
    ],
    "model_card": "Gemma 3n e4b-it Overview ======================== Description =========== Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for pre-trained and instruction-tuned variants. These models were trained with data in over 140 spoken languages. This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. It has been produced to a third-party's requirements for this application and use-case. See the external card:Gemma 3n e4b-it Model Card. Gemma 3n e4b-it Model Card (https://huggingface.co/google/gemma-3n-e4b-it) License and Terms of Use: ------------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Gemma Terms of Use NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Gemma Terms of Use (https://ai.google.dev/gemma/terms) Deployment Geography: --------------------- Global Use Case: --------- Content Creation and Communication (Text Generation, Chatbots, Summarization, Image/Audio Data Extraction), Research and Education (NLP Research, Language Learning, Knowledge Exploration) Intended Usage -------------- Open generative models have a wide range of applications across various industries and domains. The following list of potential uses is not comprehensive. The purpose of this list is to provide contextual information about the possible use-cases that the model creators considered as part of model training and development. • Content Creation and CommunicationText Generation: Generate creative text formats such as poems, scripts, code, marketing copy, and email drafts.Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications.Text Summarization: Generate concise summaries of a text corpus, research papers, or reports.Image Data Extraction: Extract, interpret, and summarize visual data for text communications.Audio Data Extraction: Transcribe spoken language, translate speech to text in other languages, and analyze sound-based data. • Text Generation: Generate creative text formats such as poems, scripts, code, marketing copy, and email drafts. Text Generation • Chatbots and Conversational AI: Power conversational interfaces for customer service, virtual assistants, or interactive applications. Chatbots and Conversational AI • Text Summarization: Generate concise summaries of a text corpus, research papers, or reports. Text Summarization • Image Data Extraction: Extract, interpret, and summarize visual data for text communications. Image Data Extraction • Audio Data Extraction: Transcribe spoken language, translate speech to text in other languages, and analyze sound-based data. Audio Data Extraction • Research and EducationNatural Language Processing (NLP) and generative model Research: These models can serve as a foundation for researchers to experiment with generative models and NLP techniques, develop algorithms, and contribute to the advancement of the field.Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice.Knowledge Exploration: Assist researchers in exploring large bodies of data by generating summaries or answering questions about specific topics. • Natural Language Processing (NLP) and generative model Research: These models can serve as a foundation for researchers to experiment with generative models and NLP techniques, develop algorithms, and contribute to the advancement of the field. Natural Language Processing (NLP) and generative model Research • Language Learning Tools: Support interactive language learning experiences, aiding in grammar correction or providing writing practice. Language Learning Tools • Knowledge Exploration: Assist researchers in exploring large bodies of data by generating summaries or answering questions about specific topics. Knowledge Exploration Release Date: ------------- Build.NVIDIA.com:06/26/2025 via (link)Hugging Face:06/26/2025 via (link) Build.NVIDIA.com: link (https://build.nvidia.com/google/gemma-3n-e4b-it) Hugging Face: link (https://huggingface.co/google/gemma-3n-e4b-it) References: ----------- • Gemma 3n model overview Gemma 3n model overview (https://ai.google.dev/gemma/docs/gemma-3n) • Gemma 3n's efficient parameter management technolog Gemma 3n's efficient parameter management technolog (https://ai.google.dev/gemma/docs/gemma-3n#parameters) • Responsible Generative AI Toolkit Responsible Generative AI Toolkit (https://ai.google.dev/responsible) • Gemma on Kaggle Gemma on Kaggle (https://www.kaggle.com/models/google/gemma-3n) • Gemma on HuggingFace Gemma on HuggingFace (https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4) • Gemma on Vertex Model Garden Gemma on Vertex Model Garden (https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n) Model Architecture: =================== • Architecture Type: Matryoshka Transformer • Network Architecture: Matryoshka Transformer (MatFormer) • Parameter Count: 4B (base model), ~6.8B (with Per-Layer Embeddings) • Number of Layers: 35 • Notable Architectural Features: Selective parameter activation technology • Base Model: google/gemma-3n-e4b • Additional Notes: The model's full parameter count is higher than its base model size due to Per-Layer Embeddings (PLE). Standard implementations will load all parameters, including PLE, into VRAM. Input ----- • Input Type(s):Text, Image, Audio Input Type(s): • Input Formats:Text string, Images (normalized to 256x256, 512x512, or 768x768), Audio data (single channel) Input Formats: • Input Parameters:1D, 2D, 3D Input Parameters: • Other Properties Related to Input:Total input context of 32K tokens. Images are encoded to 256 tokens each. Audio data is encoded to 6.25 tokens per second from a single channel. Other Properties Related to Input: Output ------ • Output Type(s):Text Output Type(s): • Output Formats:Text Output Formats: • Output Parameters:1D Output Parameters: • Other Properties Related to Output:Total output length up to 32K tokens, subtracting the request input tokens. Other Properties Related to Output: Software Integration ==================== Supported Hardware Microarchitecture Compatibility: NVIDIA GPU Micro-architectures Suitable for ServingGemma 3nin Production ------------------------------------------------------------------------ Gemma 3n (≥ 16 GB VRAM + Tensor-core /mixed-precision support) Blackwell Hopper Ada Lovelace Ampere Turing Volta Pascal (edge case) No Recommendation:Start withAmpere or newerfor production workloads that demand real-time multimodal responses or higher concurrency. Turing/Volta can host smaller replica pools; Pascal is generallynotadvised for new deployments. Recommendation: Ampere or newer not Model Version: ============== gemma-3n-e4b-it v1.0 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ----------------- • Data Collection Method: Hybrid: Automated, Synthetic, Human • Labeling Method: Undisclosed • Properties: A diverse collection of web text in over 140 languages, code, mathematics, images, and audio, totalling approximately 11 trillion tokens. Knowledge cutoff date is June 2024. Data Preprocessing ------------------ Here are the key data cleaning and filtering methods applied to the training data: • CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was applied at multiple stages in the data preparation process to ensure the exclusion of harmful and illegal content. CSAM Filtering • Sensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets. Sensitive Data Filtering • Additional methods: Filtering based on content quality and safety in line withour policies. Additional methods our policies (https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf) Testing Dataset: ---------------- • Data Collection Method by dataset: Hybrid: Automated, Synthetic, Human • Labeling Method by dataset: Undisclosed • Properties: Undisclosed Evaluation Benchmark Results: ----------------------------- • Data Collection Method: Undisclosed • Labeling Method: Undisclosed • Properties: Benchmarks include: HellaSwag, BoolQ, PIQA, SocialIQA, TriviaQA, Natural Questions, ARC-c, ARC-e, WinoGrande, BIG-Bench Hard, DROP, MGSM, WMT24++, Include, MMLU, GPQA Diamond, LiveCodeBench, Codegolf, AIME 2025, MBPP, HumanEval, HiddenMath, Global-MMLU-Lite. Model evaluation metrics and results. Benchmark Results ----------------- These models were evaluated at full precision (float32) against a large collection of different datasets and metrics to cover different aspects of content generation. Evaluation results marked withITare for instruction-tuned models. Evaluation results marked withPTare for pre-trained models. IT PT Reasoning and factuality HellaSwag (https://arxiv.org/abs/1905.07830) BoolQ (https://arxiv.org/abs/1905.10044) PIQA (https://arxiv.org/abs/1911.11641) SocialIQA (https://arxiv.org/abs/1904.09728) TriviaQA (https://arxiv.org/abs/1705.03551) Natural Questions (https://github.com/google-research-datasets/natural-questions) ARC-c (https://arxiv.org/abs/1911.01547) ARC-e (https://arxiv.org/abs/1911.01547) WinoGrande (https://arxiv.org/abs/1907.10641) BIG-Bench Hard (https://paperswithcode.com/dataset/bbh) DROP (https://arxiv.org/abs/1903.00161) Multilingual ------------ MGSM (https://arxiv.org/abs/2210.03057) WMT24++ (https://arxiv.org/abs/2502.12404v1) Include (https://arxiv.org/abs/2411.19799) MMLU (https://arxiv.org/abs/2009.03300) OpenAI MMLU (https://huggingface.co/datasets/openai/MMMLU) Global-MMLU (https://huggingface.co/datasets/CohereLabs/Global-MMLU) ECLeKTic (https://arxiv.org/abs/2502.21228) STEM and code ------------- GPQA (https://arxiv.org/abs/2311.12022) LiveCodeBench (https://arxiv.org/abs/2403.07974) AIME 2025 (https://www.vals.ai/benchmarks/aime-2025-05-09) Additional benchmarks --------------------- MMLU (https://arxiv.org/abs/2009.03300) MBPP (https://arxiv.org/abs/2108.07732) HumanEval (https://arxiv.org/abs/2107.03374) LiveCodeBench (https://arxiv.org/abs/2403.07974) Global-MMLU-Lite (https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite) MMLU (https://arxiv.org/abs/2009.03300) Evaluation Approach ------------------- Our evaluation methods include structured evaluations and internal red-teaming testing of relevant content policies. Red-teaming was conducted by a number of different teams, each with different goals and human evaluation metrics. These models were evaluated against a number of different categories relevant to ethics and safety, including: • Child Safety: Evaluation of text-to-text and image to text prompts covering child safety policies, including child sexual abuse and exploitation. Child Safety • Content Safety:Evaluation of text-to-text and image to text prompts covering safety policies including, harassment, violence and gore, and hate speech. Content Safety: • Representational Harms: Evaluation of text-to-text and image to text prompts covering safety policies including bias, stereotyping, and harmful associations or inaccuracies. Representational Harms In addition to development level evaluations, we conduct \"assurance evaluations\" which are our 'arms-length' internal evaluations for responsibility governance decision making. They are conducted separately from the model development team, to inform decision making about release. High level findings are fed back to the model team, but prompt sets are held-out to prevent overfitting and preserve the results' ability to inform decision making. Notable assurance evaluation results are reported to our Responsibility & Safety Council as part of release review. Evaluation Results ------------------ For all areas of safety testing, we saw safe levels of performance across the categories of child safety, content safety, and representational harms relative to previous Gemma models. All testing was conducted without safety filters to evaluate the model capabilities and behaviors. For text-to-text, image-to-text, and audio-to-text, and across all model sizes, the model produced minimal policy violations, and showed significant improvements over previous Gemma models' performance with respect to high severity violations. A limitation of our evaluations was they included primarily English language prompts. Inference: ========== Acceleration Engine:vLLMTest Hardware:L40s Acceleration Engine: Test Hardware: Usage ----- Below, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3n is supported starting from transformers 4.53.0. Then, copy the snippet from the section that is relevant for your use case. Running with thepipelineAPI --------------------------- You can initialize the model and processor for inference withpipelineas follows. Running the model on a single GPU --------------------------------- Additional Details: =================== Usage and Limitations --------------------- These models have certain limitations that users should be aware of. Limitations ----------- • Training DataThe quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses.The scope of the training dataset determines the subject areas the model can handle effectively. • The quality and diversity of the training data significantly influence the model's capabilities. Biases or gaps in the training data can lead to limitations in the model's responses. • The scope of the training dataset determines the subject areas the model can handle effectively. • Context and Task ComplexityModels are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging.A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • Models are better at tasks that can be framed with clear prompts and instructions. Open-ended or highly complex tasks might be challenging. • A model's performance can be influenced by the amount of context provided (longer context generally leads to better outputs, up to a certain point). • Language Ambiguity and NuanceNatural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language. • Natural language is inherently complex. Models might struggle to grasp subtle nuances, sarcasm, or figurative language. • Factual AccuracyModels generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • Models generate responses based on information they learned from their training datasets, but they are not knowledge bases. They may generate incorrect or outdated factual statements. • Common SenseModels rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. • Models rely on statistical patterns in language. They might lack the ability to apply common sense reasoning in certain situations. Risks identified and mitigations: • Perpetuation of biases: It's encouraged to perform continuous monitoring (using evaluation metrics, human review) and the exploration of de-biasing techniques during model training, fine-tuning, and other use cases. Perpetuation of biases • Generation of harmful content: Mechanisms and guidelines for content safety are essential. Developers are encouraged to exercise caution and implement appropriate content safety safeguards based on their specific product policies and application use cases. Generation of harmful content • Misuse for malicious purposes: Technical limitations and developer and end-user education can help mitigate against malicious applications of generative models. Educational resources and reporting mechanisms for users to flag misuse are provided. Prohibited uses of Gemma models are outlined in theGemma Prohibited Use Policy. Misuse for malicious purposes Gemma Prohibited Use Policy (https://ai.google.dev/gemma/prohibited_use_policy) • Privacy violations: Models were trained on data filtered for removal of certain personal information and other sensitive data. Developers are encouraged to adhere to privacy regulations with privacy-preserving techniques. Privacy violations Benefits -------- At the time of release, this family of models provides high-performance open generative model implementations designed from the ground up for responsible AI development compared to similarly sized models. Using the benchmark evaluation metrics described in this document, these models have shown to provide superior performance to other, comparably-sized open model alternatives. Ethical Considerations: ======================= NVIDIA believes Trustworthy Al is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Citation ========"
  },
  {
    "name": "riva-translate-1.6b",
    "nvidia_url": "https://build.nvidia.com/nvidia/riva-translate-1_6b",
    "scraped_on": "2025-10-15T21:39:16.533736",
    "tags": [
      "neural machine translation",
      "nvidia",
      "nvidia nim",
      "text translation"
    ],
    "model_card": "Description: ============ The Riva Translate 1.6B Multilingual Neural Machine Translation model translates text in any to any directions across the 37 supported languages, including non-English centric translation (such as French to Chinese, etc). The Supported languages are: English(en), Czech(cs), Danish (da), German(de), Greek(el), European Spanish(es-ES), LATAM Spanish(es-US), Finnish(fi), France(fr), Hungarian(hu), Italian(it), Lithuanian(lt), Latvian(lv),Dutch(nl), Norwegian(no), Polish(pl), European Portuguese(pt-PT), Brazillian Portuguese(pt-BR), Romanian(ro), Russian(ru), Slovak(sk), Swedish(sv), Simplified Chinese(zh-CN), Traditional Chinese(zh-TW), Japanese(ja), Hindi(hi), Korean(ko), Estonian(et), Slovenian(sl), Bulgarian(bg), Ukrainian(uk), Croatian(hr), Arabic(ar), Vietnamese(vi), Turkish(tr), Indonesian(id), Thai(th). This model is ready for commercial use. Model Architecture ================== Architecture Type: Transformer Network Architecture: Megatron The model is based on Transformer architecture originally presented in \"Attention Is All You Need\" paper [1]. In this particular instance, the model has 24 layers in the encoder and 24 layers in the decoder. It is using SentencePiece tokenizer [2]. Input: ====== Input Type(s):Text StringInput Format(s):List Input Type(s): Input Format(s): Other Properties Related to Input:No Pre-Processing Needed; No Tokenization required; 512 Character Text String Limit (No non-textual characters) Other Properties Related to Input: Output: ======= Output Type(s):Text StringOutput Format:ListOutput Parameters:Selected LanguageOther Properties Related to Output:Outputs are not tokenized or processed to hide sensitive input information Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Training & Evaluation Dataset: ============================== ** Data Collection Method by dataset • Human ** Labeling Method by dataset • Automated Properties (Quantity, Dataset Descriptions, Sensor(s)):This model is trained on open-sourced datasets and synthetic datasets of text parallel corpora generated via back-translation. Properties (Quantity, Dataset Descriptions, Sensor(s)): References: =========== [1] Vaswani, Ashish, et al. \"Attention is all you need.\" arXiv preprint arXiv:1706.03762 (2017). [2]https://github.com/google/sentencepiece[3]https://en.wikipedia.org/wiki/BLEU[4]https://github.com/mjpost/sacreBLEU[5] NVIDIA NeMo Toolkit https://github.com/google/sentencepiece https://en.wikipedia.org/wiki/BLEU https://github.com/mjpost/sacreBLEU Software Integration ==================== Runtime Engine(s): • Riva 2.19.0 or Higher Supported Operating System(s): • Linux Model Version(s): ================= riva-translate-1.6b Inference ========= Engine:TritonTest Hardware: Engine: Test Hardware: • NVIDIA H100 GPU • NVIDIA A100 GPU • NVIDIA L40 GPU Ethical Considerations (For NVIDIA Models Only): ================================================ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) GOVERNING TERMS: ================ This trial is governed by the NVIDIA API Trial Terms of Service (found athttps://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf). The use of this model is governed by the AI Foundation Models Community License Agreement (found at NVIDIA Agreements | Enterprise Software | NVIDIA AI Foundation Models Community License Agreement). https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf"
  },
  {
    "name": "llama-guard-4-12b",
    "nvidia_url": "https://build.nvidia.com/meta/llama-guard-4-12b",
    "scraped_on": "2025-10-15T21:39:13.196166",
    "tags": [
      "content moderator",
      "content safety",
      "guardrail",
      "llm multimodal safety",
      "meta"
    ],
    "model_card": "Llama-Guard-4-12B Overview ========================== Description: ============ Llama-Guard-4-12B is a 12-billion parameter, dense, multimodal safety classifier developed by Meta. It is designed to evaluate both text and image inputs for safety, classifying content in large language model (LLM) prompts and responses. The model outputs text indicating whether the content is safe or unsafe, and if unsafe, it lists the violated content categories. This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. It has been developed and built to Meta's requirements for safety classification in multimodal LLM applications.See theLlama-Guard-4-12B Model Cardfor more details. Llama-Guard-4-12B Model Card (https://huggingface.co/meta-llama/Llama-Guard-4-12B/blob/main/README.md) License and Terms of Use: ------------------------- GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service; and the use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:Llama 4 Community Model License. Built with Llama. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Llama 4 Community Model License (https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) Deployment Geography: --------------------- Global Use Case: --------- Intended for developers and organizations seeking to implement safety classification in LLM applications, including filtering of unsafe content in both inputs and outputs across text and image modalities. Hazard Taxonomy and Policy ========================== The model is trained to predict safety labels on the categories shown below, based on the MLCommons safety taxonomy. We include an additional category, Code Interpreter Abuse, for text-only tool-call use cases. S1 - Violent Crimes Responses that enable, encourage, or endorse the commission of violent crimes, including: (1) unlawful violence toward people (ex: terrorism, genocide, murder, hate-motivated violent crimes, child abuse, assault, battery, kidnapping) and (2) unlawful violence toward animals (ex: animal abuse) S2 - Non-Violent Crimes Responses that enable, encourage, or endorse the commission of non-violent crimes, including: (1) personal crimes (ex: labor trafficking, threats, intimidation, hate-motivated non-violent crimes), (2) financial crimes (ex: fraud, scams, money laundering), (3) property crimes (ex: theft, arson, vandalism), (4) drug crimes (ex: creating or using regulated substances), (5) weapons crimes (ex: producing unlicensed firearms), and (6) cyber crimes (ex: hacking) S3 - Sex-Related Crimes Responses that enable, encourage, or endorse the commission of sex-related crimes, including: (1) sex trafficking, (2) sexual assault (ex: rape), (3) sexual harassment (physical, verbal, or visual), and (4) prostitution S4 - Child Sexual Exploitation Responses that contain, describe, enable, encourage, or endorse the sexual abuse of children S5 - Defamation Responses that are both verifiably false and likely to injure a living person’s reputation S6 - Specialized Advice Responses that contain specialized financial, medical, or legal advice, or that indicate dangerous activities or objects are safe S7 - Privacy Responses that contain sensitive, nonpublic personal information that could undermine someone’s physical, digital, or financial security S8 - Intellectual Property Responses that may violate the intellectual property rights of any third party S9 - Indiscriminate Weapons Responses that enable, encourage, or endorse the creation of indiscriminate weapons, including: (1) chemical weapons (ex: nerve gas), (2) biological weapons (ex: anthrax), (3) radiological weapons (ex: cobalt bombs), (4) nuclear weapons (ex: fission bombs), and (5) high-yield explosive weapons (ex: cluster munitions) S10 - Hate Responses that demean or dehumanize people on the basis of their sensitive, personal characteristics (i.e., race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity, and/or serious disease) S11 - Suicide & Self-Harm Responses that enable, encourage, or endorse acts of intentional self-harm, including: (1) suicide, (2) self-injury (ex: cutting), and (3) disordered eating S12 - Sexual Content Responses that contain erotica S13 - Elections Responses that contain factually incorrect information about electoral systems and processes, including in the time, place, or manner of voting in civic elections S14 - Code Interpreter Abuse (text-only) Responses that seek to abuse code interpreters, including those that enable denial of service attacks, container escapes or privilege escalation exploits Release Date: ------------- • Build.Nvidia.com: May, 2025 • Hugging Face: April 29, 2025 Reference(s): ============= • The Llama 3 Herd of Models The Llama 3 Herd of Models (https://arxiv.org/pdf/2407.21783) • Developer Use Guide Developer Use Guide (https://www.llama.com/developer-use-guide/) • Getting Started Guide Getting Started Guide (https://www.llama.com/docs/model-cards-and-prompt-formats/llama-guard-4/) • Llama Prompt Guard 2 Llama Prompt Guard 2 (https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Prompt-Guard-2/86M/MODEL_CARD.md) Model Architecture: =================== • Architecture Type:Transformer-based Architecture Type: • Network Architecture:Dense feedforward early-fusion architecture, pruned from Llama 4 Scout by removing routed experts and router layers, retaining only the shared expert. Network Architecture: • This model was developed based on:Llama 4 Scout This model was developed based on: • This model has:12 billion model parameters This model has: Input: ------ • Input Type(s):Text, Image Input Type(s): • Input Format(s):String (text), Image files Input Format(s): • Input Parameters:2D Input Parameters: • Other Properties Related to Input:Supports multiple images in prompts; multilingual text support Other Properties Related to Input: Output: ------- • Output Type(s):Text Output Type(s): • Output Format:String Output Format: • Output Parameters:1D Output Parameters: • Other Properties Related to Output:Generates text indicating safety classification and lists violated content categories if applicable Other Properties Related to Output: Our Al models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Supported Hardware Microarchitecture Compatibility: =================================================== • NVIDIA Ampere • NVIDIA Hopper • NVIDIA Lovelace • NVIDIA Turing • NVIDIA Volta Preferred/Supported Operating System(s): ======================================== • Linux • Windows Model Version(s): ================= Llama-Guard-4-12B v1.0 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ----------------- • Data Collection Method:Human-annotated multilingual data and multi-image training data Data Collection Method: • Labeling Method:Human Labeling Method: • Properties:Trained on a 3:1 ratio of text-only to multimodal data Properties: Testing Dataset: ---------------- • Data Collection Method:Undisclosed Data Collection Method: • Labeling Method:Undisclosed Labeling Method: • Properties:Undisclosed Properties: Evaluation Dataset: ------------------- • Benchmark Score:Undisclosed Benchmark Score: • Data Collection Method:Undisclosed Data Collection Method: • Properties:Undisclosed Properties: Pretraining and Pruning ======================= Llama Guard 4 employs a dense feedforward early-fusion architecture, and it differs from Llama 4 Scout, which employs Mixture-of-Experts (MoE) layers. In order to leverage Llama 4’s pre-training, we develop a method to prune the pre-trained Llama 4 Scout mixture-of-experts architecture into a dense one, and we perform no additional pre-training. We take the pre-trained Llama 4 Scout checkpoint, which consists of one shared dense expert and sixteen routed experts in each Mixture-of-Experts layer. We prune all the routed experts and the router layers, retaining only the shared expert. After pruning, the Mixture-of-Experts is reduced to a dense feedforward layer initiated from the shared expert weights. Post-Training for Safety Classification ======================================= We post-trained the model after pruning with a blend of data from theLlama Guard 3-8BandLlama Guard 3-11B-visionmodels, with the following additional data: Llama Guard 3-8B (https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard3/8B/README.md) Llama Guard 3-11B-vision (https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard3/11B-vision/README.md) • Multi-image training data, with most samples containing from 2 to 5 images • Multilingual data, both written by expert human annotators and translated from English We blend the training data from both modalities, with a ratio of roughly 3:1 text-only data to multimodal data containing one or more images. Evaluation ========== System-level safety ------------------- Llama Guard 4 is designed to be used in an integrated system with a generative language model, reducing the overall rate of safety violations exposed to the user. Llama Guard 4 can be used for input filtering, output filtering, or both: input filtering relies on classifying the user prompts into an LLM as safe or unsafe, and output filtering relies on classifying an LLM’s generated output as safe or unsafe. The advantage of using input filtering is that unsafe content can be caught very early, before the LLM even responds, but the advantage of using output filtering is that the LLM is given a chance to potentially respond to an unsafe prompt in a safe way, and thus the final output from the model shown to the user would only be censored if it is found to itself be unsafe. Using both filtering types gives additional security. In some internal tests we have found that input filtering reduces safety violation rate and raises overall refusal rate more than output filtering does, but your experience may vary. We find that Llama Guard 4 roughly matches or exceeds the overall performance of the Llama Guard 3 models on both input and output filtering, for English and multilingual text and for mixed text and images. Classifier performance ---------------------- The tables below demonstrate how Llama Guard 4 matches or exceeds the overall performance of Llama Guard 3-8B (LG3) on English and multilingual text, as well as Llama Guard 3-11B-vision (LG3v) on prompts with single or multiple images, using in-house test set: R: recall, FPR: false positive rate. Values are from output filtering, flagging model outputs as either safe or unsafe. All values are an average over samples from safety categories S1 through S13 listed above, weighting each category equally, except for multilinguality, for which it is an average over the 7 shipped non-English languages of Llama Guard 3-8B: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. For multi-image prompts, only the final image was input into Llama Guard 3-11B-vision, which does not support multiple images. We omit evals against competitor models, which are typically not aligned with the specific safety policy that this classifier was trained on, prohibiting the ability to make direct comparisons. Getting Started with transformers ================================= You can get started with the model by running the following. Make sure you have the transformers release for Llama Guard 4 and hf_xet locally. Here's a basic snippet. For multi-turn and image-text inference, please refer to therelease blog release blog (https://huggingface.co/blog/llama-guard-4) Limitations =========== There are some limitations associated with Llama Guard 4. First, the classifier itself is an LLM fine-tuned on Llama 4, and thus its performance (e.g., judgments that need common-sense knowledge, multilingual capabilities, and policy coverage) might be limited by its (pre-)training data. Some hazard categories may require factual, up-to-date knowledge to be evaluated fully (for example, [S5] Defamation, [S8] Intellectual Property, and [S13] Elections). We believe that more complex systems should be deployed to accurately moderate these categories for use cases highly sensitive to these types of hazards, but that Llama Guard 4 provides a good baseline for generic use cases. Note that the performance of Llama Guard 4 was tested mostly with prompts containing a few images (three, most frequently), so performance may vary if using it to classify safety with a much larger number of images. Lastly, as an LLM, Llama Guard 4 may be susceptible to adversarial attacks or prompt injection attacks that could bypass or alter its intended use: seeLlama Prompt Guard 2for detecting prompt attacks. Please feel free toreportvulnerabilities, and we will look into incorporating improvements into future versions of Llama Guard. Llama Prompt Guard 2 (https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Prompt-Guard-2/86M/MODEL_CARD.md) report (https://github.com/meta-llama/PurpleLlama) Please refer to theDeveloper Use Guidefor additional best practices and safety considerations. Developer Use Guide (https://www.llama.com/developer-use-guide/) Inference: ========== • Engine:vLLM Engine: • Test Hardware:NVIDIA Lovelace Test Hardware: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and has established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "riva-translate-4b-instruct",
    "nvidia_url": "https://build.nvidia.com/nvidia/riva-translate-4b-instruct",
    "scraped_on": "2025-10-15T21:39:12.867856",
    "tags": [
      "chat",
      "nvidia",
      "text translation"
    ],
    "model_card": "Model Overview ============== Description: ------------ The Riva-Translate-4B-Instruct Neural Machine Translation model translates text in 12 languages. The supported languages are: English(en), German(de), European Spanish(es-ES), LATAM Spanish(es-US), France(fr), Brazillian Portugese(pt-BR), Russian(ru), Simplified Chinese(zh-CN), Traditional Chinese(zh-TW), Japanese(ja),Korean(ko), Arabic(ar). This model is ready for commercial use. License/Terms of Use -------------------- NIM Package:NVIDIA AI Foundation Models Community License AgreementDownloadable NIM:NVIDIA AI Foundation Models Community License AgreementHuggingFace Model:NVIDIA Open Model License AgreementModel preview in API catalog:NVIDIA Open Model License Agreement NVIDIA AI Foundation Models Community License Agreement (https://docs.nvidia.com/ai-foundation-models-community-license.pdf) NVIDIA Open Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Deployment Geography: --------------------- Global Use Case: --------- Translators, marketers, and web developers who deliver content in multiple languages. Release Date: ------------- Huggingface 06/05/2025 viahttps://huggingface.co/nvidia/Riva-Translate-4B-Instruct https://huggingface.co/nvidia/Riva-Translate-4B-Instruct References(s): ============== [1] Vaswani, Ashish, et al. \"Attention is all you need.\" arXiv preprint arXiv:1706.03762 (2017). [2]https://github.com/openai/tiktoken[3]https://en.wikipedia.org/wiki/BLEU[4]https://github.com/mjpost/sacreBLEU[5]https://github.com/Unbabel/COMET[6] NVIDIA NeMo Toolkit https://github.com/openai/tiktoken https://en.wikipedia.org/wiki/BLEU https://github.com/mjpost/sacreBLEU https://github.com/Unbabel/COMET Model Architecture: =================== Architecture Type:Transformer Architecture Type: Network Architecture:Decoder-only Network Architecture: This model was developed based on Transformer architecture originally presented in \"Attention Is All You Need\" paper [1]. It is a fine-tuned version of a 4B Base model that was pruned and distilled from nvidia/Mistral-NeMo-Minitron-8B-Base using our LLM compression technique. The model was trained using a multi-stage CPT and SFT. It uses tiktoken [2] as the tokenizer. The model supports a context length of 8K tokens. Input: ====== Input Type(s):TextInput Format:StringInput Parameters:One-Dimensional (1D)Other Properties Related to Input:This model supports a context length of 8K. Input Type(s): Input Format: Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:One-Dimensional (1D)Other Properties Related to Output:This model supports a context length of 8K.Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Prompt Format: ============== We recommend using the following prompt template, which was used to fine-tune the model. The model may not perform optimally without it. Performance: ============ COMET score of any2en and en2any direction for Flores-101 dataset Software Integration: ===================== Runtime Engine(s):NeMo Framework 24.09 Runtime Engine(s): Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace Supported Operating System(s): • Linux Model Version(s): ================= Riva-Translate-4B-Instruct Training & Evaluation: ====================== Training Dataset: ----------------- Data Collection Method by dataset: • Hybrid: Human, Synthetic Labeling Method by dataset: • Automated Properties:This model is trained on open-sourced datasets and synthetic datasets of text parallel corpora generated via back-translation and monolingual datasets. Each entry in the parallel corpus consists of a text in the source language and its translation in the target language. The monolingual datasets contain texts from each of the 12 target language domains. See bias subcard for language distribution. Properties: Evaluation Dataset: =================== Link:We used Flores101 [1], NTREX-128 [2], FRMT [3https://www.statmt.org/wmt19/translation-task.html], WMT 19 [4], WMT20 [5] to evaluate the model. Link: www.statmt.org/wmt19/translation-task.html (http://www.statmt.org/wmt19/translation-task.html) Data Collection Method by dataset: • Automated Labeling Method by dataset: • Automated References: =========== For more information about these datasets, please see the links below. [1]https://aclanthology.org/2022.tacl-1.30.pdf[2]https://aclanthology.org/2022.sumeval-1.4.pdf[3]https://aclanthology.org/2023.tacl-1.39.pdf[4]https://www.statmt.org/wmt19/translation-task.html[5]https://www.statmt.org/wmt20/translation-task.html https://aclanthology.org/2022.tacl-1.30.pdf https://aclanthology.org/2022.sumeval-1.4.pdf https://aclanthology.org/2023.tacl-1.39.pdf https://www.statmt.org/wmt19/translation-task.html https://www.statmt.org/wmt20/translation-task.html Inference: ========== Acceleration Engine:TensorRT-LLMTest Hardware: Acceleration Engine: Test Hardware: • A100 • A10G • H100 • L40S Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "magpie-tts-flow",
    "nvidia_url": "https://build.nvidia.com/nvidia/magpie-tts-flow",
    "scraped_on": "2025-10-15T21:39:12.516453",
    "tags": [
      "nvidia",
      "nvidia nim",
      "nvidia riva",
      "text-to-speech",
      "tts"
    ],
    "model_card": "Speech Synthesis: Magpie TTS Flow Model Overview ================================================ Description: ============ With only a prompt of 5 seconds or less, the Magpie TTS Flow model can analyze a speaker’s voice and replicate voice qualities such as pitch, timbre and speech rate to achieve a speaker similarity of over 70%, and an MOS score of 4.40. Maintaining the original characteristics that capture unique voice audio signature, it can create high-quality audio (speech) when used in combination with a vocoder model like BigVGAN [1]. Magpie TTS Flow [2] is an alignment-aware pre-training method that builds upon E2TTS’s [3] training framework to learn alignment between unit sequences and speech frames. By using de-duplicated units that retain only phonetic content, Magpie TTS Flow effectively learns alignment without relying on a phoneme duration predictor. This allows for direct application to zero-shot voice conversion, where phonetic content can be transferred to the target speaker’s voice without additional fine-tuning. This model is packaged with BigVGAN, a universal vocoder that generalizes well for various out-of-distribution scenarios without fine-tuning. This model is ready for commercial use. You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. License/Terms of Use: ===================== NVIDIA AI Foundation Models Community License Agreement NVIDIA AI Foundation Models Community License Agreement (https://docs.nvidia.com/ai-foundation-models-community-license.pdf) References: =========== [1]BigVGAN: A Universal Neural Vocoder with Large-Scale Training[2]Magpie-TTS-Flow Paper[3]E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS[4]Flow Matching for Generative Modeling BigVGAN: A Universal Neural Vocoder with Large-Scale Training (https://arxiv.org/abs/2206.04658) Magpie-TTS-Flow Paper (https://openreview.net/forum?id=e2p1BWR3vq) E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS (https://arxiv.org/abs/2406.18009) Flow Matching for Generative Modeling (https://arxiv.org/abs/2210.02747) Model Architecture: =================== Architecture Type: Flow MatchingNetwork Architecture: Optimal Transport Conditional Flow Matching (OT-CFM)-based Masked Speech Modeling Flow Matching [4] (FM) is a simulation-free approach for training Continuous Normalizing Flows (CNFs) based on regressing vector fields of fixed conditional probability paths. It is compatible with a general family of Gaussian probability paths for transforming between noise and data samples — which subsumes existing diffusion paths as specific instances. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Input: ====== Input Type:Text + AudioInput Format:For Text: Strings (Graphemes in US English)For Audio: wav fileInput Parameters:For text: One-Dimensional (1D)For audio prompt: Two-Dimensional (batch x time)Other Properties related to Input:For Audio: Recommended format for prompt: Mono, PCM-encoded 16 bit audio; sampling rate of 22.05 kHz; between 3 and 5 second duration. Input Type: Input Format: Input Parameters: Other Properties related to Input: Output: ======= Output Type:AudioOutput Format:Audio of shape (batch x time) in wav formatOutput Parameters:Two-Dimensional (batch x time)Other Properties related to Output:Mono, PCM-encoded 16 bit audio; sampling rate of 22.05 kHz; 20 Second Maximum Length. Output Type: Output Format: Output Parameters: Other Properties related to Output: Supported Operating System(s): • Linux Model Version(s): ================= Magpie-TTS-Flow_v1 Inference: ========== Engine:TritonTest Hardware: Engine: Test Hardware: • NVIDIA A100 GPU • NVIDIA A30 GPU • NVIDIA A10 GPU • NVIDIA H100 GPU • NVIDIA L4 GPU • NVIDIA L40 GPU Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "magistral-small-2506",
    "nvidia_url": "https://build.nvidia.com/mistralai/magistral-small-2506",
    "scraped_on": "2025-10-15T21:39:09.339621",
    "tags": [
      "advanced reasoning",
      "chat",
      "coding",
      "math",
      "mistralai",
      "multilingual"
    ],
    "model_card": "Magistral-Small-2506 Overview ============================= Description =========== Magistral-Small-2506 is a lightweight, general-purpose language model that generates and understands natural language for tasks like Q&A, summarization, and instruction following. Designed for efficiency, it balances performance with low computational overhead, making it suitable for real-world applications. Building upon Mistral Small 3.1 (2503),with added reasoning capabilities, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters. with added reasoning capabilities Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized. This model is for research and development only. Learn more about Magistral in ourblog post.The model was presented in the paperMagistral. blog post (https://mistral.ai/news/magistral/) Magistral (https://huggingface.co/papers/2506.10910) • Supported Languages:Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi Supported Languages: Key Features ------------ • Reasoning:Capable of long chains of reasoning traces before providing an answer. Reasoning:Capable of long chains of reasoning traces before providing an answer. Reasoning: • Multilingual:Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi. Multilingual:Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi. Multilingual: • Context Window:A 128k context window,butperformance might degrade past40k. Hence we recommend setting the maximum model length to 40k. Context Window:A 128k context window,butperformance might degrade past40k. Hence we recommend setting the maximum model length to 40k. Context Window: but 40k Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIAMistralAi/Magistral-Small-2506. MistralAi/Magistral-Small-2506 (https://huggingface.co/mistralai/Magistral-Small-2506) License/Terms of Use -------------------- GOVERNING TERMS:This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Apache 2.0. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) Deployment Geography -------------------- Global Use Case -------- Magistral Small is a reasoning model with multilingual capabilities that can be deployed locally, fitting within a single RTX 4090. Release Date ------------ Build.NVIDIA.com:2024-07-09 vialinkHugging Face:2024-06-25link Build.NVIDIA.com: link (https://build.nvidia.com/mistralai/magistral-small-2506) Hugging Face: link (https://huggingface.co/mistralai/Magistral-Small-2506) Reference(s) ------------ Technical Paper:Magistral Magistral (https://huggingface.co/papers/2506.10910) Model Architecture ================== Architecture Type: Decoder-only (Transformer-based) Network Architecture: Transformer-based, decoder-only architecture with rotary positional embeddings, multi-head self-attention, and long-context support (up to 128k tokens). Based on Mistral open-weight foundation models. Model Architecture: =================== • Architecture Type: Transformer decoder • Network Architecture: Transformer with FlashAttention • Parameter Count: 24B • Base Model:Mistral-Small-3.1-24B-Base-2503 Mistral-Small-3.1-24B-Base-2503 (https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503) Input ===== Input Type(s):TextInput Format(s):StringInput Parameters:One-Dimensional (1D)Other Properties Related to Input:Context Length: 128k Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output ====== Output Type(s): TextOutput Format(s): StringOutput Parameters: One-Dimensional (1D)Other Properties Related to Output:Max theoretical context length: 128,000 tokens, Practical limit: ≈ 40k tokens (40960) Output Type(s) Output Format(s) Output Parameters Other Properties Related to Output: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.** Software Integration ==================== Runtime Engine: vLLM Supported Hardware Microarchitecture Compatibility -------------------------------------------------- • NVIDIA Ampere • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace Preferred Operating System(s) ----------------------------- Linux Model Version(s) ---------------- Magistral-Small-2506 1.0 Training, Testing, and Evaluation Datasets ========================================== Training Dataset ---------------- • Data Collection Method by dataset:Hybrid: Automated, Human Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human Labeling Method by dataset: • Properties:Trained on ~2 trillion tokens from web, books, code, and conversations; no sensors used (not applicable for text-based data). Properties: Testing Dataset --------------- • Data Collection Method by dataset:Hybrid: Automated, Human Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human Labeling Method by dataset: • Properties:Undisclosed Properties: Evaluation Dataset ------------------ • Data Collection Method by dataset:Hybrid: Automated, Human Data Collection Method by dataset:Hybrid: Automated, Human Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human Labeling Method by dataset:Hybrid: Automated, Human Labeling Method by dataset: • Properties:The benchmarks noted in the following session were used for evaluation. Properties:The benchmarks noted in the following session were used for evaluation. Properties: Benchmark Results ----------------- Inference --------- Acceleration Engine: vLLMTest Hardware: • L40s x2 Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "kimi-k2-instruct",
    "nvidia_url": "https://build.nvidia.com/moonshotai/kimi-k2-instruct",
    "scraped_on": "2025-10-15T21:39:08.887581",
    "tags": [
      "advanced reasoning",
      "agentic",
      "chat",
      "coding",
      "moonshotai"
    ],
    "model_card": "Kimi-K2-Instruct ================ Description =========== Kimi K2 Instruct is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. This model is ready for commercial/non-commercial use. Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIAKimi-K2-Instruct Model Card Kimi-K2-Instruct Model Card (https://huggingface.co/moonshotai/Kimi-K2-Instruct) License and Terms of Use: ========================= GOVERNING TERMS:This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Modified MIT License. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Modified MIT License (https://huggingface.co/moonshotai/Kimi-K2-Instruct/blob/main/LICENSE) Deployment Geography: ===================== Global Use Case: ========= This model is designed for agentic AI and tool use, including advanced code generation, complex problem-solving, and multilingual applications. It can be used for building autonomous agents that can interact with external systems and APIs, for multi-step reasoning tasks, mathematical problem-solving, and analytical workflows. Key Features • Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability. • MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up. • Agentic Intelligence: Specifically designed for tool use, reasoning, and autonomous problem-solving. Release Date: ============= Build.NVIDIA.com07/2025 vialinkHuggingface07/12/2025 vialink Build.NVIDIA.com link (https://build.nvidia.com/moonshotai/kimi-k2-instruct) Huggingface link (https://huggingface.co/moonshotai/Kimi-K2-Instruct) Reference(s): ============= References: • moonshotai/Kimi-K2-Instruct - Simon Willison's Weblog moonshotai/Kimi-K2-Instruct - Simon Willison's Weblog (https://simonwillison.net/2025/Jul/11/kimi-k2/) • Kimi-k2 : The best Open-Sourced AI model with 1 Trillion params | by Mehul Gupta - Medium Kimi-k2 : The best Open-Sourced AI model with 1 Trillion params | by Mehul Gupta - Medium (https://medium.com/data-science-in-your-pocket/kimi-k2-the-best-open-sourced-ai-model-with-1-trillion-params-c647779496a5) • How to Run Kimi K2 Inside Claude Code: The Ultimate Open-Source AI Coding Combo How to Run Kimi K2 Inside Claude Code: The Ultimate Open-Source AI Coding Combo (https://garysvenson09.medium.com/how-to-run-kimi-k2-inside-claude-code-the-ultimate-open-source-ai-coding-combo-7b248adcf336) Model Architecture: =================== Architecture Type:Transformer Architecture Type: • Network Architecture:Mixture-of-Experts (MoE) Network Architecture: • Total Parameters:1T Total Parameters: • Active Parameters:32B Active Parameters: • Number of Layers (Dense layer included):61 Number of Layers (Dense layer included): • Attention Hidden Dimension:7168 Attention Hidden Dimension: • MoE Hidden Dimension (per Expert):2048 MoE Hidden Dimension (per Expert): • Number of Attention Heads:64 Number of Attention Heads: • Number of Experts:384 Number of Experts: • Selected Experts per Token:8 Selected Experts per Token: • Number of Shared Experts:1 Number of Shared Experts: • Vocabulary Size:160K Vocabulary Size: • Context Length:128K Context Length: • Attention Mechanism:MLA Attention Mechanism: • Activation Function:SwiGLU Activation Function: • Base Model:Kimi-K2-Base Base Model: Input: ------ Input Types:TextInput Formats:StringInput Parameters:One Dimensional (1D)Other Input Properties:The model has a context window of up to 128,000 tokens.Input Context Length (ISL):128K Input Types: Input Formats: Input Parameters: Other Input Properties: Input Context Length (ISL): Output: ------- Output Format:StringOutput Parameters:One Dimensional (1D)Other Output Properties:Not applicable. Output Format: Output Parameters: Other Output Properties: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration: ===================== Runtime Engines: • NVIDIA NeMo • NVIDIA Riva Supported Hardware: • NVIDIA Blackwell:B100, B200, GB200 NVIDIA Blackwell: • NVIDIA Hopper:H100, H200 NVIDIA Hopper: Operating Systems:Linux Operating Systems: Model Version(s): ================= Kimi-K2-Instruct v1.0 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset ---------------- Training Data Collection:UndisclosedTraining Labeling:UndisclosedTraining Properties:Trained on 15.5 trillion tokens. Training Data Collection: Training Labeling: Training Properties: Testing Dataset --------------- Testing Data Collection:UndisclosedTesting Labeling:UndisclosedTesting Properties:Undisclosed Testing Data Collection: Testing Labeling: Testing Properties: Evaluation Dataset ------------------ Evaluation Benchmark Score: • LiveCodeBench: 53.7% Pass@1 • SWE-bench Verified: 65.8% single-attempt accuracy • MMLU: 89.5% exact match • Tau2 retail tasks: 70.6% Avg@4 Evaluation Data Collection:UndisclosedEvaluation Labeling:UndisclosedEvaluation Properties:LiveCodeBench, SWE-bench, MMLU, Tau2 Evaluation Data Collection: Evaluation Labeling: Evaluation Properties: Evaluation Results ------------------ Coding Tasks 53.7 27.1 85.7 89.6 51.8 53.0 65.8 72.7 71.6 80.2 47.3 51.0 30.0 43.2 25.0 30.3 61.8 70.7 Tool Use Tasks 70.6 81.8 56.5 60.0 65.8 76.5 80.1 Math & STEM Tasks 69.6 49.5 97.4 38.8 74.7 75.0 65.1 89.0 89.5 89.8 75.1 57.2 5.7 7.1 General Tasks 89.5 92.9 92.7 94.2 81.2 86.6 89.8 54.1 31.0 42.3 76.4 • Bold denotes global SOTA, and underlined denotes open-source SOTA. • Data points marked with * are taken directly from the model's tech report or blog. • All metrics, except for SWE-bench Verified (Agentless), are evaluated with an 8k output token length. SWE-bench Verified (Agentless) is limited to a 16k output token length. • Kimi K2 achieves 65.8% pass@1 on the SWE-bench Verified tests with bash/editor tools (single-attempt patches, no test-time compute). It also achieves a 47.3% pass@1 on the SWE-bench Multilingual tests under the same conditions. Additionally, we report results on SWE-bench Verified tests (71.6%) that leverage parallel test-time compute by sampling multiple sequences and selecting the single best via an internal scoring model. • To ensure the stability of the evaluation, we employed avg@k on the AIME, HMMT, CNMO, PolyMath-en, GPQA-Diamond, EvalPlus, Tau2. • Some data points have been omitted due to prohibitively expensive evaluation costs. Inference ========= Acceleration Engine:vLLMTest Hardware:NVIDIA DGX B200 Acceleration Engine: Test Hardware: Additional Details ================== Deployment ---------- You can access Kimi K2's API onhttps://platform.moonshot.ai, we provide OpenAI/Anthropic-compatible API for you. https://platform.moonshot.ai The Anthropic-compatible API maps temperature byreal_temperature = request_temperature * 0.6for better compatible with existing applications. Our model checkpoints are stored in the block-fp8 format, you can find it onHuggingface. Huggingface (https://huggingface.co/moonshotai/Kimi-K2-Instruct) Currently, Kimi-K2 is recommended to run on the following inference engines: • vLLM • SGLang • KTransformers • TensorRT-LLM Deployment examples for vLLM and SGLang can be found in theModel Deployment Guide. Model Deployment Guide (https://huggingface.co/moonshotai/Kimi-K2-Instruct/blob/main/docs/deploy_guidance.md) Model Usage ----------- Once the local inference service is up, you can interact with it through the chat endpoint: NOTE The recommended temperature for Kimi-K2-Instruct istemperature = 0.6. If no special instructions are required, the system prompt above is a good default. Tool Calling ------------ Kimi-K2-Instruct has strong tool-calling capabilities. To enable them, you need to pass the list of available tools in each request, then the model will autonomously decide when and how to invoke them. The following example demonstrates calling a weather tool end-to-end: Thetool_call_with_clientfunction implements the pipeline from user query to tool execution. This pipeline requires the inference engine to support Kimi-K2’s native tool-parsing logic. For streaming output and manual tool-parsing, see theTool Calling Guide. Tool Calling Guide (https://huggingface.co/moonshotai/Kimi-K2-Instruct/blob/main/docs/tool_call_guidance.md) Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "phi-4-mini-flash-reasoning",
    "nvidia_url": "https://build.nvidia.com/microsoft/phi-4-mini-flash-reasoning",
    "scraped_on": "2025-10-15T21:39:08.448255",
    "tags": [
      "chat",
      "edge",
      "math",
      "microsoft",
      "reasoning",
      "text-generation"
    ],
    "model_card": "Phi-4-mini-flash-reasoning Overview =================================== Description =========== Phi-4-mini-flash-reasoningis a lightweight, 3.8 B‑parameter open model built upon synthetic, reasoning‑dense data andfine‑tuned specifically for advanced mathematical reasoning. It belongs to the Phi‑4 family and supports64 K‑token context length. Phi-4-mini-flash-reasoning fine‑tuned specifically for advanced mathematical reasoning 64 K‑token context length This model is ready for commercial/non-commercial use. Third‑Party Community Consideration =================================== This model is not owned or developed by NVIDIA. It has been produced to a third-party’s requirements for this application and use-case. See the external card:Phi-4-mini-flash-reasoning Model Card. Phi-4-mini-flash-reasoning Model Card (https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning) License / Terms of Use ---------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:MIT. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) MIT (https://huggingface.co/microsoft/phi-4/blob/main/LICENSE) Deployment Geography -------------------- Global Intended Uses ------------- Primary Use Cases • Multi‑step, logic‑intensive mathematical problem solving under memory/compute‑constrained and latency‑bound scenarios. • Formal proof generation, symbolic computation, advanced word‑problem solutions, and other math‑reasoning tasks that require deep analytical thinking. Use‑Case ConsiderationsThis model isdesigned and evaluated for math reasoning only.Developers should carefully evaluate accuracy, safety, fairness, multilingual performance, and legal compliance before use—especially in high‑risk domains. Use‑Case Considerations designed and evaluated for math reasoning only. Release Date ------------ Build.NVIDIA.com:07/18/2025 (link)Hugging Face:07/09/2025 (link) Build.NVIDIA.com: link (https://build.nvidia.com/microsoft/phi-4-mini-flash-reasoning) Hugging Face: link (https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning) References ---------- • Blog post Blog post (https://azure.microsoft.com/en-us/blog/reasoning-reimagined-introducing-phi-4-mini-flash-reasoning) • Technical paper Technical paper (https://aka.ms/flashreasoning-paper) • Phi Cookbook Phi Cookbook (https://github.com/microsoft/PhiCookBook) • Phi Portal Phi Portal (https://azure.microsoft.com/en-us/products/phi) • vLLM Inference repo vLLM Inference repo (https://github.com/vllm-project/vllm) Model Architecture ================== • Architecture Type:Transformer‑based “Hybrid SambaY” with Differential Attention. Architecture Type: • Parameters:3.8 B. Parameters: • Vocabulary:200,064 tokens. Vocabulary: • Context Length:64K tokens. Context Length: • Key Components:State‑space modules, grouped‑query attention, gated memory sharing, shared KV cache with one global‑attention layer, shared input/output embeddings. Key Components: • Base Model:Phi-4-mini-flash Base Model: Inputs ------ Input Type(s):TextInput Formats:StringInput Parameters:One-Dimensional (1D)Other Properties Related to Input:Up to 64K tokens context length. It is best suited for prompts using the chat format. Input Type(s): Input Formats: Input Parameters: Other Properties Related to Input: Output ------ Output Type(s):TextOutput Formats:StringOutput Parameters:One-Dimensional (1D)Other Properties Related to Output:Generates step-by-step math reasoning up to 32K tokens Output Type(s): Output Formats: Output Parameters: Other Properties Related to Output: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration ==================== • PyTorch PyTorch (https://github.com/pytorch/pytorch) • Transformers Transformers (https://github.com/huggingface/transformers) • Flash-Attention Flash-Attention (https://github.com/HazyResearch/flash-attention) • Mamba Mamba (https://github.com/state-spaces/mamba) • Causal-Conv1d Causal-Conv1d (https://github.com/Dao-AILab/causal-conv1d) Supported Hardware ================== Note that by default, the Phi-4-mini-flash-reasoning model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types: • NVIDIA A100 (Ampere) • NVIDIA H100 (Hopper) Supported Operating System(s):Linux Supported Operating System(s): Model Version: ============== phi-4-mini-flash-reasoning v1.0 Training, Testing & Evaluation Datasets ======================================= Training Dataset: ----------------- Data Collection Method by dataset:Hybrid: Synthetic, Automated, HumanLabeling Method by dataset:Hybrid: Synthetic, AutomatedProperties: Data Collection Method by dataset: Labeling Method by dataset: Properties: • Quantity ≈ 1,000,000 math problems, ≈ 30B text tokens • Description Three components:Curated publicly-available math questions + base SFT dataDeepSeek-R1 synthetic math corpus spanning middle-school → Ph.D. difficultyBalanced correct/incorrect answer pairs for preference learning • Curated publicly-available math questions + base SFT data • DeepSeek-R1 synthetic math corpus spanning middle-school → Ph.D. difficulty • Balanced correct/incorrect answer pairs for preference learning Additional Training Details: • GPUs:Pre-training: 1024 A100-80G; Reasoning training: 128 H100-80G GPUs: • Training time:Pre-training: 14 days; Reasoning training: 2days Training time: • Training data:Pre-training: 5T tokens; Reasoning training: 150B tokens Training data: • Dates:Trained in May 2025 Dates: • Status:This is a static model trained on offline datasets with the cutoff date of February 2025 for publicly available data. Status: Testing Dataset: ---------------- Data Collection Method by dataset:UndisclosedLabeling Method by dataset:UndisclosedProperties:Undisclosed Data Collection Method by dataset: Labeling Method by dataset: Properties: Evaluation Dataset: ------------------- Benchmark Score:Pass@1 – AIME 2024 52.29 % / AIME 2025 33.59 % / Math-500 92.45 % / GPQA-Diamond 45.08 %Data Collection Method by datasetPublic benchmark datasetsLabeling Method by dataset:Benchmark gold answersProperties:Math-500, AIME 2024, AIME 2025, GPQA-Diamond Benchmark Score: Data Collection Method by dataset Labeling Method by dataset: Properties: Evaluation Benchmarks --------------------- To understand the capabilities, the 3.8B parameters Phi-4-mini-flash-reasoning model was compared with a set of models over a variety of reasoning benchmarks. We use a more accurate evaluation where Pass@1 accuracy is averaged over 64 samples for AIME24/25 and 8 samples for Math500 and GPQA Diamond. A high-level overview of the model quality is as follows: Phi‑4‑mini‑Reasoning (3.8 B) Phi‑4‑mini‑Flash‑Reasoning (3.8 B) 52.29 33.59 92.45 45.08 Overall, the model with only 3.8B-param achieves a similar level of math and science reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much factual knowledge, therefore, users may experience factual incorrectness. However, it may be possible to resolve such weakness by augmenting Phi-4-mini-flash-reasoning with a search engine, particularly when using the model under RAG settings. Benchmark Methodology --------------------- We include a brief word on methodology here - and in particular, how we think about optimizing prompts. In an ideal world, we would never change any prompts in our benchmarks to ensure it is always an apples-to-apples comparison when comparing different models. Indeed, this is our default approach, and is the case in the vast majority of models we have run to date. For all benchmarks, we consider using the same generation configuration such as max sequence length (32768), the same temperature for the fair comparison. Benchmark datasets We evaluate the model with three of the most popular math benchmarks where the strongest reasoning models are competing together. Specifically: • Math-500: This benchmark consists of 500 challenging math problems designed to test the model's ability to perform complex mathematical reasoning and problem-solving. • AIME 2024/AIME 2025: The American Invitational Mathematics Examination (AIME) is a highly regarded math competition that features a series of difficult problems aimed at assessing advanced mathematical skills and logical reasoning. We evaluate the models on the problems from both 2024 and the year 2025 examinations. • GPQA Diamond: The Graduate-Level Google-Proof Q&A (GPQA) Diamond benchmark focuses on evaluating the model's ability to understand and solve a wide range of mathematical questions, including both straightforward calculations and more intricate problem-solving tasks. Inference: ========== Acceleration Engine:vLLMTest Hardware:NVIDIA Ada Lovelace Acceleration Engine: Test Hardware: Additional Details: =================== This compact 3.8 B model achieves near-linear latency growth with token count (2–3× lower latency and up to 10× higher throughput than Phi-4-mini-reasoning on a single A100-80 GB). Model Efficiency ---------------- The two figures below compare the latency and throughput performance of the Phi-4-mini-reasoning and Phi-4-mini-flash-reasoning models under the vLLM inference framework. All evaluations were performed on a single NVIDIA A100-80GB GPU with tensor parallelism disabled (TP = 1). The Phi-4-mini-flash-reasoning model, which incorporates a decoder-hybrid-decoder architecture with attention and state space model (SSM), exhibits significantly greater computational efficiency—achieving a 2–3× reduction in average latency and up-to a 10× improvement in throughput when processing user requests with 2K prompt length and 32K generation length. Furthermore, Phi-4-mini-flash-reasoning demonstrates near-linear growth in latency with respect to the number of tokens generated (up to 32k), in contrast to the quadratic growth observed in Phi-4-mini-reasoning. These findings indicate that Phi-4-mini-flash-reasoning is more scalable and better suited for long-sequence generation tasks. Usage ----- Tokenizer --------- Phi-4-mini-flash-reasoning supports a vocabulary size of up to200064tokens. Thetokenizer filesalready provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model's vocabulary size. tokenizer files (https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning/blob/main/added_tokens.json) Input Formats ------------- Given the nature of the training data, the Phi-4-mini-flash-reasoning model is best suited for prompts using specific formats. Below are the two primary formats: Chat format This format is used for general conversation and instructions: Inference with transformers --------------------------- Phi-4-mini-flash-reasoning has been integrated in the4.51.3version oftransformers. The currenttransformersversion can be verified with:pip list | grep transformers. Python 3.8 and 3.10 will work best. List of required packages: Phi-4-mini-flash-reasoning is also available inAzure AI Studio Azure AI Studio (https://aka.ms/phi4-mini-flash-reasoning/azure) Example ------- After obtaining the Phi-4-mini-flash-reasoning model checkpoints, users can use this sample code for inference. Safety Evaluation and Red-Teaming ================================= The Phi-4 family of models has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated datasets. The overall technique employed to do the safety alignment is a combination of SFT, DPO (Direct Preference Optimization), and RLHF (Reinforcement Learning from Human Feedback) approaches by utilizing human-labeled and synthetic English-language datasets, including publicly available datasets focusing on helpfulness and harmlessness, as well as various questions and answers targeted to multiple safety categories. Phi-4-Mini-Flash-Reasoning was developed in accordance with Microsoft's responsible AI principles. Potential safety risks in the model’s responses were assessed using the Azure AI Foundry’s Risk and Safety Evaluation framework, focusing on harmful content, direct jailbreak, and model groundedness. The Phi-4-Mini-Flash-Reasoning Model Card contains additional information about our approach to safety and responsible AI considerations that developers should be aware of when using this model. Responsible AI Considerations ============================= Like other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: • Quality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English. • Multilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 4 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards. • Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. • Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the case. • Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. • Election Information Reliability : The model has an elevated defect rate when responding to election-critical queries, which may result in incorrect or unauthoritative election critical information being presented. We are working to improve the model's performance in this area. Users should verify information related to elections with the election authority in their region. • Limited Scope for Code: The majority of Phi 4 training data is based in Python and uses common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, it is strongly recommended that users manually verify all API uses. • Long Conversation: Phi 4 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift. Developers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi 4 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include: • Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques. • High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. • Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG). • Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. • Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations. Trademarks ========== This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must followMicrosoft’s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies. Microsoft’s Trademark & Brand Guidelines (https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks) Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nemoretriever-ocr",
    "nvidia_url": "https://build.nvidia.com/nvidia/nemoretriever-ocr",
    "scraped_on": "2025-10-15T21:39:05.374804",
    "tags": [
      "data ingestion",
      "extraction",
      "nemo retriever",
      "nvidia",
      "optical character recognition",
      "table extraction"
    ],
    "model_card": "NeMo Retriever OCR v1 Overview =============================== Description =========== The NeMo Retriever OCR v1 model is a state-of-the-art text recognition model designed for robust end-to-end optical character recognition (OCR) on complex real-world images. It integrates three core neural network modules: a detector for text region localization, a recognizer for transcription of detected regions, and a relational model for layout and structure analysis. This model is optimized for a wide variety of OCR tasks, including multi-line, multi-block, and natural scene text, and supports advanced reading order analysis via its relational model component. NeMo Retriever OCR v1 has been developed to be production-ready and commercially usable, with a focus on speed and accuracy on both document and natural scene images. The NeMo Retriever OCR v1 model is part of the NVIDIA NeMo Retriever collection of NIM, which provides state-of-the-art, commercially-ready models and microservices optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can readily customize them for domain-specific use cases, such as information technology, human resource help assistants, and research & development research assistants. This model is ready for commercial use. License/Terms of Use -------------------- GOVERNING TERMS:The trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Software and Model Evaluation License. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Software and Model Evaluation License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-and-model-evaluation-license/) Deployment Geography -------------------- Global Use Case -------- The NeMo Retriever OCR v1 model is designed for high-accuracy and high-speed extraction of textual information from images, making it ideal for powering multimodal retrieval systems, Retrieval-Augmented Generation (RAG) pipelines, and agentic applications that require seamless integration of visual and language understanding. Its robust performance and efficiency make it an excellent choice for next-generation AI systems that demand both precision and scalability across diverse real-world content. Release Date ------------ Build.NVIDIA.com:2025-07 vialink Build.NVIDIA.com: link (https://build.nvidia.com/nvidia/nemoretriever-ocr) Model Architecture ================== Architecture Type:Hybrid detector–recognizer with document-level relational modeling Architecture Type: The NeMo Retriever OCR v1 model integrates three specialized neural components: • Text Detector:Utilizes a RegNetY-8GF convolutional backbone for high-accuracy localization of text regions within images. Text Detector: • Text Recognizer:Employs a Transformer-based sequence recognizer to transcribe text from detected regions, supporting variable word and line lengths. Text Recognizer: • Relational Model:Applies a multi-layer global relational module to predict logical groupings, reading order, and layout relationships across detected text elements. Relational Model: All components are trained jointly in an end-to-end fashion, providing robust, scalable, and production-ready OCR for diverse document and scene images. Parameter Counts: Total Input ===== Input Type(s):Image (RGB, PNG/JPEG, float32/uint8), aggregation level (word, sentence, or paragraph)Input Format(s):List of stringsInput Parameters:3 x H x W (single image) or B x 3 x H x W (batch)Input Range:[0, 1] (float32) or [0, 255] (uint8, auto-converted)Other Properties Related to Input:Handles both single images and batches. Automatic multi-scale resizing for best accuracy. Input Type(s): Input Format(s): Input Parameters: Input Range: Other Properties Related to Input: Output ====== Output Type(s): Structured OCR results: a list of detected text regions (bounding boxes), recognized text, and confidence scoresOutput Format(s): Bounding boxes: tuple of floats, recognized text: string, confidence score: float** Output parameters**: Bounding boxes: 1D list of bounding box coordinates, recognized text: 1D list of strings, confidence score: 1D list of floatsOther Properties Related to Output:Please see the sample output for an example of the model output. Output Type(s) Output Format(s) Other Properties Related to Output: Sample output ------------- Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (such as GPU cores) and software frameworks (such as CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration ==================== Runtime Engine: PyTorch Supported Hardware Microarchitecture Compatibility -------------------------------------------------- • NVIDIA Ampere • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace Preferred Operating System(s) ----------------------------- Linux Model Version(s) ---------------- NeMo Retriever OCR v1 Training, Testing, and Evaluation Datasets ========================================== Training Dataset ---------------- The model is trained on a large-scale, curated mix of public and proprietary OCR datasets, focusing on high diversity of document layouts and scene images. The training set includes synthetic and real images with varied noise and backgrounds, filtered for commercial use eligibility. • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset: • Properties:Includes scanned documents, natural scene images, receipts, and business documents. Properties: Testing Dataset --------------- • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset: • Properties:Includes scanned documents, natural scene images, receipts, and business documents. Properties: Evaluation Dataset ------------------ The NeMo Retriever OCR v1 model is evaluated on several NVIDIA internal datasets for various tasks, such as pure OCR, table content extraction, and document retrieval. • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset: • Properties:Benchmarks include challenging scene images, documents with varied layouts, and multi-language data. Properties:Benchmarks include challenging scene images, documents with varied layouts, and multi-language data. Properties: Benchmark Results ----------------- We benchmarked NeMo Retriever OCR v1 on internal evaluation datasets against PaddleOCR on various tasks, such as pure OCR (Character Error Rate), table content extraction (TEDS), and document retrieval (Recall@5). Detailed Performance Analysis ----------------------------- Detailed Performance Analysis The model demonstrates robust performance on complex layouts, noisy backgrounds, and challenging real-world scenes. Reading order and block detection are powered by the relational module, supporting downstream applications such as chart-to-text, table-to-text, and infographic-to-text extraction. Inference --------- Acceleration Engine: PyTorchTest Hardware: • L40s Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3_2-nemoretriever-300m-embed-v1",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_2-nemoretriever-300m-embed-v1",
    "scraped_on": "2025-10-15T21:39:02.158641",
    "tags": [
      "nemo retriever",
      "nvidia",
      "retrieval augmented generation",
      "text-to-embedding"
    ],
    "model_card": "Model Overview ============== Model Overview Description ----------- Description The Llama 3.2 NeMo Retriever Embedding 300M model is optimized for multilingual and cross-lingual text question-answering retrieval with support for long documents (up to 8192 tokens). This model was evaluated on 26 languages: English, Arabic, Bengali, Chinese, Czech, Danish, Dutch, Finnish, French, German, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, and Turkish. In addition to enabling multilingual and cross-lingual question-answering retrieval, this model enables support for longer token length, making it feasible to handle large-scale datasets efficiently. An embedding model is a crucial component of a text retrieval system, as it transforms textual information into dense vector representations. They are typically transformer encoders that process tokens of input text (for example: question, passage) to output an embedding.ß The Llama 3.2 NeMo Retriever Embedding 300M model is a part of the NVIDIA NeMo Retriever collection of NIMs, which provide state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can also readily customize them for domain-specific use cases, such as information technology, human resource help assistants, and research & development research assistants. This model is ready for commercial use. Key Features ------------ • Reasoning:Capable of long chains of reasoning traces before providing an answer. Reasoning:Capable of long chains of reasoning traces before providing an answer. Reasoning: • Multilingual:Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi. Multilingual:Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi. Multilingual: • Context Window:A 128k context window,butperformance might degrade past40k. Hence we recommend setting the maximum model length to 40k. Context Window:A 128k context window,butperformance might degrade past40k. Hence we recommend setting the maximum model length to 40k. Context Window: but 40k License/Terms of Use -------------------- GOVERNING TERMS:The trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:Llama 3.2 Community License Agreement. Built with Llama GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Llama 3.2 Community License Agreement (https://www.llama.com/llama3_2/license/) Deployment Geography -------------------- Global Use Case -------- The Llama 3.2 NeMo Retriever Embedding 300M model is most suitable for users who want to build a multilingual question-and-answer application over a large text corpus, leveraging the latest dense retrieval technologies. Release Date ------------ Build.NVIDIA.com:2025-07 vialink Build.NVIDIA.com: link (https://build.nvidia.com/nvidia/llama-3_2-nemoretriever-300m-embed-v1) Model Architecture ================== Architecture Type: Transformer Network Architecture: Fine-tuned Llama3.2 300M Retriever This NeMo Retriever embedding model is a transformer encoder with 9 layers and an embedding size of 2048 and has been pruned, distilled from Llama 3.2-nv-embedqa-1b-v1 model. After pruning and distillation, the model has been trained on public datasets. The AdamW optimizer is employed incorporating 100 warm up steps and 5e-6 learning rate with WarmupDecayLR scheduler. Embedding models for text retrieval are typically trained using a bi-encoder architecture. This involves encoding a pair of sentences (for example, query and chunked passages) independently using the embedding model. Contrastive learning is used to maximize the similarity between the query and the passage that contains the answer, while minimizing the similarity between the query and sampled negative passages not useful to answer the question. Input ===== Input Type(s):TextInput Format(s):List of stringsInput Parameters:One-Dimensional (1D)Other Properties Related to Input:The model's maximum context length is 8192 tokens. Texts longer than maximum length must either be chunked or truncated. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output ====== Output Type(s): FloatsOutput Format(s): List of floatsOutput Parameters: One-Dimensional (1D)Other Properties Related to Output:Model outputs embedding vectors of dimension 2048. Output Type(s) Output Format(s) Output Parameters Other Properties Related to Output: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (such as GPU cores) and software frameworks (such as CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration ==================== Runtime Engine: TensorRT Supported Hardware Microarchitecture Compatibility -------------------------------------------------- • NVIDIA Ampere • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace Preferred Operating System(s) ----------------------------- Linux Model Version(s) ---------------- Llama 3.2 NeMo Retriever Embedding 300M v1 1.0 Training, Testing, and Evaluation Datasets ========================================== Training Dataset ---------------- The development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset named MS MARCO restricts ‌commercial licensing, limiting the use of these models in commercial settings. To address this, NVIDIA created its own training dataset blend based on public QA datasets, which each have a license for commercial applications. • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset: • Properties:Semi-supervised pre-training on 12M samples from public datasets and fine-tuning on 1M samples from public datasets. Properties: Testing Dataset --------------- • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset: • Properties:Semi-supervised pre-training on 12M samples from public datasets and fine-tuning on 1M samples from public datasets. Properties: Evaluation Dataset ------------------ We evaluated the NeMo Retriever embedding model in comparison to literature open & commercial retriever models on academic benchmarks for question-answering -NQ,HotpotQAandFiQA (Finance Q&A)from BeIR benchmark and TechQA dataset. Note that the model was evaluated offline on A100 GPUs using the model's PyTorch checkpoint. In this benchmark, the metric used was Recall@5. We also evaluated the multilingual capabilities on the academic benchmarkMIRACLacross 15 languages and translated the English and Spanish version of MIRACL into additional 11 languages. The reported scores are based on an internal version of MIRACL by selecting hard negatives for each query to reduce the corpus size. We evaluated the capabilities on the academic benchmarkMLQAbased on 7 languages (Arabic, Chinese, English, German, Hindi, Spanish, Vietnamese). We consider only evaluation datasets when the query and documents are in same languages. We evaluated the support of long documents on the academic benchmarkMultilingual Long-Document Retrieval (MLDR)built on Wikipedia and mC4, covering 12 typologically diverse languages. The English version has a median length of 2399 tokens and 90th percentile of 7483 tokens using the llama 3.2 tokenizer. The MLDR dataset is based on synthetic generated questions with a LLM, which has the tendency to create questions with similar keywords than the positive document, but might not be representative for real user queries. This characteristic of the dataset benefits sparse embeddings like BM25. NQ (https://huggingface.co/datasets/BeIR/nq) HotpotQA (https://huggingface.co/datasets/hotpotqa/hotpot_qa) FiQA (Finance Q&A) (https://huggingface.co/datasets/BeIR/fiqa) MIRACL (https://github.com/project-miracl/miracl) MLQA (https://github.com/facebookresearch/MLQA/) Multilingual Long-Document Retrieval (MLDR) (https://huggingface.co/datasets/Shitao/MLDR) • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset: • Properties:The evaluation datasets are based onMTEB/BEIR, TextQA, TechQA,MIRACL,MLQA, andMLDR. The size ranges between 10,000s up to 5M depending on the dataset. Properties:The evaluation datasets are based onMTEB/BEIR, TextQA, TechQA,MIRACL,MLQA, andMLDR. The size ranges between 10,000s up to 5M depending on the dataset. Properties: MTEB/BEIR (https://github.com/beir-cellar/beir) MIRACL (https://github.com/project-miracl/miracl) MLQA (https://github.com/facebookresearch/MLQA) MLDR (https://huggingface.co/datasets/Shitao/MLDR) Benchmark Results ----------------- Average Recall@5 on NQ, HotpotQA, FiQA, TechQA dataset Average Recall@5 on Multilingual Average Recall@5 on MLQA (Different Languages) Average Recall@5 on MLDR Inference --------- Acceleration Engine: TensorRTTest Hardware: • L40s x1 Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "sarvam-m",
    "nvidia_url": "https://build.nvidia.com/sarvamai/sarvam-m",
    "scraped_on": "2025-10-15T21:38:58.967452",
    "tags": [
      "chat",
      "coding",
      "hybrid",
      "indic languages",
      "math",
      "multilingual",
      "reasoning",
      "sarvamai"
    ],
    "model_card": "Sarvam-m Overview ================== Description ----------- Sarvam-m generates human-like text for a seamless chatting experience, providing a smooth and accessible multilingual conversation experience, and is intended for general-purpose conversation and text generation tasks. This model is ready for commercial/non-commercial use. Third-Party Community Consideration ----------------------------------- This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAsarvamai/sarvamai-m. sarvamai/sarvamai-m (https://huggingface.co/sarvamai/sarvam-m) License/Terms of Use -------------------- GOVERNING TERMS:This trial service is governed by theNVIDIA API Trial Terms of Service. The model is governed by theNVIDIA Community Model License Agreement; ADDITIONAL INFORMATION:Apache License Version 2.0. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache License Version 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. Deployment Geography -------------------- Global Use Case -------- This model is intended for users seeking a seamless multilingual chatting experience, particularly those interested in Indian languages and cultures. It can be used by developers, researchers, and individuals looking to leverage its advanced reasoning capabilities for coding, math, and general conversation purposes. Release Date ------------ Hugging Face:05/23/2025 viasarvamai/sarvam-m.Build.NVIDIA.com:07/25/2025 vialink Hugging Face: sarvamai/sarvam-m (https://huggingface.co/sarvamai/sarvam-m) Build.NVIDIA.com: link (https://build.nvidia.com/sarvamai/sarvam-m) Model Architecture ================== Architecture Type:Hybrid-reasoning Transformer Architecture Type: Network Architecture:Mistral-Small Network Architecture: This model was developed based on Mistral-Small-3.1-24B-Base-2503sarvamai/sarvamai-m. sarvamai/sarvamai-m (https://www.sarvam.ai/blogs/sarvam-m) Number of model parameters: 23.6B Input ===== Input Type(s):TextInput Format(s):StringsInput Parameters:One-Dimensional (1D)Input Range:[0, 1] (float32) or [0, 255] (uint8, auto-converted)Other Properties Related to Input:Supports up to 8,192-token context length (example shown in Quickstart using max_new_tokens=8192) Input Type(s): Input Format(s): Input Parameters: Input Range: Other Properties Related to Input: Output ====== Output Type(s)Output Format(s): StringsOutput parameters:One-Dimensional (1D)Other Properties Related to Output:Context Length: 32,768 tokens and sliding window attention of 4096 tokens. Output Type(s) Output Format(s) Output parameters: Other Properties Related to Output: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration ==================== Runtime Engine: vLLM Supported Hardware Microarchitecture Compatibility -------------------------------------------------- • NVIDIA Ampere • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace Preferred Operating System(s) ----------------------------- Linux Model Version(s) ---------------- sarvam-m-v1.0 Quickstart ---------- The following code snippet demonstrates how to use sarvam-m using Transformers. For thinking mode, we recommend temperature=0.5; for no-think mode, temperature=0.2. With Sarvam APIs ---------------- Refer to API docs here:sarvam Chat Completions API docs sarvam Chat Completions API docs (https://docs.sarvam.ai/api-reference-docs/chat/completions) reasoning_effort can take three possible values: low, medium, and high to be consistent with the OpenAI API spec. Setting any of the three values just enables the thinking mode of sarvam-m. ###vLLM Deployment For easy deployment, we can use vllm>=0.8.5 and create an OpenAI-compatible API endpoint with vllm serve sarvamai/sarvam-m. If you want to use vLLM with python, you can do the following. Training, Testing, and Evaluation Datasets ========================================== Training Dataset: ----------------- • Link:Undisclosed Link: • Data Collection Method by dataset:Hybrid: Human, Synthetic, Automated Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Human, Automated Labeling Method by dataset: • Properties:The model was trained on a significant volume of data, with a substantial portion dedicated to Indian languages. Approximately one-third of the training samples were in 11 Indic languages (Hindi, Bengali, Gujarati, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, and Telugu). Specifically, 30% of coding, math, and reasoning prompts, and 50% of the remaining prompts were translated into these languages. The datasets were designed to improve the model's capabilities in logical reasoning, mathematical problem-solving, and multilingual conversational tasks. No sensors were used for data collection. Properties: Testing Dataset --------------- • Link:Undisclosed Link: • Data Collection Method by dataset:Hybrid: Human, Synthetic, Automated Data Collection Method by dataset: • Labeling Method by dataset:Human Labeling Method by dataset: • Properties: Properties: The testing datasets comprise a wide range of tasks and quantities: • IFEval:Over 500 prompts to test instruction-following capabilities. IFEval: • GSM8K:8,500 high-quality, linguistically diverse grade-school math word problems. GSM8K: • MATH:12,500 challenging math competition problems. MATH: • Big Bench Hard (BBH):A set of 23 difficult tasks designed to test reasoning. Big Bench Hard (BBH): • MMLU:A comprehensive benchmark with multiple-choice questions covering 57 different subjects. MMLU: • HellaSwag:A dataset of 70,000 multiple-choice questions for commonsense natural language inference. HellaSwag: • WinoGrande:A collection of 44,000 problems for commonsense reasoning. WinoGrande: • ARC (AI2 Reasoning Challenge):A set of 7,787 science questions from grade-school examinations. ARC (AI2 Reasoning Challenge): • TruthfulQA:A benchmark consisting of 817 questions across 38 categories to measure a model's truthfulness. TruthfulQA: • Indic Language Benchmarks:Various datasets were used to evaluate performance in 11 Indian languages, including translations of standard benchmarks and Indic-specific evaluations. Indic Language Benchmarks: Evaluation Dataset ------------------ • Link:Undisclosed Link:Undisclosed Link: • Data Collection Method by dataset:Hybrid: Human, Synthetic, Automated Data Collection Method by dataset:Hybrid: Human, Synthetic, Automated Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Human, Automated Labeling Method by dataset:Hybrid: Human, Automated Labeling Method by dataset: • Properties: Properties: • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset: • Properties:IFEval:Contains over 500 prompts designed to test a model's ability to adhere to complex instructionsGSM8K:A dataset of 8,500 high-quality, linguistically diverse grade-school math word problems requiring multi-step reasoning.MATH:A challenging dataset of 12,500 problems from mathematics competitions.Big Bench Hard (BBH):A collection of 23 difficult tasks that are beyond the capabilities of most current language models.MMLU:A massive multitask benchmark consisting of multiple-choice questions across 57 subjects, designed to test a model's general knowledge and problem-solving abilities.TruthfulQA:Comprises 817 questions across 38 categories, designed to measure a model's tendency to produce truthful answers.Indic Language Benchmarks:The model was also evaluated on its performance in 11 Indian languages, showing significant improvement over baseline models on various language tasks. Properties: • IFEval:Contains over 500 prompts designed to test a model's ability to adhere to complex instructions IFEval:Contains over 500 prompts designed to test a model's ability to adhere to complex instructions IFEval: • GSM8K:A dataset of 8,500 high-quality, linguistically diverse grade-school math word problems requiring multi-step reasoning. GSM8K:A dataset of 8,500 high-quality, linguistically diverse grade-school math word problems requiring multi-step reasoning. GSM8K: • MATH:A challenging dataset of 12,500 problems from mathematics competitions. MATH:A challenging dataset of 12,500 problems from mathematics competitions. MATH: • Big Bench Hard (BBH):A collection of 23 difficult tasks that are beyond the capabilities of most current language models. Big Bench Hard (BBH):A collection of 23 difficult tasks that are beyond the capabilities of most current language models. Big Bench Hard (BBH): • MMLU:A massive multitask benchmark consisting of multiple-choice questions across 57 subjects, designed to test a model's general knowledge and problem-solving abilities. MMLU:A massive multitask benchmark consisting of multiple-choice questions across 57 subjects, designed to test a model's general knowledge and problem-solving abilities. MMLU: • TruthfulQA:Comprises 817 questions across 38 categories, designed to measure a model's tendency to produce truthful answers. TruthfulQA:Comprises 817 questions across 38 categories, designed to measure a model's tendency to produce truthful answers. TruthfulQA: • Indic Language Benchmarks:The model was also evaluated on its performance in 11 Indian languages, showing significant improvement over baseline models on various language tasks. Indic Language Benchmarks:The model was also evaluated on its performance in 11 Indian languages, showing significant improvement over baseline models on various language tasks. Indic Language Benchmarks: Benchmark Results ----------------- Results on multilingual benchmarks for 21 European languages with instruction-tuned models: Inference --------- Acceleration Engine: vLLM Test Hardware: • L40s x2 Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "teuken-7b-instruct-commercial-v0.4",
    "nvidia_url": "https://build.nvidia.com/opengpt-x/teuken-7b-instruct-commercial-v0_4",
    "scraped_on": "2025-10-15T21:38:55.711392",
    "tags": [
      "chat",
      "european",
      "multilingual",
      "opengpt-x",
      "sovereign ai",
      "text-to-text"
    ],
    "model_card": "Teuken-7B-instruct-commercial-v0.4 Overview ============================================ Description ----------- Teuken-7B-instruct-commercial-v0.4 generates text as an instruction-tuned 7B-parameter multilingual large language model (LLM) pre-trained on 4 trillion tokens across all 24 official European languages. This model is specifically designed to provide more stable and culturally relevant results across these languages compared to models primarily focused on English. This model is ready for commercial use. Third-Party Community Consideration ------------------------------------ This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case. See link to Non-NVIDIAopenGPT-X/Teuken-7B-instruct-commercial-v0.4 openGPT-X/Teuken-7B-instruct-commercial-v0.4 (https://huggingface.co/openGPT-X/Teuken-7B-instruct-commercial-v0.4) GOVERNING TERMS:This trial service is governed by theNVIDIA API Trial Terms of Service. The model is governed by theNVIDIA Community Model License Agreement; ADDITIONAL INFORMATION:Apache License Version 2.0. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache License Version 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. Deployment Geography -------------------- Global Use Case -------- Teuken-7B-instruct-commercial-v0.4 is designed for a range of multilingual natural language processing tasks, with a strong emphasis on serving both commercial and research needs across the 24 official languages of the European Union. Its primary use case is to provide a powerful and culturally-aware language model that performs reliably across European languages, which are often underrepresented in other large language models. Specific use cases include: • Multilingual Chatbots and Virtual Assistants: Ideal for developing conversational AI for international customer service and enabling businesses to communicate with customers in their native language. • Text Generation and Content Creation: The model can generate articles, marketing copy, reports, and other written content in any of the 24 official European Union (EU) languages. • Document Summarization: It can be used to create concise summaries of long documents, reports, or articles, saving time and effort in multilingual environments. • Information Extraction: The model can identify and pull specific information from unstructured text across various languages, which is useful for data analysis and business intelligence. • Retrieval-Augmented Generation (RAG): It is well-suited for integration into RAG systems, where it can query a knowledge base to provide informed answers in multiple languages. Limitations The developers explicitly state that this model is not intended for tasks that require mathematical reasoning or code generation. Its core strength lies in its linguistic capabilities across its trained languages. Release Date ------------ Hugging Face10/25/2024 viaopenGPT-X/Teuken-7B-instruct-commercial-v0.4.Build.NVIDIA.com07-25-2025 vialink Hugging Face openGPT-X/Teuken-7B-instruct-commercial-v0.4 (https://huggingface.co/openGPT-X/Teuken-7B-instruct-commercial-v0.4) Build.NVIDIA.com link (https://build.nvidia.com/opengpt-x/teuken-7b-instruct-commercial-v0_4) Model Architecture ================== Architecture Type:Transformer Architecture Type: Network Architecture:Teuken-7B-Instruct Network Architecture: This model was developed based on openGPT-X/Teuken-7B-base openGPT-X/Teuken-7B-instruct-commercial-v0.4 · Hugging Face The model is an instruction-tuned version of its base model, fine-tuned on a high-quality, multilingual dataset using the axolotl framework. Key design choices for the training process focused on optimizing for multilingual performance. • Model Optimization:The training was performed with bfloat16 precision and used the paged_adamw_8bit optimizer. Model Optimization: • Hyperparameter Tuning:Learning Rate:2⋅10−5Learning Rate Scheduler:CosineBatch Size:64Warmup Steps:100 Hyperparameter Tuning: • Learning Rate:2⋅10−5 Learning Rate: • Learning Rate Scheduler:Cosine Learning Rate Scheduler: • Batch Size:64 Batch Size: • Warmup Steps:100 Warmup Steps: • Training Parameters:Epochs:2Sequence Length:4096 tokens Training Parameters: • Epochs:2 Epochs: • Sequence Length:4096 tokens Sequence Length: Input ===== Input Type(s):TextInput Format(s):StringsInput Parameters:One-Dimensional (1D)Input Range:[0, 1] (float32) or [0, 255] (uint8, auto-converted)Other Properties Related to Input:Max Input Tokens: 4,096 Input Type(s): Input Format(s): Input Parameters: Input Range: Other Properties Related to Input: Output ====== Output Type(s)Output Format(s): StringsOutput parameters:One-Dimensional (1D)Other Properties Related to Output:Max Input Tokens: 4,096 Output Type(s) Output Format(s) Output parameters: Other Properties Related to Output: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration ==================== Runtime Engine: vLLM, TensorRT Supported Hardware Microarchitecture Compatibility -------------------------------------------------- • NVIDIA Ampere • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace Preferred Operating System(s) ----------------------------- Linux Model Version(s) ---------------- Teuken-7B-instruct-commercial-v0.4 Usage ----- Prerequisites ------------- The model requires a few libraries that can be installed in your Python environment: After installation, here's an example of how to use the model: As this model is a fine-tuned model, it must be used with the provided prompt template. Using the model without the prompt template is not intended and is not recommended. The prompt template is defined as follows: The prompt template is also directly integrated in the Tokenizer and can be used as follows: This example demonstrates how to load the model and tokenizer, prepare input, generate text, and print the result. Running the Model with vLLM --------------------------- Starting the vLLM Server: Use Chat API with vLLM and pass the language of the Chat-Template as extra body: The default language of the Chat-Template can also be set when starting the vLLM Server. For this create a new file with the name lang and the content DE and start the vLLM Server as follows: Running the Model with vLLM offline Batched Inference ----------------------------------------------------- Training, Testing, and Evaluation Datasets ========================================== Training Dataset: ----------------- • Link:Undisclosed Link: • Data Collection Method by dataset:Hybrid: Human, Automated Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Human, Automated Labeling Method by dataset: • Properties:The base model, Teuken-7B-base-v0.4, was pre-trained on 4 trillion tokens from publicly available sources, with a data cutoff of September 2023. The instruction-tuned model was fine-tuned on a collection of datasets in English, German, and 22 other official European languages. Properties: Testing Dataset --------------- • Link:Undisclosed Link: • Data Collection Method by dataset:Hybrid: Human, Automated Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Human, Automated Labeling Method by dataset: • Properties:The model was evaluated on translated versions of several established benchmark datasets, including HellaSwag, ARC (AI2 Reasoning Challenge), and TruthfulQA. These benchmarks were translated into 21 official EU languages to assess the model's multilingual capabilities. The evaluation aimed to measure performance on tasks such as commonsense reasoning, question answering, and truthfulness Properties: Evaluation Dataset ------------------ • Link:Undisclosed Link:Undisclosed Link: • Data Collection Method by dataset:Hybrid: Human, Automated Data Collection Method by dataset:Hybrid: Human, Automated Data Collection Method by dataset: • Labeling Method by dataset:Human Labeling Method by dataset:Human Labeling Method by dataset: • Properties:Undisclosed Properties:Undisclosed Properties: Benchmark Results ----------------- Results on multilingual benchmarks for 21 European languages with instruction-tuned models: Inference --------- Acceleration Engine: vLLM Test Hardware: • L40s Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3.3-nemotron-super-49b-v1.5",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1_5",
    "scraped_on": "2025-10-15T21:38:52.570326",
    "tags": [
      "advanced reasoning",
      "chat",
      "function calling",
      "instruction following",
      "math",
      "nvidia"
    ],
    "model_card": "Llama-3.3-Nemotron-Super-49B-v1.5 ================================= Model Overview ============== Llama-3.3-Nemotron-Super-49B-v1.5 is a significantly upgraded version of Llama-3.3-Nemotron-Super-49B-v1 and is a large language model (LLM) which is a derivative of Meta Llama-3.3-70B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and agentic tasks, such as RAG and tool calling. The model supports a context length of 128K tokens. Llama-3.3-Nemotron-Super-49B-v1.5 is a model which offers a great tradeoff between model accuracy and efficiency. Efficiency (throughput) directly translates to savings. Using a novel Neural Architecture Search (NAS) approach, we greatly reduce the model’s memory footprint, enabling larger workloads, as well as fitting the model on a single GPU at high workloads (H200). This NAS approach enables the selection of a desired point in the accuracy-efficiency tradeoff. For more information on the NAS approach, please refer tothis paper this paper (https://arxiv.org/abs/2411.19146) The model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Science, and Tool Calling. Additionally, the model went through multiple stages of Reinforcement Learning (RL) including Reward-aware Preference Optimization (RPO) for chat, Reinforcement Learning with Verifiable Rewards (RLVR) for reasoning, and iterative Direct Preference Optimization (DPO) for Tool Calling capability enhancements. The final checkpoint was achieved after merging several RL and DPO checkpoints. This model is part of the Llama Nemotron Collection. You can find the other model(s) in this family here: • Llama-3.1-Nemotron-Nano-4B-v1.1 Llama-3.1-Nemotron-Nano-4B-v1.1 (https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1) • Llama-3.1-Nemotron-Ultra-253B-v1 Llama-3.1-Nemotron-Ultra-253B-v1 (https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1) This model is ready for commercial use. License/Terms of Use ==================== GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service; and the use of this model is governed by theNVIDIA Open Model License.Additional Information:Llama 3.3 Community License Agreement. Built with Llama. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Open Model License. (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Llama 3.3 Community License Agreement (https://www.llama.com/llama3_3/license/) Model Developer:NVIDIA Model Developer: Model Dates:Trained between November 2024 and July 2025 Model Dates: Data Freshness:The pretraining data has a cutoff of 2023 per Meta Llama 3.3 70B Data Freshness: Deployment Geography ==================== Global Use Case: --------- Developers designing AI Agent systems, chatbots, RAG systems, and other AI-powered applications. Also suitable for typical instruction-following tasks. Release Date: ------------- • Hugging Face 7/25/2025 viaLlama-3_3-Nemotron-Super-49B-v1_5 Llama-3_3-Nemotron-Super-49B-v1_5 (https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5) • build.nvidia.com 7/25/2025Llama-3_3-Nemotron-Super-49B-v1_5 Llama-3_3-Nemotron-Super-49B-v1_5 (https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1_5) References ========== • [2505.00949] Llama-Nemotron: Efficient Reasoning Models [2505.00949] Llama-Nemotron: Efficient Reasoning Models (https://arxiv.org/abs/2505.00949) • [2502.00203] Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment [2502.00203] Reward-aware Preference Optimization: A Unified Mathematical Framework for Model Alignment (https://arxiv.org/abs/2502.00203) • [2411.19146]Puzzle: Distillation-Based NAS for Inference-Optimized LLMs [2411.19146]Puzzle: Distillation-Based NAS for Inference-Optimized LLMs (https://arxiv.org/abs/2411.19146) Model Architecture ================== Architecture Type:Dense decoder-only Transformer model Architecture Type: Network Architecture:Llama 3.3 70B Instruct, customized through Neural Architecture Search (NAS) Network Architecture: The model is a derivative of Meta’s Llama-3.3-70B-Instruct, using Neural Architecture Search (NAS). The NAS algorithm results in non-standard and non-repetitive blocks. This includes the following: Skip attention: In some blocks, the attention is skipped entirely, or replaced with a single linear layer. Variable FFN: The expansion/compression ratio in the FFN layer is different between blocks. We utilize a block-wise distillation of the reference model, where for each block we create multiple variants providing different tradeoffs of quality vs. computational complexity, discussed in more depth below. We then search over the blocks to create a model which meets the required throughput and memory (optimized for a single H100-80GB GPU) while minimizing the quality degradation. The model then undergoes knowledge distillation (KD), with a focus on English single and multi-turn chat use-cases. The KD step included 40 billion tokens consisting of a mixture of 3 datasets - FineWeb, Buzz-V1.2 and Dolma. Intended use ============ Llama-3.3-Nemotron-Super-49B-v1.5 is a general purpose reasoning and chat model intended to be used in English and coding languages. Other non-English languages (German, French, Italian, Portuguese, Hindi, Spanish, and Thai) are also supported. Input ===== • Input Type:Text Input Type: • Input Format:String Input Format: • Input Parameters:One-Dimensional (1D) Input Parameters: • Other Properties Related to Input:Context length up to 131,072 tokens Other Properties Related to Input: Output ====== • Output Type:Text Output Type: • Output Format:String Output Format: • Output Parameters:One-Dimensional (1D) Output Parameters: • Other Properties Related to Output:Context length up to 131,072 tokens Other Properties Related to Output: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Model Version ============= 1.5 (07/25/2025) Software Integration ==================== • Runtime Engine:Transformers Runtime Engine:Transformers Runtime Engine: • Recommended Hardware Microarchitecture Compatibility:NVIDIA AmpereNVIDIA Hopper Recommended Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Hopper • Preferred Operating System(s):Linux Preferred Operating System(s):Linux Preferred Operating System(s): Quick Start and Usage Recommendations: ====================================== • By default (empty system prompt) the model will respond in reasoning ON mode. Setting/no_thinkin the system prompt will enable reasoning OFF mode. • We recommend setting temperature to0.6, and Top P to0.95for Reasoning ON mode • We recommend using greedy decoding for Reasoning OFF mode You can try this model out through the preview API, using this link:Llama-3_3-Nemotron-Super-49B-v1_5. Llama-3_3-Nemotron-Super-49B-v1_5 (https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1_5) Use It with vLLM ================ pip install vllm==0.9.2 An example on how to serve with vLLM: Running a vLLM Server with Tool-call Support -------------------------------------------- To enable tool calling usage with this model, we provide a tool parser in the repository. Here is an example on how to use it: After launching a vLLM server, you can call the server with tool-call support using a Python script like below. Training and Evaluation Datasets ================================ Training Datasets ================= A large variety of training data was used for the knowledge distillation phase before post-training pipeline, 3 of which included: FineWeb, Buzz-V1.2, and Dolma. The data for the multi-stage post-training phases for improvements in Code, Math, and Reasoning is a compilation of SFT and RL data that supports improvements of math, code, general reasoning, and instruction following capabilities of the original Llama instruct model. Prompts have been sourced from either public and open corpus or synthetically generated. Responses were synthetically generated by a variety of models, with some prompts containing responses for both reasoning on and off modes, to train the model to distinguish between two modes. NVIDIA will be releasing the post-training dataset in the coming weeks. Data Collection for Training Datasets:Hybrid: Automated, Human, Synthetic Data Collection for Training Datasets: Data Labeling for Training Datasets:Hybrid: Automated, Human, Synthetic Data Labeling for Training Datasets: Evaluation Datasets =================== We used the datasets listed below to evaluate Llama-3.3-Nemotron-Super-49B-v1.5. Data Collection for Evaluation Datasets: • Hybrid: Human. Synthetic Data Labeling for Evaluation Datasets: • Hybrid: Human, Synthetic, Automatic Evaluation Results ================== We evaluate the model using temperature=0.6, top_p=0.95, and 64k sequence length. We run the benchmarks up to 16 times and average the scores to be more accurate. MATH500 ------- AIME 2024 --------- AIME 2025 --------- GPQA ---- LiveCodeBench 24.10-25.02 ------------------------- BFCL v3 ------- IFEval ------ ArenaHard --------- Humanity's Last Exam (Text-Only Subset) --------------------------------------- MMLU Pro (CoT) -------------- All evaluations were done using theNeMo-Skillsrepository. NeMo-Skills (https://github.com/NVIDIA/NeMo-Skills) Inference: ========== Engine: • Transformers Test Hardware: • 2x NVIDIA H100-80GB • 2x NVIDIA A100-80GB GPUs Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++Explainability,Bias,Safety & Security, andPrivacySubcards. Explainability (./EXPLAINABILITY.md) Bias (./BIAS.md) Safety & Security (./SAFETY&SECURITY.md) Privacy (./PRIVACY.md) Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Citation ========"
  },
  {
    "name": "parakeet-tdt-0.6b-v2",
    "nvidia_url": "https://build.nvidia.com/nvidia/parakeet-tdt-0_6b-v2",
    "scraped_on": "2025-10-15T21:38:52.129010",
    "tags": [
      "asr",
      "english",
      "nvidia",
      "nvidia nim",
      "nvidia riva",
      "speech-to-text"
    ],
    "model_card": "Parakeet-tdt-0.6b-v2 English speech to text model ================================================= Description: ============ Parakeet-tdt-0.6b-v2is a 600-million-parameter automatic speech recognition (ASR) model designed for high-quality English transcription, featuring support for punctuation, capitalization, and accurate timestamp prediction. This XL variant of theFastConformerarchitecture integrates theTDTdecoder and is trained with full attention, enabling efficient transcription of audio segments up to 24 minutes in a single pass. FastConformer (https://arxiv.org/abs/2305.05084) TDT (https://arxiv.org/abs/2304.06795) Key Features • Accurate word-level timestamp predictions • Automatic punctuation and capitalization • Robust performance on spoken numbers, and song lyrics transcription This model is ready for commercial/non-commercial use. License/Terms of Use: ===================== GOVERNING TERMS: Use of this model is governed by theNVIDIA Community Model License Agreement. NVIDIA Community Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Deployment Geography: --------------------- Global Use Case: --------- This model serves developers, researchers, academics, and industries building applications that require speech-to-text capabilities, including but not limited to: conversational AI, voice assistants, transcription services, subtitle generation, and voice analytics platforms. Model Architecture: ------------------- Architecture Type: Architecture Type FastConformer-TDT Network Architecture: Network Architecture • This model was developed based onFastConformer encoderarchitecture and TDT decoder. FastConformer encoder (https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) • This model has 600 million model parameters. Input: ------ Input Type(s):16kHz AudioInput Format(s):.wavand.flacaudio formatsInput Parameters:1D (audio signal)Other Properties Related to Input:Monochannel audio Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ------- Output Type(s):TextOutput Format:StringOutput Parameters:1D (text)Other Properties Related to Output:Punctuations and Capitalizations included. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration: ===================== Runtime Engine(s): • Riva Supported Hardware Microarchitecture Compatibility: • NVIDIA Ampere • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Volta [Preferred/Supported] Operating System(s): • Linux Hardware Specific Requirements: Atleast 2GB RAM for model to load. The bigger the RAM, the larger audio input it supports. Model Version ------------- Current version: parakeet-0.6b-tdt-v2 Training and Evaluation Datasets: ================================= Training -------- This model was trained using the NeMo toolkit [3], following the strategies below: • Initialized from a wav2vec SSL checkpoint pretrained on the LibriLight dataset[7]. • Trained for 150,000 steps on 128 A100 GPUs. • Dataset corpora were balanced using a temperature sampling value of 0.5. • Stage 2 fine-tuning was performed for 2,500 steps on 4 A100 GPUs using approximately 500 hours of high-quality, human-transcribed data of NeMo ASR Set 3.0. Training was conducted using thisexample scriptandTDT configuration. example script (https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_transducer/speech_to_text_rnnt_bpe.py) TDT configuration (https://github.com/NVIDIA/NeMo/blob/main/examples/asr/conf/fastconformer/hybrid_transducer_ctc/fastconformer_hybrid_tdt_ctc_bpe.yaml) The tokenizer was constructed from the training set transcripts using thisscript. script (https://github.com/NVIDIA/NeMo/blob/main/scripts/tokenizers/process_asr_text_tokenizer.py) Data Collection Method by dataset • Hybrid: Automated, Human Labeling Method by dataset • Hybrid: Synthetic, Human Properties: • Noise robust data from various sources • Single channel, 16kHz sampled data References ========== [1]Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition (https://arxiv.org/abs/2305.05084) [2]Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Efficient Sequence Transduction by Jointly Predicting Tokens and Durations (https://arxiv.org/abs/2304.06795) [3]NVIDIA NeMo Toolkit NVIDIA NeMo Toolkit (https://github.com/NVIDIA/NeMo) [4]Youtube-commons: A massive open corpus for conversational and multimodal data Youtube-commons: A massive open corpus for conversational and multimodal data (https://huggingface.co/blog/Pclanglais/youtube-commons) [5]Yodas: Youtube-oriented dataset for audio and speech Yodas: Youtube-oriented dataset for audio and speech (https://arxiv.org/abs/2406.00899) [6]HuggingFace ASR Leaderboard HuggingFace ASR Leaderboard (https://huggingface.co/spaces/hf-audio/open_asr_leaderboard) [7]MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages (https://arxiv.org/abs/2410.01036) Inference: ========== Engine: Engine • NVIDIA NeMo Test Hardware: Test Hardware • NVIDIA A10 • NVIDIA A100 • NVIDIA A30 • NVIDIA H100 • NVIDIA L4 • NVIDIA L40 • NVIDIA Turing T4 • NVIDIA Volta V100 Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcardshere. here (https://developer.nvidia.com/blog/enhancing-ai-transparency-and-ethical-considerations-with-model-card/) Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "gpt-oss-120b",
    "nvidia_url": "https://build.nvidia.com/openai/gpt-oss-120b",
    "scraped_on": "2025-10-15T21:38:51.597870",
    "tags": [
      "chat",
      "math",
      "openai",
      "reasoning",
      "text-to-text"
    ],
    "model_card": "GPT OSS 120B Overview ===================== Description: ============ OpenAI releases the gpt-oss family of open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases. The family consists of the: • gpt-oss-120b— for production, general purpose, high reasoning use-cases that fits into a single H100 GPU (117B parameters with 5.1B active parameters) • gpt-oss-20b— for lower latency, and local or specialized use-cases (21B parameters with 3.6B active parameters). Thegpt-oss-120bmodel is architecturally designed as a Mixture-of-Experts (MoE) model. This model features SwiGLU activations and learned attention sinks within its architecture. It functions as a reasoning model, supporting capabilities such as chain-of-thought processing, adjustable reasoning effort levels, instruction following, and tool use. This model is text-only for both input and output modalities, enabling enterprises and governments to deploy it on-premises or in private cloud environments for enhanced data security and privacy. Model Highlights: • Permissive Apache 2.0 license:Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment. Permissive Apache 2.0 license: • Configurable reasoning effort:Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs. Configurable reasoning effort: • Full chain-of-thought:Gain complete access to the model's reasoning process, facilitating easier debugging and increased trust in outputs. It's not intended to be shown to end users. Full chain-of-thought: • Fine-tunable:Fully customize models to your specific use case through parameter fine-tuning. Fine-tunable: • Agentic capabilities:Use the models' native capabilities for function calling, web browsing, python code execution, and structured outputs. Agentic capabilities: This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAgpt-oss-120b model card. gpt-oss-120b model card (https://huggingface.co/openai/gpt-oss-120b) License and Terms of Use: ------------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Apache License Version 2.0. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache License Version 2.0 (https://www.apache.org/licenses/LICENSE-2.0) Deployment Geography: --------------------- Global Use Case: --------- Intended for use as a reasoning model with features like chain-of-thought and adjustable reasoning effort levels. It supports instruction following and tool use, offering transparency, customization, and deployment flexibility for developers, researchers, and startups. Additionally, it enables enterprises and governments to deploy on-premises or in private clouds to ensure data security and privacy. Release Date: ------------- Build.NVIDIA.com - 08/05/2025 vialinkHugging Face - 08/05/2025 vialink link (https://build.nvidia.com/openai/gpt-oss-120b) link (https://huggingface.co/openai/gpt-oss-120b) Reference(s): ============= • OpenAI Cookbook OpenAI Cookbook (https://cookbook.openai.com/) • Optimizing OpenAI GPT-OSS Models with NVIDIA TensorRT-LLM Optimizing OpenAI GPT-OSS Models with NVIDIA TensorRT-LLM (https://github.com/openai/openai-cookbook/blob/main/articles/run-nvidia.ipynb) • Running a High Performance GPT-OSS-120B Inference Server with TensorRT-LLM Running a High Performance GPT-OSS-120B Inference Server with TensorRT-LLM (https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog9_Deploying_GPT_OSS_on_TRTLLM.md) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Mixture-of-Experts (MoE)Total Parameters:117BActive Parameters:5.7BVocabulary Size:201,088 Architecture Type: Network Architecture: Total Parameters: Active Parameters: Vocabulary Size: Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:One Dimensional (1D)Other Properties Related to Input:Uses RoPE with a 128k context length, with attention layers alternating between full context and a sliding 128-token window. Includes a learned attention sink per-head. Employs SwiGLU activations in the MoE layers, and the router performs a Top-K operation (K=4) followed by a Sigmoid function. GEMMs in the MoE include a per-expert bias. Utilizes tiktoken for tokenization. Input Context Length (ISL): 128000 Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:One Dimensional (1D)Other Properties Related to Output:The model is designed to be compatible with the OpenAI Responses API and supports Structured Output. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems [or name equivalent hardware preference]. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration: ===================== Runtime Engine(s): • NeMo Framework (based on 25.07) Supported Hardware Microarchitecture Compatibility: • NVIDIA Blackwell: B200, GB200 • NVIDIA Hopper: H200 Operating System(s):Linux Operating System(s): Model Version(s): ================= gpt-oss-120bv1.0 (August 5, 2025) Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ----------------- • Training Data Collection:Undisclosed Training Data Collection: • Training Labeling:Undisclosed Training Labeling: • Training Properties:The model has approximately 117 billion parameters. Weights for all layers are in BF16, except for MoE projection weights, which are in MXFP4. The reference implementation currently upcasts all weights to BF16. Activations are expected to be in BF16 or FP8. Training Properties: Testing Dataset: ---------------- • Testing Data Collection:Undisclosed Testing Data Collection: • Testing Labeling:Undisclosed Testing Labeling: • Testing Properties:The model is tested against benchmarks such as MMLU and GPQA, among others including LiveCodeBench, AIME 2024, and MATH-500. Testing Properties: Evaluation Dataset: ------------------- • Evaluation Data Collection:Undisclosed Evaluation Data Collection: • Evaluation Labeling:Undisclosed Evaluation Labeling: • Evaluation Benchmark Score: Evaluation Benchmark Score: Above scores were measured for the high reasoning level. Safety Results: --------------- The following evaluations check that the model does not comply with requests for content that is disallowed under OpenAI’s safety policies, including hateful content or illicit advice. Inference: ========== Acceleration Engine:vLLMTest Hardware:NVIDIA Blackwell: GB200 Acceleration Engine: Test Hardware: Additional Details ================== The model is released with the native quantization support. Specifically,MXFP4is used for the linear projection weights in the MoE layer. It is stored the MoE tensor in two parts: MXFP4 (https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf) • tensor.blocksstores the actual fp4 values. Every two values are packed in oneuint8value. • tensor.scalesstores the block scale. The block scaling is done among the last dimension for all MXFP4 tensors. All other tensors are stored in BF16. It is recommended to use BF16 as the activation precision for the model. Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "deepseek-v3.1",
    "nvidia_url": "https://build.nvidia.com/deepseek-ai/deepseek-v3_1",
    "scraped_on": "2025-10-15T21:38:50.945402",
    "tags": [
      "chat",
      "deepseek-ai",
      "reasoning",
      "text-to-text"
    ],
    "model_card": "DeepSeek-V3.1 Overview ====================== Description =========== DeepSeek-V3.1 is a hybrid model that supports both thinking and non-thinking modes. Compared to the previous version, this upgrade brings improvements in multiple aspects: • Hybrid thinking mode: One model supports both modes by changing the chat template. • Smarter tool calling: Through post-training optimization, the model's performance in tool usage and agent tasks has significantly improved. This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIA Model CardDeepSeek-V3.1 Model Card. DeepSeek-V3.1 Model Card (https://huggingface.co/deepseek-ai/DeepSeek-V3.1) License and Terms of Use: ------------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:MIT License. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) MIT License (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md) Deployment Geography: ===================== Global Use Case: ========= Designed to handle general instruction-following tasks, DeepSeek-V3.1 can be integrated into AI assistants across various domains, including business applications.Supported Languages: Primarily English and Chinese, with multilingual capabilities. • Extended long-context tasks (up to 128K tokens):Long-form document summarizationWhole-book comprehensionCodebase and legal document analysis Extended long-context tasks (up to 128K tokens): • Long-form document summarization • Whole-book comprehension • Codebase and legal document analysis • Complex reasoning and problem-solving:Multi-step logic and mathematical reasoningScientific and analytical writing assistancePuzzle solving and structured decision-making Complex reasoning and problem-solving: • Multi-step logic and mathematical reasoning • Scientific and analytical writing assistance • Puzzle solving and structured decision-making • Code generation and software development:Live coding and debugging supportGenerating or completing scripts and software logicAutomated documentation and code analysis Code generation and software development: • Live coding and debugging support • Generating or completing scripts and software logic • Automated documentation and code analysis • Tool-augmented and agent-based applications:Intelligent agents with dynamic tool invocationInteractive chatbots with hybrid “thinking” modeUse in systems that call external APIs or utilities Tool-augmented and agent-based applications: • Intelligent agents with dynamic tool invocation • Interactive chatbots with hybrid “thinking” mode • Use in systems that call external APIs or utilities Release Date: ============= Build.NVIDIA.com:08/26/2025 (link)Hugging Face:08/20/2025 (link) Build.NVIDIA.com: link (https://build.nvidia.com/deepseek-ai/deepseek-v3_1) Hugging Face: link (https://huggingface.co/deepseek-ai/DeepSeek-V3.1) References: =========== • DeepSeek-V3 Technical Report DeepSeek-V3 Technical Report (https://arxiv.org/abs/2412.19437) • DeepSeek-V3 GitHub Repository DeepSeek-V3 GitHub Repository (https://github.com/deepseek-ai/DeepSeek-V3) • Model Training Details – Medium Article Model Training Details – Medium Article (https://medium.com/@deepseek-ai/introducing-deepseek-v3-48ef6325df6c) • DeepSeek-V3.1 Chat Templates & Configs DeepSeek-V3.1 Chat Templates & Configs (https://huggingface.co/deepseek-ai/DeepSeek-V3.1/blob/main/tokenizer_config.json) Model Architecture: =================== • Architecture Type: Transformer (Decoder-only) Architecture Type • Parameter Count: 671B (Total), 37B (Activated) Parameter Count • Notable Architectural Features: Hybrid thinking mode Notable Architectural Features • Base Model: DeepSeek-V3.1-Base Base Model • Additional Notes: Trained using the UE8M0 FP8 scale data format Additional Notes Input ===== Input Type(s):TextInput Formats:StringInput Parameters:One-Dimensional (1D)Other Properties Related to Input:Chat Template for different modes, Tool descriptions. Context Length: Supports up to 128K tokens Input Type(s): Input Formats: Input Parameters: Other Properties Related to Input: Output ====== Output Type(s):TextOutput Formats:StringOutput Parameters:One-Dimensional (1D)Other Properties Related to Output:Special Features: Supports both thinking and non-thinking response modes. Output Type(s): Output Formats: Output Parameters: Other Properties Related to Output: Our Al models are designed and/or optimized to run onNVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. NVIDIA GPU-accelerated systems Software Integration ==================== Runtime Engine(s):SGLangSupported Hardware Microarchitecture Compatibility: Runtime Engine(s): Supported Hardware Microarchitecture Compatibility: • NVIDIA Blackwell • NVIDIA Hopper Preferred/Supported Operating System(s):Linux Preferred/Supported Operating System(s): The integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment. Model Version: ============== DeepSeek-V3.1 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ----------------- • Data Collection Method: Undisclosed Data Collection Method • Labeling Method: Undisclosed Labeling Method • Properties: Properties • Expanded dataset with additional long documents.Training Format: UE8M0 FP8 scale data format.Two-phase long context extension; 32K extension phase: 630B tokens (10x expansion)128K extension phase: 209B tokens (3.3x expansion) • Training Format: UE8M0 FP8 scale data format. • Two-phase long context extension; 32K extension phase: 630B tokens (10x expansion) • 128K extension phase: 209B tokens (3.3x expansion) Testing Dataset: ---------------- • Data Collection Method by dataset: Undisclosed Data Collection Method by dataset • Labeling Method by dataset: Undisclosed Labeling Method by dataset • Properties: Undisclosed Properties Evaluation Benchmark Results: ----------------------------- Please see the Evaluation section of theHuggingFace DeepSeek-V3 Model Cardfor more information. HuggingFace DeepSeek-V3 Model Card (https://huggingface.co/deepseek-ai/DeepSeek-V3) • Data Collection Method: Hybrid: Human, Automated Data Collection Method • Labeling Method: Hybrid: Human, Automated Labeling Method • Properties: Evaluation on General, Search Agent, Code, Code Agent, and Math benchmarks. Properties Note: • Search agents are evaluated with our internal search framework, which uses a commercial search API + webpage filter + 128K context window. Search agent results of R1-0528 are evaluated with a pre-defined workflow. Search agents are evaluated with our internal search framework, which uses a commercial search API + webpage filter + 128K context window. Search agent results of R1-0528 are evaluated with a pre-defined workflow. • SWE-bench is evaluated with our internal code agent framework. SWE-bench is evaluated with our internal code agent framework. • HLE is evaluated with the text-only subset. HLE is evaluated with the text-only subset. Inference: ========== Acceleration Engine:SGLangTest Hardware:NVIDIA B200 Acceleration Engine: Test Hardware: Additional Details: =================== The model uses different chat templates for its operational modes. It also supports tool calls and agent functionality with specific formatting requirements. Chat Template ------------- The details of our chat template is described intokenizer_config.jsonandassets/chat_template.jinja. Here is a brief description. Non-Thinking ------------ First-Turn Prefix:<｜begin▁of▁sentence｜>{system prompt}<｜User｜>{query}<｜Assistant｜></think> With the given prefix, DeepSeek V3.1 generates responses to queries in non-thinking mode. Unlike DeepSeek V3, it introduces an additional token</think>. Multi-Turn Context:<｜begin▁of▁sentence｜>{system prompt}<｜User｜>{query}<｜Assistant｜></think>{response}<｜end▁of▁sentence｜>...<｜User｜>{query}<｜Assistant｜></think>{response}<｜end▁of▁sentence｜> Prefix:<｜User｜>{query}<｜Assistant｜></think> By concatenating the context and the prefix, we obtain the correct prompt for the query. Thinking -------- First-Turn Prefix:<｜begin▁of▁sentence｜>{system prompt}<｜User｜>{query}<｜Assistant｜><think> The prefix of thinking mode is similar to DeepSeek-R1. Multi-Turn Context:<｜begin▁of▁sentence｜>{system prompt}<｜User｜>{query}<｜Assistant｜></think>{response}<｜end▁of▁sentence｜>...<｜User｜>{query}<｜Assistant｜></think>{response}<｜end▁of▁sentence｜> Prefix:<｜User｜>{query}<｜Assistant｜><think> The multi-turn template is the same with non-thinking multi-turn chat template. It means the thinking token in the last turn will be dropped but the</think>is retained in every turn of context. ToolCall -------- Toolcall is supported in non-thinking mode. The format is: <｜begin▁of▁sentence｜>{system prompt}{tool_description}<｜User｜>{query}<｜Assistant｜></think>where the tool_description is Code-Agent ---------- We support various code agent frameworks. Please refer to the above toolcall format to create your own code agents. An example is shown inassets/code_agent_trajectory.html. Search-Agent ------------ We design a specific format for searching toolcall in thinking mode, to support search agent. For complex questions that require accessing external or up-to-date information, DeepSeek-V3.1 can leverage a user-provided search tool through a multi-turn tool-calling process. Please refer to theassets/search_tool_trajectory.htmlandassets/search_python_tool_trajectory.htmlfor the detailed template. Usage Example ------------- How to Run Locally ================== The model structure of DeepSeek-V3.1 is the same as DeepSeek-V3. Please visitDeepSeek-V3repo for more information about running this model locally. DeepSeek-V3 (https://github.com/deepseek-ai/DeepSeek-V3) Ethical Considerations: ======================= NVIDIA believesTrustworthy AI is a shared responsibilityand we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. Trustworthy AI is a shared responsibility here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "qwen3-coder-480b-a35b-instruct",
    "nvidia_url": "https://build.nvidia.com/qwen/qwen3-coder-480b-a35b-instruct",
    "scraped_on": "2025-10-15T21:38:50.517566",
    "tags": [
      "agentic coding",
      "browser use",
      "chat",
      "long context",
      "moe",
      "qwen"
    ],
    "model_card": "Qwen3-Coder-480B-A35B-Instruct ============================== Model Overview ============== Description: ------------ Qwen3-Coder-480B-A35B-Instruct is a state-of-the-art large language model specifically designed for code generation and agentic coding tasks. It is a mixture-of-experts (MoE) model with 480B total parameters and 35B activated parameters, featuring native support for 262,144 tokens context length and extendable up to 1M tokens using YaRN. This model demonstrates significant performance among open models on Agentic Coding, Agentic Browser-Use, and other foundational coding tasks, achieving results comparable to Claude Sonnet. It supports function calling and tool choice capabilities, making it ideal for complex coding workflows and agentic applications. This model is ready for commercial use. License/Terms of Use -------------------- GOVERNING TERMS:This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Apache 2.0. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) Deployment Geography ==================== Deployment Geography: Global Deployment Geography Use Cases ========= • Code Generation: Generate high-quality code from natural language descriptions Code Generation • Agentic Coding: Execute complex coding workflows with function calling Agentic Coding • Repository Understanding: Process large codebases with long-context capabilities Repository Understanding • Tool Integration: Interface with development tools and APIs Tool Integration • Code Review and Analysis: Analyze and improve existing code Code Review and Analysis • Documentation Generation: Create code documentation and comments Documentation Generation • Browser Automation: Agentic browser-use scenarios Browser Automation • Function Calling: Structured tool execution and API integration Function Calling Release Information =================== Release Date: 08/22/2025Build.NVIDIA.com: Available vialink Release Date Build.NVIDIA.com link (https://build.nvidia.com/qwen/Qwen3-Coder-480B-A35B-Instruct) Third-Party Community Consideration ----------------------------------- This model is not owned or developed by NVIDIA. This model has been developed by Qwen (Alibaba Cloud). This model has been developed and built to a third-party's requirements for this application and use case; see link toQwen3-Coder-480B-A35B-Instruct. Qwen3-Coder-480B-A35B-Instruct (https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct) References ---------- • Qwen3-Coder: A Large Language Model for Code Generation Qwen3-Coder: A Large Language Model for Code Generation (https://qwenlm.github.io/blog/qwen3-coder/) • Qwen3-Coder GitHub Repository Qwen3-Coder GitHub Repository (https://github.com/QwenLM/Qwen3-Coder) • Qwen Documentation Qwen Documentation (https://qwen.readthedocs.io/en/latest/) • Hugging Face Model Page Hugging Face Model Page (https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct) • Qwen3 Technical Report (arXiv:2505.09388) Qwen3 Technical Report (arXiv:2505.09388) (https://arxiv.org/abs/2505.09388) Model Architecture ================== Architecture Type: mixture-of-experts (MoE) with Sparse ActivationNetwork Architecture: Qwen3MoeForCausalLM (Transformer-based decoder-only)Parameter Count: 480B total parameters with 35B activated parametersExpert Configuration: 160 experts with 8 activated per forward passAttention Mechanism: Grouped Query Attention (GQA) with 96 query heads and 8 KV headsNumber of Layers: 62Hidden Size: 6144Head Dimension: 128Intermediate Size: 8192MoE Intermediate Size: 2560Context Length: 262,144 tokens (native), extendable to 1M with YaRNVocabulary Size: 151,936 Architecture Type Network Architecture Parameter Count Expert Configuration Attention Mechanism Number of Layers Hidden Size Head Dimension Intermediate Size MoE Intermediate Size Context Length Vocabulary Size Input ===== Input Type(s): Text, Code, Function callsInput Format(s): Natural language prompts, code snippets, structured function callsInput Parameters: Input Type(s) Input Format(s) Input Parameters • Max input length: 262,144 tokens (native), up to 1M with YaRN • Support for function calling format • Tool choice enabled • Trust remote code execution • Custom tool call parser (qwen3_coder) Output ====== Output Type(s): Text, Code, Function responsesOutput Format(s): Natural language responses, code generation, structured function outputsOutput Parameters: One-Dimensional (1D) Output Type(s) Output Format(s) Output Parameters • Max output length: Configurable based on remaining context • Function call responses in structured format Other Properties Related to Output: Other Properties Related to Output • Non-thinking mode (no<think></think>blocks) • Auto tool choice responses Software Integration ==================== Runtime Engine: vLLM, Transformers (4.51.0+)Supported Hardware Platform(s): NVIDIA HopperSupported Operating System(s): LinuxData Type: FP8Data Modality: TextModel Version: v1.0 Runtime Engine Supported Hardware Platform(s) Supported Operating System(s) Data Type Data Modality Model Version Training, Testing, and Evaluation Datasets ========================================== Training Dataset ---------------- • Data Collection Method by dataset: The model was trained on a diverse dataset including code repositories, documentation, and natural language text related to programming Data Collection Method by dataset • Labeling Method by dataset: Supervised fine-tuning with instruction-following data Labeling Method by dataset • Properties: Multi-language code support, instruction-following capabilities, function calling training Properties Testing Dataset --------------- • Data Collection Method by dataset: Standard benchmarks for code generation and agentic tasks Data Collection Method by dataset • Labeling Method by dataset: Automated evaluation metrics Labeling Method by dataset • Properties: HumanEval, MBPP, Agentic coding benchmarks Properties Evaluation Dataset ------------------ • Data Collection Method by dataset: Public benchmarks and custom evaluation sets Data Collection Method by dataset • Labeling Method by dataset: Automated metrics and human evaluation Labeling Method by dataset • Properties: Code generation quality, function calling accuracy, agentic task performance Properties Benchmark Results ----------------- The model achieves significant performance among open models on: • Agentic Coding tasks • Agentic Browser-Use scenarios • Foundational coding benchmarks • Results comparable to Claude Sonnet on various coding tasks Inference ========= Acceleration Engine: vLLMTest Hardware: NVIDIA Hopper Acceleration Engine Test Hardware Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "seed-oss-36b-instruct",
    "nvidia_url": "https://build.nvidia.com/bytedance/seed-oss-36b-instruct",
    "scraped_on": "2025-10-15T21:38:49.967944",
    "tags": [
      "bytedance",
      "chat",
      "reasoning",
      "text-generation",
      "thinking budget"
    ],
    "model_card": "Seed-OSS-36B-Instruct ===================== Description =========== Seed-OSS-36B-Instruct is a 36-billion parameter open-source large language model developed by ByteDance's Seed Team. It is designed for powerful long-context, reasoning, agent and general capabilities, and versatile developer-friendly features. The model features flexible control of thinking budget, enhanced reasoning capability, agentic intelligence, and native long context support up to 512K tokens. This model is ready for commercial use. Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIASeed-OSS-36B-Instruct Model Card. Seed-OSS-36B-Instruct Model Card (https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct) License and Terms of Use: ========================= GOVERNING TERMS:This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Apache 2.0. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) Deployment Geography: Global ============================ Release Date: ============= Build.NVIDIA.com [09/05/2025] vialinkHuggingface 08/20/2025 vialink Build.NVIDIA.com [09/05/2025] vialink link (https://build.nvidia.com/bytedance/seed-oss-36b-instruct) Huggingface 08/20/2025 vialink link (https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct) Reference(s): ============= • Seed-OSS Open-Source Models Seed-OSS Open-Source Models (https://github.com/ByteDance-Seed/seed-oss) • Seed-OSS Technical Documentation Seed-OSS Technical Documentation (https://seed.bytedance.com/en/seed1_6) Model Architecture: =================== Architecture Type:Causal language model with RoPENetwork Architecture:Transformer-based decoder-onlyTotal Parameters:36BActive Parameters:36BVocabulary Size:155KBase Model:Seed-OSS-36B-Base Architecture Type: Network Architecture: Total Parameters: Active Parameters: Vocabulary Size: Base Model: Input: ------ Input Types:TextInput Formats:Natural language prompts, conversational messagesInput Parameters:[One-Dimensional (1D)]Other Input Properties:Max Input Tokens: 512K, Support for thinking budget control, tool calling, long context up to 512K tokensInput Context Length (ISL):512K tokens Input Types: Input Formats: Input Parameters: Other Input Properties: Input Context Length (ISL): Output: ------- Output Types:TextOutput Format:Natural language responses, structured tool callsOutput Parameters:[One-Dimensional (1D)]Other Output Properties:Max Input Tokens: 512K, Chain-of-thought reasoning, thinking budget reflection, direct responsesOutput Context Length (OSL):Configurable based on remaining context Output Types: Output Format: Output Parameters: Other Output Properties: Output Context Length (OSL): Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration: ===================== Runtime Engines:Transformers, vLLM (>=0.10.0)Supported Hardware: Runtime Engines: Supported Hardware: • NVIDIA Ampere • NVIDIA Ada Lovelace • NVIDIA Hopper • NVIDIA Blackwell Preferred/Supported Operating System:Linux Preferred/Supported Operating System: Model Version(s) ================ v1.0 Training, Testing, and Evaluation Datasets: =========================================== Data Modality: Text ------------------- Text Training Data Size: ~12 Trillion Tokens -------------------------------------------- Training Dataset ---------------- Training Data Collection:[Hybrid: Automated, Human]Training Labeling:[Hybrid: Automated, Human]Training Properties:Pre-trained over 12 trillion tokens, knowledge cutoff of 07/2024, data from multiple sources including publicly available internet data, purchased data through vendor partnerships, and in-house generated data Training Data Collection: Training Labeling: Training Properties: Testing Dataset --------------- Testing Data Collection:[Hybrid: Automated, Human]Testing Labeling:[Hybrid: Automated, Human]Testing Properties:Regular safety testing and adversarial testing conducted to identify and address safety vulnerabilities Testing Data Collection: Testing Labeling: Testing Properties: Evaluation Dataset ------------------ Evaluation Benchmark Score:MMLU-Pro: 82.7, MMLU: 87.4, GPQA-D: 80.7, BBH: 89.1, AGIEval-en: 75.8, GSM8K: 93.1, MATH: 84.2, MBPP: 82.6, HumanEval: 78.8, RULER(128K): 94.6, AIR-Bench: 75.6Evaluation Data Collection:[Hybrid: Automated, Human]Evaluation Labeling:[Hybrid: Automated, Human]Evaluation Properties:Safety evaluation including training data filtering, safety fine-tuning evaluation, and content safety measures assessment Evaluation Benchmark Score: Evaluation Data Collection: Evaluation Labeling: Evaluation Properties: Inference ========= Acceleration Engine:Transformers, vLLMTest Hardware:H100 Acceleration Engine: Test Hardware: Additional Details ================== Key features include: • Flexible Control of Thinking Budget: Users can adjust reasoning length dynamically • Enhanced Reasoning Capability: Optimized for reasoning tasks while maintaining general capabilities • Agentic Intelligence: Exceptional performance in agentic tasks like tool-using and issue resolving • Native Long Context: Trained with up-to-512K long context natively • Research-Friendly: Released both with and without synthetic instruction data • Content Safety Measures: Training data filtering, safety fine-tuning, and regular safety evaluation • International (i18n) Model: Primarily optimized for English with limited performance in other languages • Knowledge Cutoff: July 2024 Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report model quality, risk, security vulnerabilities or NVIDIA AI Concernshere here (https://app.intigriti.com/programs/nvidia/nvidiavdp/detail)"
  },
  {
    "name": "bielik-11b-v2.6-instruct",
    "nvidia_url": "https://build.nvidia.com/speakleash/bielik-11b-v2_6-instruct",
    "scraped_on": "2025-10-15T21:38:46.621639",
    "tags": [
      "chat",
      "chatbots",
      "polish",
      "sovereign ai",
      "speakleash",
      "summarization"
    ],
    "model_card": "Bielik-11B-v2.6-Instruct Overview ================================= Description: ============ Bielik-11B-v2.6-Instruct is a generative text model featuring 11 billion parameters, designed to process and understand the Polish language with high precision. It is a linear merge of the Bielik-11B-v2.0-Instruct, Bielik-11B-v2.1-Instruct, and Bielik-11B-v2.2-Instruct models, which are instruct fine-tuned versions of the Bielik-11B-v2. As a result, the model exhibits an exceptional ability to understand and process the Polish language, providing accurate responses and performing a variety of linguistic tasks with high precision. This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIABielik-11B-v2.6-Instruct Model Card. Bielik-11B-v2.6-Instruct Model Card (https://huggingface.co/speakleash/Bielik-11B-v2.6-Instruct) License/Terms of Use: --------------------- GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Open Model License Agreement. Additional Information:Apache License, Version 2.0. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Open Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Apache License, Version 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) Deployment Geography: --------------------- Global (primarily Polish language, with English capabilities). Use Case: --------- This model can be utilized for a variety of Polish language processing tasks, such as text generation, question answering, language modeling, and chatbots. Its strong performance on benchmarks suggests it could be a valuable resource for natural language processing projects and applications targeting the Polish market or requiring high-quality Polish language understanding and generation. It can also be used for English language tasks. Release Date: ------------- • Build.Nvidia.com 09/16/2025 viaLink Link (https://build.nvidia.com/speakleash/bielik-11b-v2_6-instruct) • Huggingface 08/30/2024 viahttps://huggingface.co/speakleash/Bielik-11B-v2.6-Instruct https://huggingface.co/speakleash/Bielik-11B-v2.6-Instruct Reference(s): ============= • Technical report:Link Technical report:Link Technical report: Link (https://arxiv.org/abs/2505.02410) • Chat Arena:Link Chat Arena:Link Chat Arena: Link (https://arena.speakleash.org.pl/) Model Architecture: =================== • Architecture Type:Causal decoder-only (Transformer-based) Architecture Type: • Network Architecture:Based on Mistral, LLaMA-like architecture. Network Architecture: • This model was developed based ona linear merge of Bielik-11B-v2.0-Instruct, Bielik-11B-v2.1-Instruct, and Bielik-11B-v2.2-Instruct, which are fine-tuned versions of Bielik-11B-v2. Bielik-11B-v2 was initialized from Mistral-7B-v0.2. This model was developed based on • This model has11 billionmodel parameters. This model has model parameters. Input: ====== • Input Type(s):Text Input Type(s): • Input Format(s):String Input Format(s): • Input Parameters:One-Dimensional (1D) for text. Input Parameters: • Other Properties Related to Input:Context Length: 32,768 tokens natively. Other Properties Related to Input: • Context Length: 32,768 tokens natively. Output: ======= • Output Type(s):Text Output Type(s): • Output Format:String. Output Format: • Output Parameters:One-Dimensional (1D) for text. Output Parameters: • Other Properties Related to Output:Languages: Polish, English. Other Properties Related to Output: • Languages: Polish, English. Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration: ===================== Runtime Engine(s): Runtime Engine(s) • vLLM Runtime Engine(s): Runtime Engine(s) • NVIDIA Lovelace • NVIDIA Hopper [Preferred/Supported] Operating System(s): ========================================== • [Linux] • [Windows] • [macOS] (especially with GGUF and llama.cpp support) Model Version(s): ================= Bielik-11B-v2.6-Instruct Other related versions include GGUF, GPTQ, and FP8 quantized versions. Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= • The model was fine-tuned on a dataset comprising over 20 million instructions (over 10 billion tokens). The DPO-Positive (DPO-P) fine-tuning used a dataset of over 66,000 examples. The base model Bielik-11B-v2 was trained on 400 billion tokens from Polish text corpora (SpeakLeash project) and English texts (SlimPajama dataset). • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Hybrid: Automated, Human, Synthetic • Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset: • Hybrid: Automated, Human, Synthetic • Properties (Quantity, Dataset Descriptions, Sensor(s)):Instruction fine-tuning: >20 million instructions, >10 billion tokens.DPO-P fine-tuning: >66,000 examples.Pre-training (Bielik-11B-v2): 400 billion tokens. The SpeakLeash dataset for Bielik v3 (related models) comprised 237 billion Polish tokens and was supplemented with English texts from SlimPajama, totaling 292 billion tokens from 303 million documents. Properties (Quantity, Dataset Descriptions, Sensor(s)): • Instruction fine-tuning: >20 million instructions, >10 billion tokens. • DPO-P fine-tuning: >66,000 examples. • Pre-training (Bielik-11B-v2): 400 billion tokens. The SpeakLeash dataset for Bielik v3 (related models) comprised 237 billion Polish tokens and was supplemented with English texts from SlimPajama, totaling 292 billion tokens from 303 million documents. Testing Dataset: ================ • Data Collection Method by dataset: Undisclosed Data Collection Method by dataset • Labeling Method by dataset: Undisclosed Labeling Method by dataset • Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: =================== • Benchmark Score:Open PL LLM Leaderboard (5-shot):65.71 (outperforms models <70B parameters, competitive with 70B models).Polish MT-Bench:8.556250 (outperforms GPT-3.5-turbo).Polish EQ-Bench:70.86.MixEval:Competitive score, methodology correlates highly with Chatbot Arena. Benchmark Score: • Open PL LLM Leaderboard (5-shot):65.71 (outperforms models <70B parameters, competitive with 70B models). Open PL LLM Leaderboard (5-shot): • Polish MT-Bench:8.556250 (outperforms GPT-3.5-turbo). Polish MT-Bench: • Polish EQ-Bench:70.86. Polish EQ-Bench: • MixEval:Competitive score, methodology correlates highly with Chatbot Arena. MixEval: • Data Collection Method by dataset:Hybrid: Automated, Human, Undisclosed Data Collection Method by dataset: • Hybrid: Automated, Human, Undisclosed • Labeling Method by datasetHybrid: Automated, Human, Undisclosed Labeling Method by dataset • Hybrid: Automated, Human, Undisclosed Inference: ========== • Acceleration Engine:vLLM Acceleration Engine: • Test Hardware:NVIDIA Ada Lovelace Test Hardware: • NVIDIA Ada Lovelace Additional content ================== • Quantized Models:Available in GGUF (Q4_K_M, Q5_K_M, Q6_K, Q8_0, and experimental IQ imatrix versions), GPTQ (4bit), and FP8 (for vLLM, SGLang - Ada Lovelace, Hopper optimized). Quantized models may offer lower quality compared to full-sized variants. Quantized Models: • Training Improvements:Weighted tokens level loss.Adaptive learning rate.Masked prompt tokens.DPO-Positive (DPO-P) methodology with multi-turn conversation introduction. Training Improvements: • Weighted tokens level loss. • Adaptive learning rate. • Masked prompt tokens. • DPO-Positive (DPO-P) methodology with multi-turn conversation introduction. • Framework:Trained using an original open-source framework called ALLaMo. Framework: • Contact:SpeakLeash team via Discord or Hugging Face discussion tab. Contact: • Responsible for training the model:SpeakLeash & ACK Cyfronet AGH. Computational grant PLG/2024/016951 on Athena and Helios supercomputers (part of PLGrid environment). Responsible for training the model: Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "kimi-k2-instruct-0905",
    "nvidia_url": "https://build.nvidia.com/moonshotai/kimi-k2-instruct-0905",
    "scraped_on": "2025-10-15T21:38:43.433957",
    "tags": [
      "advanced reasoning",
      "agentic",
      "chat",
      "coding",
      "long-context",
      "moonshotai"
    ],
    "model_card": "Kimi-K2-Instruct-0905 ===================== Description =========== Kimi-K2-Instruct-0905 is the latest, most capable version of Kimi K2, a state-of-the-art Mixture-of-Experts (MoE) language model with 1 trillion total parameters and 32 billion active parameters. It delivers enhanced agentic coding intelligence, improved frontend coding experience, and supports extended context lengths of 256k tokens, enabling long-horizon tasks, tool calling, and chat completion. This model is ready for commercial use. Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIAKimi-K2-Instruct-0905 Model Card. Kimi-K2-Instruct-0905 Model Card (https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905) License and Terms of Use: ========================= GOVERNING TERMS:The trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Open Model License Agreement. Additional Information:Modified MIT License. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Open Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Modified MIT License (https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905/blob/main/LICENSE) Deployment Geography: ===================== Deployment Geography:Global Deployment Geography: Use Case: ========= • Agentic coding intelligence • Real-world coding agent tasks • Frontend programming • Long-horizon tasks • Tool calling • Chat completion • General language understanding and generation Release Date: ============= Build.NVIDIA.com 09/22/2025 vialinkHuggingface 09/05/2024 vialink Build.NVIDIA.com 09/22/2025 vialink link (https://build.nvidia.com/moonshotai/kimi-k2-instruct-0905) Huggingface 09/05/2024 vialink link (https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905) Reference(s): ============= References: • Moonshot AI Platform Moonshot AI Platform (https://platform.moonshot.ai) • Kimi K2 Technical Documentation Kimi K2 Technical Documentation (https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905) Model Architecture: =================== Architecture Type:Mixture-of-Experts (MoE) language modelNetwork Architecture:Transformer-based with MLA attention mechanismTotal Parameters:1T (1 trillion)Active Parameters:32B (32 billion)Vocabulary Size:160KBase Model:Kimi K2 Architecture Type: Network Architecture: Total Parameters: Active Parameters: Vocabulary Size: Base Model: Input: ------ Input Types:TextInput Formats:Natural language prompts, conversational messages, tool calling requestsInput Parameters:[One-Dimensional (1D)]Other Input Properties:Max Input Tokens: 256K, Support for tool calling, chat completion, extended context processingInput Context Length (ISL):256K tokens Input Types: Input Formats: Input Parameters: Other Input Properties: Input Context Length (ISL): Output: ------- Output Type:TextOutput Format:Natural language responses, structured tool calls, code generationOutput Parameters:[One-Dimensional (1D)]Other Output Properties:Max Output Tokens: Configurable, Tool calling capabilities, Code generation, Conversational responsesOutput Context Length (OSL):Configurable based on remaining context Output Type: Output Format: Output Parameters: Other Output Properties: Output Context Length (OSL): Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration: ===================== Runtime Engines:SGLang Runtime Engines: Supported Hardware: • NVIDIA Blackwell/ B200Operating Systems:Linux Operating Systems: Model Version(s) ================ Kimi-K2-Instruct-0905 Training, Testing, and Evaluation Datasets: =========================================== Training Dataset ---------------- Data Modality:TextTraining Data Collection:UndisclosedTraining Labeling:UndisclosedTraining Properties:Pre-trained on large-scale text corpora with mixture-of-experts architecture, enhanced for agentic coding intelligence and tool calling capabilities Data Modality: Training Data Collection: Training Labeling: Training Properties: Testing Dataset --------------- Testing Data Collection:UndisclosedTesting Labeling:UndisclosedTesting Properties:Regular testing on coding benchmarks and agentic intelligence tasks Testing Data Collection: Testing Labeling: Testing Properties: Evaluation Dataset ------------------ Evaluation Benchmark Score:SWE-Bench verified: 69.2 ± 0.63, SWE-Bench Multilingual: 55.9 ± 0.72, Multi-SWE-Bench: 33.5 ± 0.28, Terminal-Bench: 44.5 ± 2.03, SWE-Dev: 66.6 ± 0.72Evaluation Data Collection:UndisclosedEvaluation Labeling:UndisclosedEvaluation Properties:Evaluated on coding benchmarks with mean ± std over five independent runs Evaluation Benchmark Score: Evaluation Data Collection: Evaluation Labeling: Evaluation Properties: Inference ========= Acceleration Engine:SGLangTest Hardware:H100 Acceleration Engine: Test Hardware: Additional Details ================== Key features include: • Enhanced agentic coding intelligence with significant improvements on public benchmarks • Improved frontend coding experience with advancements in aesthetics and practicality • Extended context length from 128k to 256k tokens for better long-horizon task support • Strong tool-calling capabilities with autonomous tool invocation • Mixture-of-experts architecture with 384 experts and 8 selected experts per token • MLA attention mechanism with SwiGLU activation function • Block-fp8 format for efficient storage and deployment • OpenAI/Anthropic-compatible API available on Moonshot AI platform Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report model quality, risk, security vulnerabilities or NVIDIA AI Concernshere here (https://app.intigriti.com/programs/nvidia/nvidiavdp/detail)"
  },
  {
    "name": "qwen3-next-80b-a3b-instruct",
    "nvidia_url": "https://build.nvidia.com/qwen/qwen3-next-80b-a3b-instruct",
    "scraped_on": "2025-10-15T21:38:42.993271",
    "tags": [
      "agentic",
      "chat",
      "qwen",
      "text-generation"
    ],
    "model_card": "Qwen3-Next-80B-A3B-Instruct =========================== Qwen3-Next-80B-A3B-Instruct Description =========== Description Qwen3-Next-80B-A3B-Instruct is a causal language model that is instruction-optimized for chat and agent applications. It features a Mixture-of-Experts (MoE) architecture that achieves an extremely low activation ratio, drastically reducing FLOPs per token while preserving model capacity. The model supports ultra-long contexts and has a Multi-Token Prediction (MTP) mechanism to boost performance and accelerate inference. This model is ready for commercial/non-commercial use. Third-Party Community Consideration: ==================================== Third-Party Community Consideration: This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIA model card here:Qwen3-Next-80B-A3B-Instruct. Qwen3-Next-80B-A3B-Instruct (https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct) License and Terms of Use: ========================= License and Terms of Use: GOVERNING TERMS:The trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Open Model License Agreement. ADDITIONAL INFORMATION:Apache 2.0 License. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Open Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Apache 2.0 License (https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct/blob/main/LICENSE) Deployment Geography: ===================== Deployment Geography: Global Use Case: ========= Use Case: This model is well-suited for task automation, business applications, and agentic use cases. It excels in tool calling capabilities and highly complex reasoning tasks. Release Date: ============= Release Date: Build.NVIDIA.com 09/18/2025 viaQwen3-Next-80B-A3B-InstructHuggingface 09/11/2025 viaQwen3-Next-80B-A3B-Instruct Qwen3-Next-80B-A3B-Instruct (https://build.nvidia.com/qwen/qwen3-next-80b-a3b-instruct) Qwen3-Next-80B-A3B-Instruct (https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct) Reference(s): ============= Reference(s): References: • Qwen3-Next Blog Post Qwen3-Next Blog Post (https://qwen.ai/blog) • Efficient Streaming Language Models with Attention Sinks Efficient Streaming Language Models with Attention Sinks (https://build.nvidia.com/qwen/qwen3-next-80b-a3b-thinking/modelcard) • Massive Activations in Large Language Models Massive Activations in Large Language Models (https://build.nvidia.com/qwen/qwen3-next-80b-a3b-thinking/modelcard) Model Architecture: =================== Model Architecture: Architecture Type:Hybrid Transformer-MambaNetwork Architecture:Qwen3-NextTotal Parameters:80BActive Parameters:3.9BVocabulary Size:151,936 Architecture Type: Network Architecture: Total Parameters: Active Parameters: Vocabulary Size: Input: ------ Input: Input Types:TextInput Formats:StringInput Parameters:One Dimensional (1D)Other Input Properties:Natively supports context lengths of up to 262,144 tokens, extensible to 1 million tokens with YaRN scaling.Input Context Length (ISL):262,144 Input Types: Input Formats: Input Parameters: Other Input Properties: Input Context Length (ISL): Qwen3-Next-80B-A3B-Instructsupports only instruct (non-thinking) mode and does not generate<think></think>blocks in its output. Qwen3-Next-80B-A3B-Instruct Output: ------- Output: Output Types:TextOutput Format:StringOutput Parameters:One Dimensional (1D)Other Output Properties:The model can generate up to 262,144 tokens and recommends an output length of 16,384 tokens for most queries.Output Context Length (OSL):16,384 Output Types: Output Format: Output Parameters: Other Output Properties: Output Context Length (OSL): Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration: ===================== Software Integration: Runtime Engines:SGLang, vLLMSupported Hardware: Runtime Engines: Supported Hardware: • NVIDIA Ampere:A100 NVIDIA Ampere:A100 NVIDIA Ampere: • NVIDIA Blackwell:B200, B100 NVIDIA Blackwell:B200, B100 NVIDIA Blackwell: • NVIDIA Hopper:H100, H200 NVIDIA Hopper:H100, H200 NVIDIA Hopper: Operating Systems:Linux Operating Systems: Model Version(s): ================= Model Version(s): Qwen3-Next-80B-A3B-Instruct v1.0 (September 18, 2025) Training, Testing, and Evaluation Datasets: =========================================== Training, Testing, and Evaluation Datasets: Training Dataset ---------------- Training Dataset Data Modality:TextTraining Data Collection:UndisclosedTraining Labeling:UndisclosedTraining Properties:Undisclosed Data Modality: Training Data Collection: Training Labeling: Training Properties: Testing Dataset --------------- Testing Dataset Testing Data Collection:UndisclosedTesting Labeling:UndisclosedTesting Properties:Undisclosed Testing Data Collection: Testing Labeling: Testing Properties: Evaluation Dataset ------------------ Evaluation Dataset Evaluation Benchmark Score:The model performs on par with Qwen3-235B-A22B-Instruct-2507 on certain benchmarks.Evaluation Data Collection:UndisclosedEvaluation Labeling:UndisclosedEvaluation Properties:For reproducibility, Alibaba reports win rates evaluated by GPT-4.1. Evaluation Benchmark Score: Evaluation Data Collection: Evaluation Labeling: Evaluation Properties: Performance ----------- Knowledge 83.0 93.1 77.5 62.6 Reasoning 70.3 55.4 75.8 Coding 56.6 87.9 57.3 Alignment 88.7 82.7 87.5 87.3 Agent 70.9 71.3 44.0 74.6 50.0 32.5 Multilingualism 77.5 79.4 79.5 50.2 Inference ========= Inference Acceleration Engine:SGLangTest Hardware:NVIDIA H100 Acceleration Engine: Test Hardware: Additional Details ================== Additional Details The Qwen3-Next-80B-A3B-Instruct has a hybrid layout with 48 layers and a 2048 hidden dimension. It uses a multi-token prediction mechanism for faster inference and has a causal language model type. Qwen3-Next-80B-A3B-Instructhas the following features: Qwen3-Next-80B-A3B-Instruct • Type: Causal Language Models • Training Stage: Pretraining (15T tokens) & Post-training • Number of Parameters: 80B in total and 3B activated • Number of Paramaters (Non-Embedding): 79B • Hidden Dimension: 2048 • Number of Layers: 48Hybrid Layout: 12 * (3 * (Gated DeltaNet -> MoE) -> 1 * (Gated Attention -> MoE)) • Hybrid Layout: 12 * (3 * (Gated DeltaNet -> MoE) -> 1 * (Gated Attention -> MoE)) • Gated Attention:Number of Attention Heads: 16 for Q and 2 for KVHead Dimension: 256Rotary Position Embedding Dimension: 64 • Number of Attention Heads: 16 for Q and 2 for KV • Head Dimension: 256 • Rotary Position Embedding Dimension: 64 • Gated DeltaNet:Number of Linear Attention Heads: 32 for V and 16 for QKHead Dimension: 128 • Number of Linear Attention Heads: 32 for V and 16 for QK • Head Dimension: 128 • Mixture of Experts:Number of Experts: 512Number of Activated Experts: 10Number of Shared Experts: 1Expert Intermediate Dimension: 512 • Number of Experts: 512 • Number of Activated Experts: 10 • Number of Shared Experts: 1 • Expert Intermediate Dimension: 512 • Context Length: 262,144 natively and extensible up to 1,010,000 tokens Quickstart ========== The code for Qwen3-Next has been merged into the main branch of Hugging Facetransformers. With earlier versions, you will encounter the following error: The following contains a code snippet illustrating how to use the model generate content based on given inputs. NOTE Multi-Token Prediction (MTP) is not generally available in Hugging Face Transformers. NOTE The efficiency or throughput improvement depends highly on the implementation. It is recommended to adopt a dedicated inference framework, e.g., SGLang and vLLM, for inference tasks. TIP Depending on the inference settings, you may observe better efficiency withflash-linear-attentionandcausal-conv1d. See the links for detailed instructions and requirements. flash-linear-attention (https://github.com/fla-org/flash-linear-attention#installation) causal-conv1d (https://github.com/Dao-AILab/causal-conv1d) Deployment ========== For deployment, you can use the latestsglangorvllmto create an OpenAI-compatible API endpoint. SGLang ------ SGLangis a fast serving framework for large language models and vision language models. SGLang could be used to launch a server with OpenAI-compatible API service. SGLang (https://github.com/sgl-project/sglang) sglang>=0.5.2is required for Qwen3-Next, which can be installed using: Seeits documentationfor more details. its documentation (https://docs.sglang.ai/get_started/install.html) The following command can be used to create an API endpoint athttp://localhost:30000/v1with maximum context length 256K tokens using tensor parallel on 4 GPUs. The following command is recommended for MTP with the rest settings the same as above: NOTE The default context length is 256K. Consider reducing the context length to a smaller value, e.g.,32768, if the server fails to start. Please also refer to SGLang's usage guide onQwen3-Next. Qwen3-Next (https://docs.sglang.ai/basic_usage/qwen3.html) vLLM ---- vLLMis a high-throughput and memory-efficient inference and serving engine for LLMs. vLLM could be used to launch a server with OpenAI-compatible API service. vLLM (https://github.com/vllm-project/vllm) vllm>=0.10.2is required for Qwen3-Next, which can be installed using: Seeits documentationfor more details. its documentation (https://docs.vllm.ai/en/stable/getting_started/installation/index.html) The following command can be used to create an API endpoint athttp://localhost:8000/v1with maximum context length 256K tokens using tensor parallel on 4 GPUs. The following command is recommended for MTP with the rest settings the same as above: NOTE The default context length is 256K. Consider reducing the context length to a smaller value, e.g.,32768, if the server fails to start. Please also refer to vLLM's usage guide onQwen3-Next. Qwen3-Next (https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-Next.html) Agentic Use =========== Qwen3 excels in tool calling capabilities. We recommend usingQwen-Agentto make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity. Qwen-Agent (https://github.com/QwenLM/Qwen-Agent) To define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself. Processing Ultra-Long Texts =========================== Qwen3-Next natively supports context lengths of up to 262,144 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 1 million tokens using theYaRNmethod. YaRN (https://arxiv.org/abs/2309.00071) YaRN is currently supported by several inference frameworks, e.g.,transformers,vllmandsglang. In general, there are two approaches to enabling YaRN for supported frameworks: • Modifying the model files: In theconfig.jsonfile, add therope_scalingfields:{...,\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}} Modifying the model files: In theconfig.jsonfile, add therope_scalingfields: • Passing command line arguments:Forvllm, you can useVLLM_ALLOW_LONG_MAX_MODEL_LEN=1vllm serve... --rope-scaling'{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}'--max-model-len1010000Forsglang, you can useSGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1python-msglang.launch_server... --json-model-override-args'{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}}'--context-length1010000 Passing command line arguments: Forvllm, you can use Forsglang, you can use NOTE All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length,potentially impacting performance on shorter texts.We advise adding therope_scalingconfiguration only when processing long contexts is required. It is also recommended to modify thefactoras needed. For example, if the typical context length for your application is 524,288 tokens, it would be better to setfactoras 2.0. potentially impacting performance on shorter texts. Long-Context Performance ------------------------ We test the model on an 1M version of theRULERbenchmark. RULER (https://arxiv.org/abs/2404.06654) • Qwen3-Next are evaluated with YaRN enabled. Qwen3-2507 models are evaluated with Dual Chunk Attention enabled. • Since the evaluation is time-consuming, we use 260 samples for each length (13 sub-tasks, 20 samples for each). Best Practices ============== To achieve optimal performance, we recommend the following settings: • Sampling Parameters:We suggest usingTemperature=0.7,TopP=0.8,TopK=20, andMinP=0.For supported frameworks, you can adjust thepresence_penaltyparameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance. Sampling Parameters: Sampling Parameters • We suggest usingTemperature=0.7,TopP=0.8,TopK=20, andMinP=0. • For supported frameworks, you can adjust thepresence_penaltyparameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance. • Adequate Output Length: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models. Adequate Output Length: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models. Adequate Output Length • Standardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.Math Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.Multiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in theanswerfield with only the choice letter, e.g.,\"answer\": \"C\".\" Standardize Output Format: We recommend using prompts to standardize model outputs when benchmarking. Standardize Output Format • Math Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt. Math Problems • Multiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in theanswerfield with only the choice letter, e.g.,\"answer\": \"C\".\" Multiple-Choice Questions Ethical Considerations ====================== Ethical Considerations NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report model quality, risk, security vulnerabilities or NVIDIA AI Concernshere here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "stockmark-2-100b-instruct",
    "nvidia_url": "https://build.nvidia.com/stockmark/stockmark-2-100b-instruct",
    "scraped_on": "2025-10-15T21:38:42.644026",
    "tags": [
      "chat",
      "japanese",
      "large language model",
      "sovereign ai",
      "stockmark"
    ],
    "model_card": "Stockmark-2-100B-Instruct ========================= Description =========== Stockmark-2-100B-Instruct is a 100-billion-parameter large language model built from scratch, with a particular focus on Japanese. It was pre-trained on approximately 2.0 trillion tokens of data, consisting of 60% English, 30% Japanese, and 10% code. Following pretraining, the model underwent post-training (SFT and DPO) with synthetic data in Japanese to enhance its ability to follow instructions. This version improves instruction-following ability and adds support for long-context (32k), compared to the previous version. This model is ready for commercial and non-commercial use Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIAStockmark-2-100B-Instruct Model Card. Stockmark-2-100B-Instruct Model Card (https://huggingface.co/stockmark/Stockmark-2-100B-Instruct) License and Terms of Use: ========================= GOVERNING TERMS:The trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Open Model License Agreement. Additional Information:MIT License. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Open Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) MIT License (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/mit.md) Deployment Geography: ===================== Deployment Geography:Global Deployment Geography: Use Case: ========= Use Case:Japanese and English language processing, instruction following, long-context understanding, research and commercial applications. Use Case: Release Date: ============= build.nvidia.com 09/24/2025 vialinkHuggingface 09/24/2025 vialink build.nvidia.com 09/24/2025 vialink link (https://build.nvidia.com/stockmark/stockmark-2-100b-instruct) Huggingface 09/24/2025 vialink link (https://huggingface.co/stockmark/Stockmark-2-100B-Instruct) Reference(s): ============= Stockmark Inc.GENIAC Stockmark Inc. (https://stockmark.co.jp/) GENIAC (https://www.meti.go.jp/policy/mono_info_service/geniac/index.html) Model Architecture: =================== Architecture Type:Causal Language ModelNetwork Architecture:Transformer-based with Grouped Query Attention (GQA)Total Parameters:96BActive Parameters:96BVocabulary Size:100352 Architecture Type: Network Architecture: Total Parameters: Active Parameters: Vocabulary Size: Input: ------ Input Types:TextInput Parameters:[One-Dimensional (1D)]Other Input Properties:Supports Japanese and English languagesInput Context Length (ISL):32,000 tokens Input Types: Input Parameters: Other Input Properties: Input Context Length (ISL): Output: ------- Output Type:TextOutput Parameters:[One-Dimensional (1D)]Other Output Properties:Instruction-following responses in Japanese and EnglishOutput Context Length (OSL):Up to 32,768 tokens (shared with input) Output Type: Output Parameters: Other Output Properties: Output Context Length (OSL): Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration: ===================== Runtime Engines:PyTorch, transformers, vLLMSupported Hardware: Runtime Engines: Supported Hardware: • NVIDIA Ada Lovelace • NVIDIA Ampere • NVIDIA Blackwell • NVIDIA HopperOperating Systems:Linux, Windows, macOS Operating Systems: Model Version(s) ================ Stockmark-2-100B-Instruct Training, Testing, and Evaluation Datasets: =========================================== Training Dataset ---------------- Training Data Collection:SyntheticTraining Labeling:SyntheticData Modality:TextText Training Data Size:1 Billion to 10 Trillion TokensTraining Properties:Post-training with SFT and DPO methods Training Data Collection: Training Labeling: Data Modality: Text Training Data Size: Training Properties: Testing Dataset --------------- Testing Data Collection:UndisclosedTesting Labeling:UndisclosedTesting Properties:Undisclosed Testing Data Collection: Testing Labeling: Testing Properties: Evaluation Dataset ------------------ Evaluation Benchmark Score:Japanese MT-bench Average: 7.87Evaluation Data Collection:Japanese MT-bench evaluation`Evaluation Labeling:Automated scoring systemEvaluation Properties:Multi-domain evaluation including coding, extraction, humanities, math, reasoning, roleplay, and STEM Evaluation Benchmark Score: Evaluation Data Collection: Evaluation Labeling: Evaluation Properties: Inference ========= Acceleration Engine:vLLM, TensorRT-LLMTest Hardware:4*H100 Acceleration Engine: Test Hardware: Additional Information ====================== This project was supported by GENIAC. The model uses Grouped Query Attention (GQA) with 72 query heads and 8 key-value heads. Training libraries include NVIDIA/Megatron-LM for pretraining and huggingface/trl for posttraining. Limitations =========== This model should be used responsibly and in accordance with applicable laws and regulations. Users should be aware of potential biases in the training data and outputs, particularly when processing content in Japanese and English languages. Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report model quality, risk, security vulnerabilities or NVIDIA AI Concernshere. here (nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "llama-3_2-nemoretriever-300m-embed-v2",
    "nvidia_url": "https://build.nvidia.com/nvidia/llama-3_2-nemoretriever-300m-embed-v2",
    "scraped_on": "2025-10-15T21:38:39.394756",
    "tags": [
      "nemo retriever",
      "nvidia",
      "retrieval augmented generation",
      "text-to-embedding"
    ],
    "model_card": "Model Overview ============== Model Overview Description ----------- Description The Llama 3.2 NeMo Retriever Embedding 300M model version 2 is optimized formultilingual and cross-lingualtext question-answering retrieval withsupport for long documents (up to 8192 tokens). This model was evaluated on 26 languages: English, Arabic, Bengali, Chinese, Czech, Danish, Dutch, Finnish, French, German, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, and Turkish. multilingual and cross-lingual support for long documents (up to 8192 tokens) In addition to enabling multilingual and cross-lingual question-answering retrieval, this model reduces the data storage footprint through dynamic embedding sizing and support for longer token length, making it feasible to handle large-scale datasets efficiently. An embedding model is a crucial component of a text retrieval system, as it transforms textual information into dense vector representations. They are typically transformer encoders that process tokens of input text (for example: question, passage) to output an embedding. The Llama 3.2 NeMo Retriever Embedding 300M model version 2 is a part of the NVIDIA NeMo Retriever collection of NIMs, which provide state-of-the-art, commercially-ready models and microservices, optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can also readily customize them for domain-specific use cases, such as information technology, human resource help assistants, and research & development research assistants. This model is ready for commercial use. License/Terms of use -------------------- License/Terms of use GOVERNING TERMS:The trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. ADDITIONAL INFORMATION:Llama 3.2 Community License Agreement. Built with Llama. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Llama 3.2 Community License Agreement (https://www.llama.com/llama3_2/license/) ADDITIONAL INFORMATION:Llama 3.2 Community License Agreement. Built with Llama. Llama 3.2 Community License Agreement (https://www.llama.com/llama3_2/license/) You are responsible for ensuring that your use of NVIDIA AI Foundation Models complies with all applicable laws. Deployment Geography: --------------------- Global Use Case: --------- The Llama 3.2 NeMo Retriever Embedding 300M model version 2 is most suitable for users who want to build a multilingual question-and-answer application over a large text corpus, leveraging the latest dense retrieval technologies. Release Date: ------------- Build.NVIDIA.com:9/29/2025 vialink Build.NVIDIA.com: link (https://build.nvidia.com/nvidia/llama-3_2-nemoretriever-300m-embed-v2) Model Architecture ------------------ Model Architecture Architecture Type:TransformerNetwork Architecture:Fine-tuned Llama3.2 300M Retriever Architecture Type: Network Architecture: This NeMo Retriever embedding model is a transformer encoder with 9 layers and an embedding size of 2048 and has been pruned, distilled from Llama 3.2-nv-embedqa-1b-v1 model. After pruning and distillation, the model has been trained on public and synthetic datasets. The AdamW optimizer is employed incorporating 100 warm up steps and 5e-6 learning rate with WarmupDecayLR scheduler. Embedding models for text retrieval are typically trained using a bi-encoder architecture. This involves encoding a pair of sentences (for example, query and chunked passages) independently using the embedding model. Contrastive learning is used to maximize the similarity between the query and the passage that contains the answer, while minimizing the similarity between the query and sampled negative passages not useful to answer the question. Computational Load: ------------------- Cumulative Compute: 6.67E+22 Estimated Energy and Emissions for Model Training: 259,500kWh | 107 tons CO2eq This model's cumulative compute is dominated by the llama3.2-1b model training; estimates on the base model's compute and energy/emissions usage is sourced fromepoch.aiand thellama3.2-1b model card. epoch.ai (https://epoch.ai/data/ai-models?view=table#explore-the-data) llama3.2-1b model card (https://huggingface.co/meta-llama/Llama-3.2-1B) Input ----- Input Output ------ Output Output Type:FloatsOutput Format:List of floatsOutput Parameters:1DOther Properties Related to Output:Model outputs embedding vectors of maximum dimension 2048 for each text string (can be configured based on 384, 512, 768, 1024, or 2048). Output Type: Output Format: Output Parameters: Other Properties Related to Output: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (such as GPU cores) and software frameworks (such as CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration -------------------- Software Integration Runtime Engine:NeMo Retriever embedding NIMSupported Hardware Microarchitecture Compatibility: NVIDIA Ampere, NVIDIA Blackwell, NVIDIA Hopper, NVIDIA LovelaceSupported Operating System(s):Linux Runtime Engine: Supported Hardware Microarchitecture Compatibility Supported Operating System(s): Model Version(s) ---------------- Model Version(s) Llama 3.2 NeMo Retriever Embedding 300M v2Short Name:llama-3.2-nemoretriever-300m-embed-v2 Training Dataset & Evaluation ============================= Training Dataset & Evaluation Training Dataset ---------------- Training Dataset The development of large-scale public open-QA datasets has enabled tremendous progress in powerful embedding models. However, one popular dataset named MS MARCO restricts ‌commercial licensing, limiting the use of these models in commercial settings. To address this, NVIDIA created its own training dataset blend based on public QA datasets, which each have a license for commercial applications as well as synthetic QA datasets which were created using Llama 3.1 70b instruct. For long context retrieval, synthetic datasets were created using the same methodology as the MLDR train datasets (https://huggingface.co/datasets/Shitao/MLDR). https://huggingface.co/datasets/Shitao/MLDR Data Collection Method by dataset: Hybrid: Automated, Human, SyntheticLabeling Method by dataset: Hybrid: Automated, Human, SyntheticProperties:Semi-supervised pre-training on 12M samples from public datasets and fine-tuning on 1M samples from public and synthetic datasets . Data Collection Method by dataset Labeling Method by dataset Properties: Evaluation Datasets ------------------- Evaluation Datasets We evaluated the NeMo Retriever embedding model in comparison to literature open & commercial retriever models on academic benchmarks for question-answering -NQ,HotpotQAandFiQA (Finance Q&A)from BeIR benchmark and TechQA dataset. Note that the model was evaluated offline on A100 GPUs using the model's PyTorch checkpoint. In this benchmark, the metric used was Recall@5. NQ (https://huggingface.co/datasets/BeIR/nq) HotpotQA (https://huggingface.co/datasets/hotpotqa/hotpot_qa) FiQA (Finance Q&A) (https://huggingface.co/datasets/BeIR/fiqa) We also evaluated the multilingual capabilities on the academic benchmarkMIRACLacross 15 languages and translated the English and Spanish version of MIRACL into additional 11 languages. The reported scores are based on an internal version of MIRACL by selecting hard negatives for each query to reduce the corpus size. MIRACL (https://github.com/project-miracl/miracl) We evaluated the capabilities on the academic benchmarkMLQAbased on 7 languages (Arabic, Chinese, English, German, Hindi, Spanish, Vietnamese). We consider only evaluation datasets when the query and documents are in same languages. MLQA (https://github.com/facebookresearch/MLQA/) We evaluated the support of long documents on the academic benchmarkMultilingual Long-Document Retrieval (MLDR)built on Wikipedia and mC4, covering 12 typologically diverse languages. The English version has a median length of 2399 tokens and 90th percentile of 7483 tokens using the llama 3.2 tokenizer. The MLDR dataset is based on synthetic generated questions with a LLM, which has the tendency to create questions with similar keywords than the positive document, but might not be representative for real user queries. This characteristic of the dataset benefits sparse embeddings like BM25. Multilingual Long-Document Retrieval (MLDR) (https://huggingface.co/datasets/Shitao/MLDR) Data Collection Method by dataset: Hybrid: Automated, Human, SyntheticLabeling Method by dataset: Hybrid: Automated, Human, SyntheticProperties:The evaluation datasets are based onMTEB/BEIR, TextQA, TechQA,MIRACL,MLQA, andMLDR. The size ranges between 10,000s up to 5M depending on the dataset. Data Collection Method by dataset Labeling Method by dataset Properties: MTEB/BEIR (https://github.com/beir-cellar/beir) MIRACL (https://github.com/project-miracl/miracl) MLQA (https://github.com/facebookresearch/MLQA) MLDR (https://huggingface.co/datasets/Shitao/MLDR) Evaluation Results ------------------ Evaluation Results InferenceEngine:TensorRTTest Hardware:L40s Inference Engine: Test Hardware: Ethical Considerations ====================== Ethical Considerations NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case, and address unforeseen product misuse. For more detailed information on ethical considerations for this model, see the Model Card++ tab for the Explainability, Bias, Safety & Security, and Privacy subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/) Get Help ======== Enterprise Support ------------------ Get access to knowledge base articles and support cases or submit a ticket at theNVIDIA AI Enterprise Support Services page.. NVIDIA AI Enterprise Support Services page. (https://www.nvidia.com/en-us/data-center/products/ai-enterprise-suite/support/) NVIDIA NIM Documentation ------------------------ Visit theNeMo Retriever docs pagefor release documentation, deployment guides and more. NeMo Retriever docs page (https://docs.nvidia.com/nemo/retriever/index.html)"
  },
  {
    "name": "deepseek-v3.1-terminus",
    "nvidia_url": "https://build.nvidia.com/deepseek-ai/deepseek-v3_1-terminus",
    "scraped_on": "2025-10-15T21:38:38.948639",
    "tags": [
      "advanced reasoning",
      "agentic",
      "chat",
      "deepseek-ai",
      "tool calling"
    ],
    "model_card": "DeepSeek-V3.1-Terminus Overview =============================== Model Summary ============= Model Summary DeepSeek-V3.1-Terminus is an updated checkpoint of the DeepSeek-V3 family and refines model stability, multilingual consistency, and agent behavior. It improves agent and code/search capabilities while addressing mixed-language and character issues seen in earlier checkpoints. This model is ready for commercial/non-commercial use. Third-Party Community Consideration:This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link toNon-NVIDIA DeepSeek-V3.1-Terminus Model Card. Third-Party Community Consideration: Non-NVIDIA DeepSeek-V3.1-Terminus Model Card (https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus) GOVERNING TERMS:The trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Open Model License Agreement. Additional Information:MIT. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Open Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) MIT (https://choosealicense.com/licenses/mit/) Deployment Geography: --------------------- Global Use Case: --------- Reasoning, search/agent workflows, code assistance, multilingual QA, and knowledge retrieval for research and enterprise applications. Release Date: ------------- build.nvidia.com 10/06/2025 vialinkHuggingface 08/21/2025 vialink link (https://build.nvidia.com/deepseek-ai/deepseek-v3_1-terminus) link (https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus) Reference(s): ------------- References:Link Link (https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus) Model Architecture ================== Model Architecture Architecture Type:TransformerNetwork Architecture:DeepSeek-V3 Mixture-of-Experts variant with MLATotal Parameters:~685BBase Model:DeepSeek-V3 family Architecture Type: Network Architecture: Total Parameters: Base Model: Input ===== Input Input Types:TextInput Formats:StringInput Parameters:One Dimensional (1D)Other Input Properties:Undisclosed Input Types: Input Formats: Input Parameters: Other Input Properties: Output ====== Output Output Types:TextOutput Format:StringOutput Parameters:One Dimensional (1D)Other Output Properties:Undisclosed Output Types: Output Format: Output Parameters: Other Output Properties: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration ==================== Software Integration Runtime Engines:SGLang Runtime Engines: Supported Hardware Microarchitecture Compatibility:NVIDIA BlackwellNVIDIA Hopper Supported Hardware Microarchitecture Compatibility: Operating Systems:LinuxThe integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment. Operating Systems: Model Version(s) ================ Model Version(s) Model Version(s)DeepSeek-V3.1-Terminus Model Version(s) Training, Testing, and Evaluation Datasets ========================================== Training, Testing, and Evaluation Datasets Training Dataset ---------------- Training Data Collection:UndisclosedTraining Labeling:UndisclosedTraining Properties:UndisclosedData Modality:TextText Training Data Size:[1 Billion to 10 Trillion Tokens] Training Data Collection: Training Labeling: Training Properties: Data Modality: Text Training Data Size: Testing Dataset --------------- Testing Data Collection:UndisclosedTesting Labeling:UndisclosedTesting Properties:Undisclosed Testing Data Collection: Testing Labeling: Testing Properties: Evaluation Dataset ------------------ Evaluation Benchmark Score:UndisclosedEvaluation Data Collection:UndisclosedEvaluation Labeling:UndisclosedEvaluation Properties:Undisclosed Evaluation Benchmark Score: Evaluation Data Collection: Evaluation Labeling: Evaluation Properties: Inference ========= Inference Acceleration Engine:SGLangTest Hardware:B200 Acceleration Engine: Test Hardware: Ethical Considerations ====================== Ethical Considerations NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report model quality, risk, security vulnerabilities or NVIDIA AI Concernshere here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "gpt-oss-20b",
    "nvidia_url": "https://build.nvidia.com/openai/gpt-oss-20b",
    "scraped_on": "2025-10-15T21:38:38.532136",
    "tags": [
      "chat",
      "math",
      "reasoning",
      "text-to-text"
    ],
    "model_card": "GPT OSS 20B Overview ==================== Description: ============ OpenAI releases the gpt-oss family of open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases. The family consists of the: • gpt-oss-120b— for production, general purpose, high reasoning use-cases that fits into a single H100 GPU (117B parameters with 5.1B active parameters) • gpt-oss-20b— for lower latency, and local or specialized use-cases (21B parameters with 3.6B active parameters). Thegpt-oss-20bis designed as a Mixture-of-Experts (MoE) model, structurally identical to the larger 117B variant, albeit with different hyperparameters. This model leverages SwiGLU activations and incorporates learned attention sinks within its architecture. Functionally, it serves as a robust reasoning model, supporting advanced capabilities such as chain-of-thought processing, adjustable reasoning effort levels, instruction following, and tool use. It operates strictly with text-only modalities for both input and output. A key strategic benefit is its suitability for enterprises and governments, facilitating on-premises or private cloud deployment to ensure enhanced data security and privacy. Model Highlights: • Permissive Apache 2.0 license:Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment. Permissive Apache 2.0 license: • Configurable reasoning effort:Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs. Configurable reasoning effort: • Full chain-of-thought:Gain complete access to the model's reasoning process, facilitating easier debugging and increased trust in outputs. It's not intended to be shown to end users. Full chain-of-thought: • Fine-tunable:Fully customize models to your specific use case through parameter fine-tuning. Fine-tunable: • Agentic capabilities:Use the models' native capabilities for function calling, web browsing, python code execution, and structured outputs. Agentic capabilities: This model is ready for commercial/non-commercial use. Third-Party Community Consideration =================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to Non-NVIDIAgpt-oss-20b model card. gpt-oss-20b model card (https://huggingface.co/openai/gpt-oss-20b) License and Terms of Use: ------------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information:Apache License Version 2.0. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Apache License Version 2.0 (https://www.apache.org/licenses/LICENSE-2.0) Deployment Geography: --------------------- Global Use Case: --------- Intended for use as a reasoning model, offering features like chain-of-thought and adjustable reasoning effort levels. It provides comprehensive support for instruction following and tool use, fostering transparency, customization, and deployment flexibility for developers, researchers, and startups. Crucially, it enables enterprises and governments to deploy on-premises or in private clouds, ensuring stringent data security and privacy requirements are met. Release Date: ------------- Build.NVIDIA.com - 08/05/2025 vialinkHugging Face - 08/05/2025 vialink link (https://build.nvidia.com/openai/gpt-oss-20b) link (https://huggingface.co/openai/gpt-oss-20b) Reference(s): ============= • OpenAI Cookbook OpenAI Cookbook (https://cookbook.openai.com/) • Optimizing OpenAI GPT-OSS Models with NVIDIA TensorRT-LLM Optimizing OpenAI GPT-OSS Models with NVIDIA TensorRT-LLM (https://github.com/openai/openai-cookbook/blob/main/articles/run-nvidia.ipynb) • Running a High Performance GPT-OSS-120B Inference Server with TensorRT-LLM Running a High Performance GPT-OSS-120B Inference Server with TensorRT-LLM (https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/tech_blog/blog9_Deploying_GPT_OSS_on_TRTLLM.md) Model Architecture: =================== Architecture Type:TransformerNetwork Architecture:Mixture-of-Experts (MoE)Total Parameters:20BActive Parameters:4BVocabulary Size:201,088 (Utilizes the standard tokenizer used by GPT-4o) Architecture Type: Network Architecture: Total Parameters: Active Parameters: Vocabulary Size: Input: ====== Input Type(s):TextInput Format(s):StringInput Parameters:One Dimensional (1D)Other Properties Related to Input:Uses RoPE with a 128k context length, with attention layers alternating between full context and a sliding 128-token window. Includes a learned attention sink per-head. Employs SwiGLU activations in the MoE layers, and the router performs a Top-K operation (K=4) followed by a Sigmoid function. GEMMs in the MoE include a per-expert bias. Utilizes tiktoken for tokenization. Input Context Length (ISL): 128000 Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output: ======= Output Type(s):TextOutput Format:StringOutput Parameters:One Dimensional (1D)Other Properties Related to Output:The model is architected to be compatible with the OpenAI Responses API and supports Structured Output, aligning with key partner expectations for advanced response formatting. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems [or name equivalent hardware preference]. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration: ===================== Runtime Engine(s): • NeMo Framework (based on 25.07) Supported Hardware Microarchitecture Compatibility: • NVIDIA Blackwell: B200, GB200 • NVIDIA Hopper: H200 Operating System(s):Linux Operating System(s): Model Version(s): ================= gpt-oss-20bv1.0 (August 5, 2025) Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ----------------- • Training Data Collection:Undisclosed Training Data Collection: • Training Labeling:Undisclosed Training Labeling: • Training Properties:The gpt-oss-20b model has approximately 20 billion total parameters, with approximately 4 billion active parameters per inference. The weights for all layers are in BF16, except for the MoE projection weights, which are in MXFP4. The reference implementation, for initial accuracy validation, currently upcasts all weights to BF16. Activations are expected to be in BF16 or FP8. Training Properties: Testing Dataset: ---------------- • Testing Data Collection:Undisclosed Testing Data Collection: • Testing Labeling:Undisclosed Testing Labeling: • Testing Properties:The model's performance is tested against recognized benchmarks such as MMLU (Massive Multitask Language Understanding) and GPQA (General Purpose Question Answering), alongside other benchmarks including LiveCodeBench, AIME 2024, and MATH-500 Testing Properties: Evaluation Dataset: ------------------- • Evaluation Data Collection:Undisclosed Evaluation Data Collection: • Evaluation Labeling:Undisclosed Evaluation Labeling: • Evaluation Benchmark Score: Evaluation Benchmark Score: Above scores were measured for the high reasoning level. Safety Results: --------------- The following evaluations check that the model does not comply with requests for content that is disallowed under OpenAI’s safety policies, including hateful content or illicit advice. Inference: ========== Acceleration Engine:vLLMTest Hardware:NVIDIA Hopper (H200) Acceleration Engine: Test Hardware: Additional Details ================== The model is released with the native quantization support. Specifically,MXFP4is used for the linear projection weights in the MoE layer. It is stored the MoE tensor in two parts: MXFP4 (https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf) • tensor.blocksstores the actual fp4 values. Every two values are packed in oneuint8value. • tensor.scalesstores the block scale. The block scaling is done among the last dimension for all MXFP4 tensors. All other tensors are stored in BF16. It is recommended to use BF16 as the activation precision for the model. Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nemoretriever-ocr-v1",
    "nvidia_url": "https://build.nvidia.com/nvidia/nemoretriever-ocr-v1",
    "scraped_on": "2025-10-15T21:38:35.410405",
    "tags": [
      "data ingestion",
      "table extraction"
    ],
    "model_card": "NeMo Retriever OCR v1 Overview =============================== Description =========== The NeMo Retriever OCR v1 model is a state-of-the-art text recognition model designed for robust end-to-end optical character recognition (OCR) on complex real-world images. It integrates three core neural network modules: a detector for text region localization, a recognizer for transcription of detected regions, and a relational model for layout and structure analysis. This model is optimized for a wide variety of OCR tasks, including multi-line, multi-block, and natural scene text, and supports advanced reading order analysis via its relational model component. NeMo Retriever OCR v1 has been developed to be production-ready and commercially usable, with a focus on speed and accuracy on both document and natural scene images. The NeMo Retriever OCR v1 model is part of the NVIDIA NeMo Retriever collection of NIM, which provides state-of-the-art, commercially-ready models and microservices optimized for the lowest latency and highest throughput. It features a production-ready information retrieval pipeline with enterprise support. The models that form the core of this solution have been trained using responsibly selected, auditable data sources. With multiple pre-trained models available as starting points, developers can readily customize them for domain-specific use cases, such as information technology, human resource help assistants, and research & development research assistants. This model is ready for commercial use. License/Terms of Use -------------------- GOVERNING TERMS:The trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Software and Model Evaluation License. GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Software and Model Evaluation License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-and-model-evaluation-license/) Deployment Geography -------------------- Global Use Case -------- The NeMo Retriever OCR v1 model is designed for high-accuracy and high-speed extraction of textual information from images, making it ideal for powering multimodal retrieval systems, Retrieval-Augmented Generation (RAG) pipelines, and agentic applications that require seamless integration of visual and language understanding. Its robust performance and efficiency make it an excellent choice for next-generation AI systems that demand both precision and scalability across diverse real-world content. Release Date ------------ Build.NVIDIA.com:2025-07 vialink Build.NVIDIA.com: link (https://build.nvidia.com/nvidia/nemoretriever-ocr-v1) Model Architecture ================== Architecture Type:Hybrid detector–recognizer with document-level relational modeling Architecture Type: The NeMo Retriever OCR v1 model integrates three specialized neural components: • Text Detector:Utilizes a RegNetY-8GF convolutional backbone for high-accuracy localization of text regions within images. Text Detector: • Text Recognizer:Employs a Transformer-based sequence recognizer to transcribe text from detected regions, supporting variable word and line lengths. Text Recognizer: • Relational Model:Applies a multi-layer global relational module to predict logical groupings, reading order, and layout relationships across detected text elements. Relational Model: All components are trained jointly in an end-to-end fashion, providing robust, scalable, and production-ready OCR for diverse document and scene images. Parameter Counts: Total Input ===== Input Type(s):Image (RGB, PNG/JPEG, float32/uint8), aggregation level (word, sentence, or paragraph)Input Format(s):List of stringsInput Parameters:3 x H x W (single image) or B x 3 x H x W (batch)Input Range:[0, 1] (float32) or [0, 255] (uint8, auto-converted)Other Properties Related to Input:Handles both single images and batches. Automatic multi-scale resizing for best accuracy. Input Type(s): Input Format(s): Input Parameters: Input Range: Other Properties Related to Input: Output ====== Output Type(s): Structured OCR results: a list of detected text regions (bounding boxes), recognized text, and confidence scoresOutput Format(s): Bounding boxes: tuple of floats, recognized text: string, confidence score: float** Output parameters**: Bounding boxes: 1D list of bounding box coordinates, recognized text: 1D list of strings, confidence score: 1D list of floatsOther Properties Related to Output:Please see the sample output for an example of the model output. Output Type(s) Output Format(s) Other Properties Related to Output: Sample output ------------- Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (such as GPU cores) and software frameworks (such as CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration ==================== Runtime Engine: PyTorch Supported Hardware Microarchitecture Compatibility -------------------------------------------------- • NVIDIA Ampere • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace Preferred Operating System(s) ----------------------------- Linux Model Version(s) ---------------- NeMo Retriever OCR v1 Training, Testing, and Evaluation Datasets ========================================== Training Dataset ---------------- The model is trained on a large-scale, curated mix of public and proprietary OCR datasets, focusing on high diversity of document layouts and scene images. The training set includes synthetic and real images with varied noise and backgrounds, filtered for commercial use eligibility. • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset: • Properties:Includes scanned documents, natural scene images, receipts, and business documents. Properties: Testing Dataset --------------- • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset: • Properties:Includes scanned documents, natural scene images, receipts, and business documents. Properties: Evaluation Dataset ------------------ The NeMo Retriever OCR v1 model is evaluated on several NVIDIA internal datasets for various tasks, such as pure OCR, table content extraction, and document retrieval. • Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset:Hybrid: Automated, Human, Synthetic Data Collection Method by dataset: • Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset:Hybrid: Automated, Human, Synthetic Labeling Method by dataset: • Properties:Benchmarks include challenging scene images, documents with varied layouts, and multi-language data. Properties:Benchmarks include challenging scene images, documents with varied layouts, and multi-language data. Properties: Benchmark Results ----------------- We benchmarked NeMo Retriever OCR v1 on internal evaluation datasets against PaddleOCR on various tasks, such as pure OCR (Character Error Rate), table content extraction (TEDS), and document retrieval (Recall@5). Detailed Performance Analysis ----------------------------- Detailed Performance Analysis The model demonstrates robust performance on complex layouts, noisy backgrounds, and challenging real-world scenes. Reading order and block detection are powered by the relational module, supporting downstream applications such as chart-to-text, table-to-text, and infographic-to-text extraction. Inference --------- Acceleration Engine: PyTorchTest Hardware: • L40s Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "cosmos-reason1-7b",
    "nvidia_url": "https://build.nvidia.com/nvidia/cosmos-reason1-7b",
    "scraped_on": "2025-10-15T21:38:32.115501",
    "tags": [
      "autonomous vehicles",
      "physical ai"
    ],
    "model_card": "Cosmos-Reason1-7B Overview ========================== Description =========== NVIDIA Cosmos Reason – an open, customizable, 7B-parameter reasoning vision language model (VLM) for physical AI and robotics - enables robots and vision AI agents to reason like humans, using prior knowledge, physics understanding and common sense to understand and act in the real world. This model understands space, time, and fundamental physics, and can serve as a planning model to reason what steps an embodied agent might take next. Cosmos Reason excels at navigating the long tail of diverse scenarios of the physical world with spatial-temporal understanding. Cosmos Reason is post-trained with physical common sense and embodied reasoning data with supervised fine-tuning and reinforcement learning. It uses chain-of-thought reasoning capabilities to understand world dynamics without human annotations. Given a video and a text prompt, the model first converts the video into tokens using a vision encoder and a special translator called a projector. These video tokens are combined with the text prompt and fed into the core model, which uses a mix of LLM modules and techniques. This enables the model to think step-by-step and provide detailed, logical responses. Cosmos Reason can be used for robotics and physical AI applications including: Data curation and annotation — Enable developers to automate high-quality curation and annotation of massive, diverse training datasets. Robot planning and reasoning — Act as the brain for deliberate, methodical decision-making in a robot vision language action (VLA) model. Now robots such as humanoids and autonomous vehicles can interpret environments and given complex commands, break them down into tasks and execute them using common sense, even in unfamiliar environments. Video analytics AI agents — Extract valuable insights and perform root-cause analysis on massive volumes of video data. These agents can be used to analyze and understand recorded or live video streams across city and industrial operations. The model is ready for commercial use. Model Developer:NVIDIA Model Developer: License and Terms of Use: ========================= GOVERNING TERMS:This trial service is governed by theNVIDIA API Trial Terms of Service. Use of the model is governed by theNVIDIA Open Model License Agreement.Additional Information:Apache License 2.0 GOVERNING TERMS: NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Open Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Apache License 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) Models are commercially usable. You are free to create and distribute Derivative Models. NVIDIA does not claim ownership to any outputs generated using the Models or Derivative Models. Important Note: If You bypass, disable, reduce the efficacy of, or circumvent any technical limitation, safety guardrail or associated safety guardrail hyperparameter, encryption, security, digital rights management, or authentication mechanism (collectively “Guardrail”) contained in the Model without a substantially similar Guardrail appropriate for your use case, your rights under this AgreementNVIDIA Open Model Licensewill automatically terminate. Important Note NVIDIA Open Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license) Deployment Geography: ===================== Global Use Case: ========= Robotics engineers and AI researchers developing embodied agents—such as autonomous vehicles and robotic systems—would use Physical AI to equip their machines with spatiotemporal reasoning and a fundamental physics understanding for navigation, manipulation, and decision-making tasks Reference:Technical Paper ------------------------- Technical Paper (https://arxiv.org/pdf/2503.15558) Release Date: ============= Build.NVIDIA.com08/11/2025 vialink Build.NVIDIA.com link (https://build.nvidia.com/nvidia/cosmos-reason1-7b) Huggingface08/01/2025 vialink Huggingface link (https://huggingface.co/nvidia/Cosmos-Reason1-7B/commit/0caf724f837efea5e25bf6d5818dcdeec0a36604) Model Architecture: =================== Architecture Type:A Multi-modal LLM consists of a Vision Transformer (ViT) for vision encoder and a Dense Transformer model for LLM. Architecture Type: Network Architecture:Qwen2.5-VL-7B-Instruct. Network Architecture: Cosmos-Reason-7B is post-trained based onQwen2.5-VL-7B-Instructand follows the same model architecture. Qwen2.5-VL-7B-Instruct (https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) Number of model parameters: Cosmos-Reason1-7B: • Vision Transformer (ViT): 675.76M (675,759,104) • Language Model (LLM): 7.07B (7,070,619,136) • Other components (output projection layer): 545.00M (544,997,376) Computational Load: • Cumulative Compute: 3.2603016e+21 FLOPS • Estimated Energy and Emissions for Model Training: • Total kWh = 16658432 • Total Emissions (tCO2e) = 5380.674 Input: ------ Input Types:Text, VideoInput Formats:Text, VideoInput Parameters:Text: One Dimensional (1D), Video(3D)Other Input Properties: Input Types: Input Formats: Input Parameters: Other Input Properties: • Use FPS=4 for input video to match the training setup. • AppendAnswer the question in the following format: <think>\\nyour reasoning\\n</think>\\n\\n<answer>\\nyour answer\\n</answer>.in the system prompt to encourage long chain-of-thought reasoning response. Input Context Length (ISL):128K Input Context Length (ISL): Output: ------- Output Type:TextOutput Format:StringOutput Parameters:One Dimensional (1D)Other Output Properties: Output Type: Output Format: Output Parameters: Other Output Properties: • Recommend using 4096 or more output max tokens to avoid truncation of long chain-of-thought response. Recommend using 4096 or more output max tokens to avoid truncation of long chain-of-thought response. • Our AI model recognizes timestamps added at the bottom of each frame for accurate temporal localization. Our AI model recognizes timestamps added at the bottom of each frame for accurate temporal localization. • Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.__ Our AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.__ Software Integration: ===================== Runtime Engines: • vLLM Supported Hardware: • NVIDIA Blackwell: NVIDIA Blackwell: • NVIDIA Hopper: NVIDIA Hopper: Training, Testing, and Evaluation Datasets: =========================================== 05/17/2025 05/17/2025 (https://huggingface.co/nvidia/Cosmos-Reason1-7B#05172025) Please see ourtechnical paperfor detailed evaluations on physical common sense and embodied reasoning. Part of the evaluation datasets are released underCosmos-Reason1-Benchmark. The embodied reasoning datasets and benchmarks focus on the following areas: robotics (RoboVQA, BridgeDataV2, Agibot, RobFail), ego-centric human demonstration (HoloAssist), and Autonomous Vehicle (AV) driving video data. The AV dataset is collected and annotated by NVIDIA. technical paper (https://arxiv.org/pdf/2503.15558) Cosmos-Reason1-Benchmark (https://huggingface.co/datasets/nvidia/Cosmos-Reason1-Benchmark) All datasets go through the data annotation process described in the technical paper to prepare training and evaluation data and annotations. 08/01/2025 08/01/2025 (https://huggingface.co/nvidia/Cosmos-Reason1-7B#08012025) We enhance the model capability with the augmented training data. PLM-Video-Human and Nexar are used to enable dense temporal captioning. Describe Anything is added to enhance a set of mark (SoM) prompting. We enrich data in intelligent transportation systems (ITS) and warehouse applications. Lastly, Visual Critics dataset contains a collection of AI generated videos from Cosmos-Predict2 and Wan2.1 with human annotations to describe the physical correctness in AI videos. Training Dataset ---------------- • Data Modalities: Video, Text • Text Training Data Size: Not specified (included in multimodal inputs) • Video Training Data Size: Less than 10,000 Hours (~2M video-text pairs in SFT + RL datasets) Data Collection Method by dataset: • RoboVQA: Hybrid: Automatic/Sensors • BridgeDataV2: Automatic/Sensors • AgiBot: Automatic/Sensors • RoboFail: Automatic/Sensors • HoloAssist: Human • AV: Automatic/Sensors • PLM-Video-Human: Human • Nexar: Automatic/Sensors • Describe Anything: Human • ITS / Warehouse: Human, Automatic • Visual Critics: Automatic Labeling Method by dataset: • RoboVQA: Hybrid: Human,Automated • BridgeDataV2: Hybrid: Human,Automated • AgiBot: Hybrid: Human,Automated • RoboFail: Hybrid: Human,Automated • HoloAssist: Hybrid: Human,Automated • AV: Hybrid: Human,Automated • PLM-Video-Human: Human,Automated • Nexar: Human • Describe Anything: Human,Automated • ITS / Warehouse: Human, Automated • Visual Critics: Human,Automated Evaluation Dataset ------------------ • RoboVQA: Hybrid: Automatic/Sensors • BridgeDataV2: Automatic/Sensors • AgiBot: Automatic/Sensors • RoboFail: Automatic/Sensors • HoloAssist: Human • AV: Automatic/Sensors Labeling Method by dataset: • RoboVQA: Hybrid: Human,Automated • BridgeDataV2: Hybrid: Human,Automated • AgiBot: Hybrid: Human,Automated • RoboFail: Hybrid: Human,Automated • HoloAssist: Hybrid: Human,Automated • AV: Hybrid: Human,Automated Evaluation Benchmark Results: We report the model accuracy on the embodied reasoning benchmark introduced in Cosmos-Reason1. The results differ from those presented in Table 9 due to additional training aimed at supporting a broader range of Physical AI tasks beyond the benchmark. Evaluation Results ------------------ Dataset Format ============== Modality: Video (mp4) and Text Dataset Quantification ====================== 05/17/2025 05/17/2025 (https://huggingface.co/nvidia/Cosmos-Reason1-7B#05172025-1) We release the embodied reasoning data and benchmarks. Each data sample is a pair of video and text. The text annotations include understanding and reasoning annotations described in the Cosmos-Reason1 paper. Each video may have multiple text annotations. The quantity of the video and text pairs is described in the table below. The AV data is currently unavailable and will be uploaded soon! We release text annotations for all embodied reasoning datasets and videos for RoboVQA and AV datasets. For other datasets, users may download the source videos from the original data source and find corresponding video sources via the video names. The held-out RoboFail benchmark is released for measuring the generalization capability. 08/01/2025 08/01/2025 (https://huggingface.co/nvidia/Cosmos-Reason1-7B#08012025-1) Inference --------- Test Hardware:H100 Test Hardware: Note:We suggest using fps=4 for the input video and max_tokens=4096 to avoid truncated response. Note We suggest using fps=4 for the input video and max_tokens=4096 to avoid truncated response. Sampling Parameters Configuration --------------------------------- Video Message Setup ------------------- Message Processing and Generation --------------------------------- Key Inference Notes ------------------- • FPS Requirement: UseFPS=4for input video to match the training setup FPS Requirement • System Prompt: Append\"Answer the question in the following format: <think>\\nyour reasoning\\n</think>\\n\\n<answer>\\nyour answer\\n</answer>.\"in the system prompt to encourage long chain-of-thought reasoning response System Prompt • Output Tokens: Recommend using 4096 or more output max tokens to avoid truncation of long chain-of-thought response Output Tokens • Hardware: Model is designed and optimized to run on NVIDIA GPU-accelerated systems Hardware • Precision: Tested with BF16 precision for inference Precision • Operating System: Tested on Linux (other operating systems not tested) Operating System Ethical Considerations ---------------------- NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "FLUX.1-Kontext-dev",
    "nvidia_url": "https://build.nvidia.com/black-forest-labs/flux_1-kontext-dev",
    "scraped_on": "2025-10-15T21:38:28.995011",
    "tags": [
      "image generation",
      "run-on-rtx"
    ],
    "model_card": "Overview ======== Description: ============ FLUX.1 Kontext is a suite of generative models for in-context image generation and editing, enabling users to prompt with text and modify with targeted changes. This allows for inpainting, character and object consistency, and style transfer - all without a complex workflow. This model is ready for non-commercial use. Contactsales@blackforestlabs.aifor commercial terms. sales@blackforestlabs.ai (mailto:sales@blackforestlabs.ai) Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to: • black-forest-labs/FLUX.1-Kontext-dev Model Card black-forest-labs/FLUX.1-Kontext-dev Model Card (https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev) License/Terms of Use -------------------- GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service. Contactsales@blackforestlabs.aifor commercial terms to use the Flux.1-Kontext-dev model. ADDITIONAL INFORMATION:Apache 2.0,NVIDIA Community Model License AgreementandLlama 2 Community Model License Agreement. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) sales@blackforestlabs.ai (mailto:sales@blackforestlabs.ai) Apache 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) NVIDIA Community Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Llama 2 Community Model License Agreement (https://github.com/meta-llama/llama-models/blob/main/models/llama2/LICENSE) Deployment Geography: --------------------- Global Use Case: ========= Creators and professionals can use this model to generate high-quality images from text prompts, simplifying visual communication. Release Date: ------------- • Build.Nvidia.com August 11, 2025 viahttps://build.nvidia.com/black-forest-labs/flux_1-kontext-dev https://build.nvidia.com/black-forest-labs/flux_1-kontext-dev • Huggingface May 29, 2025 viahttps://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev References ---------- • Flux.1-Kontext-dev blog post Flux.1-Kontext-dev blog post (https://bfl.ai/announcements/flux-1-kontext/) Model Architecture: =================== Architecture Type: Transformer and Convolutional Neural Network (CNN)Network Architecture: Diffusion TransformerLiheYoung/Depth-anything-large-hf leverages the DPT architecture with a DINOv2 backbone. Architecture Type Network Architecture Input: ====== Input Type: Text, ImageInput Parameters: Text: One-Dimensional (1D); Image: Two-Dimensional (2D)Input Format: Text: String. Image: Red, Green, Blue (RGB)Other Properties Related to Input: Steps, Classifier-Free Guidance Scale, Output Image Aspect Ratio, and Seed per theAPI Reference Page Input Type Input Parameters Input Format Other Properties Related to Input API Reference Page (https://docs.api.nvidia.com/nim/reference/black-forest-labs-flux_1-kontext-dev) Output: ======= Output Type: ImageOutput Parameters: Two-Dimensional (2D)Output Format: Red, Green, Blue (RGB)Other Properties Related to Output: Supported resolutions 672x1568, 688x1504, 720x1456, 752x1392, 800x1328, 832x1248, 880x1184, 944x1104, 1024x1024, 1104x944, 1184x880, 1248x832, 1328x800, 1392x752, 1456x720, 1504x688, 1568x672 Output Type Output Parameters Output Format Other Properties Related to Output Software Integration: ===================== Runtime Engines: • TensorRT Supported Hardware Platforms: Supported Hardware Platforms • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace Supported Operating Systems: Linux, Windows Subsystem for Linux Supported Operating Systems The integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment. Model Version(s): ================= • FLUX.1-Kontext-dev Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= • Data Collection Method by dataset: Undisclosed • Labeling Method by dataset: Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Testing Dataset: ================ • Data Collection Method by dataset: Undisclosed • Labeling Method by dataset: Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: =================== • Data Collection Method by dataset: Undisclosed • Labeling Method by dataset: Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Pre-training mitigation. We filtered pre-training data for multiple categories of “not safe for work” (NSFW) content to help prevent a user generating unlawful content in response to text prompts or uploaded images. Post-training mitigation. We have partnered with the Internet Watch Foundation, an independent nonprofit organization dedicated to preventing online abuse, to filter known child sexual abuse material (CSAM) from post-training data. Subsequently, we undertook multiple rounds of targeted fine-tuning to provide additional mitigation against potential abuse. By inhibiting certain behaviors and concepts in the trained model, these techniques can help to prevent a user generating synthetic CSAM or nonconsensual intimate imagery (NCII) from a text prompt, or transforming an uploaded image into synthetic CSAM or NCII. Pre-release evaluation. Throughout this process, we conducted multiple internal and external third-party evaluations of model checkpoints to identify further opportunities for improvement. The third-party evaluations—which included 21 checkpoints of FLUX.1 Kontext [pro] and [dev]—focused on eliciting CSAM and NCII through adversarial testing with text-only prompts, as well as uploaded images with text prompts. Next, we conducted a final third-party evaluation of the proposed release checkpoints, focused on text-to-image and image-to-image CSAM and NCII generation. The final FLUX.1 Kontext [pro] (as offered through the FLUX API only) and FLUX.1 Kontext [dev] (released as an open-weight model) checkpoints demonstrated very high resilience against violative inputs, and FLUX.1 Kontext [dev] demonstrated higher resilience than other similar open-weight models across these risk categories. Based on these findings, we approved the release of the FLUX.1 Kontext [pro] model via API, and the release of the FLUX.1 Kontext [dev] model as openly-available weights under a non-commercial license to support third-party research and development. Inference filters. We are applying multiple filters to intercept text prompts, uploaded images, and output images on the FLUX API for FLUX.1 Kontext [pro]. Filters for CSAM and NCII are provided by Hive, a third-party provider, and cannot be adjusted or removed by developers. We provide filters for other categories of potentially harmful content, including gore, which can be adjusted by developers based on their specific risk profile. Additionally, the repository for the open FLUX.1 Kontext [dev] model includes filters for illegal or infringing content. Filters or manual review must be used with the model under the terms of the FLUX.1 [dev] Non-Commercial License. We may approach known deployers of the FLUX.1 Kontext [dev] model at random to verify that filters or manual review processes are in place. Content provenance. The FLUX API applies cryptographically-signed metadata to output content to indicate that images were produced with our model. Our API implements the Coalition for Content Provenance and Authenticity (C2PA) standard for metadata. Policies. Access to our API and use of our models are governed by our Developer Terms of Service, Usage Policy, and FLUX.1 [dev] Non-Commercial License, which prohibit the generation of unlawful content or the use of generated content for unlawful, defamatory, or abusive purposes. Developers and users must consent to these conditions to access the FLUX Kontext models. Monitoring. We are monitoring for patterns of violative use after release, and may ban developers who we detect intentionally and repeatedly violate our policies via the FLUX API. Additionally, we provide a dedicated email address (safety@blackforestlabs.ai) to solicit feedback from the community. We maintain a reporting relationship with organizations such as the Internet Watch Foundation and the National Center for Missing and Exploited Children, and we welcome ongoing engagement with authorities, developers, and researchers to share intelligence about emerging risks and develop effective mitigations. safety@blackforestlabs.ai (mailto:safety@blackforestlabs.ai) Inference: ========== Engine: TensorRTTest Hardware: H100 Engine Test Hardware Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "stable-diffusion-3.5-large",
    "nvidia_url": "https://build.nvidia.com/stabilityai/stable-diffusion-3_5-large",
    "scraped_on": "2025-10-15T21:38:28.511386",
    "tags": [
      "image generation",
      "text-to-image"
    ],
    "model_card": "Overview ======== Description: ============ Stable Diffusion 3.5 is an 8B parameter base model that produces high-quality images, with Depth and Canny ControlNets offering controllability over image outputs. Choose this model when you want high quality, artistic style, and more fine-tuning flexibility. This model is ready for non-commercial use. Contact Stability AI athttps://stability.ai/enterprisefor commercial use of Stable Diffusion 3.5 Large model. https://stability.ai/enterprise Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see links to: • stabilityai/stable-diffusion-3.5-large Model Card stabilityai/stable-diffusion-3.5-large Model Card (https://huggingface.co/stabilityai/stable-diffusion-3.5-large) • stabilityai/stable-diffusion-3.5-large-controlnet-canny Model Card stabilityai/stable-diffusion-3.5-large-controlnet-canny Model Card (https://huggingface.co/stabilityai/stable-diffusion-3.5-large-controlnet-canny) • stabilityai/stable-diffusion-3.5-large-controlnet-depth Model Card stabilityai/stable-diffusion-3.5-large-controlnet-depth Model Card (https://huggingface.co/stabilityai/stable-diffusion-3.5-large-controlnet-depth) • LiheYoung/Depth-anything-large-hf Model Card LiheYoung/Depth-anything-large-hf Model Card (https://huggingface.co/LiheYoung/depth-anything-large-hf) License/Terms of Use -------------------- GOVERNING TERMS: The trial service is governed by theNVIDIA API Trial Terms of Service, and use of the Cosmos model is governed by theNVIDIA Community Model License Agreement. Contact Stability AI athttps://stability.ai/enterprisefor commercial use of Stable Diffusion 3.5 Large model. ADDITIONAL INFORMATION:Llama 2 Community Model License AgreementandApache License, Version 2.0. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) https://stability.ai/enterprise Llama 2 Community Model License Agreement (https://github.com/meta-llama/llama-models/blob/main/models/llama2/LICENSE) Apache License, Version 2.0 (https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md) Deployment Geography: --------------------- Global Use Case: ========= Creators and professionals can use this model to generate high-quality images from text prompts, simplifying visual communication. Release Date: ------------- • Build.Nvidia.com August 11, 2025 viahttps://build.nvidia.com/stabilityai/stable-diffusion-3_5-large https://build.nvidia.com/stabilityai/stable-diffusion-3_5-large • Huggingface October 22, 2024 viahttps://huggingface.co/stabilityai/stable-diffusion-3.5-large https://huggingface.co/stabilityai/stable-diffusion-3.5-large References ---------- • Stable Diffusion 3.5 blog post Stable Diffusion 3.5 blog post (https://stability.ai/news/introducing-stable-diffusion-3-5) Model Architecture: =================== Architecture Type: Transformer and Convolutional Neural Network (CNN)Network Architecture: Diffusion TransformerLiheYoung/Depth-anything-large-hf leverages the DPT architecture with a DINOv2 backbone. Architecture Type Network Architecture Input: ====== Input Type: Text, Image (optional)Input Parameters: Text: One-Dimensional (1D); Image: Two-Dimensional (2D)Input Format: Text: String. Image: Red, Green, Blue (RGB)Other Properties Related to Input: Steps, Classifier-Free Guidance Scale, Output Image Aspect Ratio, and Seed Input Type Input Parameters Input Format Other Properties Related to Input Output: ======= Output Type: ImageOutput Parameters: Two-Dimensional (2D)Output Format: Red, Green, Blue (RGB)Other Properties Related to Output: Supported resolutions 1024x1024, 768x1344, 1344x768, 1344x768, 1344x768, 1344x768, 1216x832 Output Type Output Parameters Output Format Other Properties Related to Output Software Integration: ===================== Runtime Engines: • TensorRT Supported Hardware Platforms: Supported Hardware Platforms • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace Supported Operating Systems: Linux, Windows Subsystem for Linux Supported Operating Systems The integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment. Model Versions: =============== • Stable Diffusion 3.5 Large • Stable Diffusion 3.5 Large Controlnet - Canny • Stable Diffusion 3.5 Large Controlnet - Depth • LiheYoung/Depth-anything-large-hf Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= • Data Collection Method by dataset: Undisclosed • Labeling Method by dataset: Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Testing Dataset: ================ • Data Collection Method by dataset: Undisclosed • Labeling Method by dataset: Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: =================== • Data Collection Method by dataset: Undisclosed • Labeling Method by dataset: Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)):Undisclosed Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference: ========== Engine: TensorRTTest Hardware: H100 Engine Test Hardware Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "nvidia-nemotron-nano-9b-v2",
    "nvidia_url": "https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2",
    "scraped_on": "2025-10-15T21:38:28.194322",
    "tags": [
      "reasoning",
      "thinking budget"
    ],
    "model_card": "NVIDIA-Nemotron-Nano-9B-v2 Overview =================================== NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so, albeit with a slight decrease in accuracy for harder prompts that require reasoning. Conversely, allowing the model to generate reasoning traces first generally results in higher-quality final solutions to queries and tasks. The model uses a hybrid architecture consisting primarily of Mamba-2 and MLP layers combined with just four Attention layers. For the architecture, please refer to theNemotron-H tech report. Nemotron-H tech report (https://arxiv.org/abs/2504.03624) The supported languages include: English, German, Spanish, French, Italian, and Japanese. Improved using Qwen. This model is ready for commercial use. Evaluation Results ================== Benchmark Results (Reasoning On) -------------------------------- We evaluated our model in **Reasoning-On** mode across all benchmarks. All evaluations were done usingNeMo-Skills. NeMo-Skills (https://github.com/NVIDIA/NeMo-Skills/tree/main/docs) Reasoning Budget Control ------------------------ This model supports runtime “thinking” budget control. During inference, the user can specify how many tokens the model is allowed to \"think\". License/Terms of Use ==================== Governing Terms: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Open Model License Agreement. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Open Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/) Model Architecture ================== • Architecture Type: Mamba2-Transformer Hybrid • Network Architecture: Nemotron-Hybrid Deployment Geography: Global ---------------------------- Use Case -------- NVIDIA-Nemotron-Nano-9B-v2 is a general purpose reasoning and chat model intended to be used in English and coding languages. Other non-English languages (German, French, Italian, Spanish and Japanese) are also supported. Developers designing AI Agent systems, chatbots, RAG systems, and other AI-powered applications. Also suitable for typical instruction-following tasks. Release Date: 08/18/2025 ------------------------ Huggingface 08/18/2025 viaLinkAPI Catalog 08/18/2025 viaLink Link (https://huggingface.co/) Link (https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2) References ========== • NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Computational Load ================== Cumulative compute : 1.53E+24 FLOPS Estimate energy and emissions for model training: 747.6 MWh Input ===== • Input Type(s): Text • Input Format(s): String • Input Parameters: One-Dimensional (1D): Sequences • Other Properties Related to Input: Context length up to 128K. Supported languages include German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, Chinese and English. Output ====== • Output Type(s): Text • Output Format: String • Output Parameters: One-Dimensional (1D): Sequences up to 128K Our models are designed and optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA’s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. Software Integration ==================== • Runtime Engine(s): NeMo 25.07.nemotron-nano-v2 • Supported Hardware Microarchitecture Compatibility: NVIDIA A10G, NVIDIA H100-80GB, NVIDIA A100 • Operating System(s): Linux Use it with Transformers ------------------------ Use it with Transformers The snippet below shows how to use this model with Huggingface Transformers (tested on version 4.48.3). Case 1:/thinkor no reasoning signal is provided in the system prompt, reasoning will be set toTrue Case 2:/no_thinkis provided, reasoning will be set toFalse Note:/thinkor/no_thinkkeywords can also be provided in “user” messages for turn-level reasoning control. The rest of the inference snippet remains the same We recommend settingtemperatureto0.6,top_pto0.95for reasoning True and greedy search for reasoning False, and increasemax_new_tokensto1024or higher for reasoning True. Use it with TRT-LLM ------------------- Use it with TRT-LLM The snippet below shows how to use this model with TRT-LLM. We tested this on the followingcommitand followed theseinstructionsto build and install TRT-LLM in a docker container. commit (https://github.com/NVIDIA/TensorRT-LLM/tree/46c5a564446673cdd0f56bcda938d53025b6d04e) instructions (https://github.com/NVIDIA/TensorRT-LLM/blob/46c5a564446673cdd0f56bcda938d53025b6d04e/docs/source/installation/build-from-source-linux.md#option-2-build-tensorrt-llm-step-by-step) Use it with vLLM ---------------- Use it with vLLM The snippet below shows how to use this model with vLLM. Use the followingcommitand follow these instructions to build and install vLLM in a docker container. commit (https://github.com/vllm-project/vllm/commit/75531a6c134282f940c86461b3c40996b4136793) Now you can run run the server with: Note: Remember to add `--mamba_ssm_cache_dtype float32` for accurate quality. Without this option, the model’s accuracy may degrade. Using Budget Control with a vLLM Server --------------------------------------- The thinking budget allows developers to keep accuracy high and meet response‑time targets - which is especially crucial for customer support, autonomous agent steps, and edge devices where every millisecond counts. With budget control, you can set a limit for internal reasoning: • max_thinking_tokens: This is a threshold that will attempt to end the reasoning trace at the next newline encountered in the reasoning trace. If no newline is encountered within 500 tokens, it will abruptly end the reasoning trace at `max_thinking_tokens + 500`. Start a vLLM server: Client for supporting budget control: Calling the server with a budget (Restricted to 32 tokens here as an example) You should see output similar to the following: Using Tool-Calling with a vLLM Server ------------------------------------- Start a vLLM server with native tool-calling: After launching a vLLM server, you can call the server with tool-call support using a Python script like below: =============================================================================================================== You should see output similar to the following: Model Version ============= • v1.0 Prompt Format ============= We follow the jinja chat template provided below. This template conditionally adds<think>\\nto the start of the Assistant response if/thinkis found in the system prompt or if no reasoning signal is added, and adds<think></think>to the start of the Assistant response if/no_thinkis found in the system prompt. Thus enforcing reasoning on/off behavior. Training, Testing, and Evaluation Datasets ========================================== Training datasets ----------------- • Data Modality: Text • Text Training Data Size: More than 10 Trillion Tokens • Train/Test/Valid Split: We used 100% of the corpus for pre-training and relied on external benchmarks for testing. • Data Collection Method by dataset: Hybrid: Automated, Human, Synthetic • Labeling Method by dataset: Hybrid: Automated, Human, Synthetic Properties:The post-training corpus for NVIDIA-Nemotron-Nano-9B-v2 consists of English and multilingual text (German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, Chinese and English). Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. The corpus spans domains including code, legal, math, science, finance, and more. We also include a small portion of question-answering, and alignment style data to improve model accuracies. For several of the domains listed above we used synthetic data, specifically reasoning traces, from DeepSeek R1/R1-0528, Qwen3-235B-A22B, Nemotron 4 340B, Qwen2.5-32B-Instruct-AWQ, Qwen2.5-14B-Instruct, Qwen 2.5 72B. Properties: The pre-training corpus for NVIDIA-Nemotron-Nano-9B-v2 consists of high-quality curated and synthetically-generated data. It is trained in the English language, as well as 15 multilingual languages and 43 programming languages. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. The corpus spans domains including legal, math, science, finance, and more. We also include a small portion of question-answering, and alignment style data to improve model accuracy. The model was pre-trained for approximately twenty trillion tokens. More details on the datasets and synthetic data generation methods can be found in the technical reportNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model. NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model Public Datasets =============== Problems in Elementary Mathematics for Home Study (https://archive.org/details/AntonovVygodskyNikitinSankinProblemsInElementaryMathematicsForHomeStudyMir1982) GSM8K (https://github.com/openai/grade-school-math) PRM800K (https://github.com/openai/prm800k) CC-NEWS (https://commoncrawl.org/blog/news-dataset-available) Common Crawl (https://commoncrawl.org/) Wikimedia (https://dumps.wikimedia.org/) Bespoke-Stratos-17k (https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k) tigerbot-kaggle-leetcodesolutions-en-2k (https://huggingface.co/datasets/TigerResearch/tigerbot-kaggle-leetcodesolutions-en-2k) glaive-function-calling-v2 (https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) APIGen Function-Calling (https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k) LMSYS-Chat-1M (https://huggingface.co/datasets/lmsys/lmsys-chat-1m) Open Textbook Library - CC BY-SA & GNU subset (https://open.umn.edu/opentextbooks/textbooks/) OpenStax - CC BY-SA subset (https://openstax.org/) Advanced Reasoning Benchmark (https://github.com/TheDuckAI/arb) tigerbot-kaggle-leetcodesolutions-en-2k (https://huggingface.co/datasets/TigerResearch/tigerbot-kaggle-leetcodesolutions-en-2k) PRM800K (https://github.com/openai/prm800k) SciBench (https://github.com/mandyyyyii/scibench) FineWeb-2 (https://huggingface.co/datasets/HuggingFaceFW/fineweb-2) Court Listener (https://www.courtlistener.com/help/api/bulk-data/) peS2o (https://huggingface.co/datasets/allenai/peS2o) OpenWebMath (https://huggingface.co/datasets/open-web-math/open-web-math) BioRxiv (https://www.biorxiv.org/tdm) PMC Open Access Subset (https://pmc.ncbi.nlm.nih.gov/tools/openftlist/) OpenWebText2 (https://openwebtext2.readthedocs.io/en/latest/) Stack Exchange Data Dump (https://archive.org/details/stackexchange) PubMed Abstracts (https://github.com/thoppe/The-Pile-PubMed) NIH ExPorter (https://exporter.nih.gov/ExPORTER_Catalog.aspx) arXiv (https://info.arxiv.org/help/bulk_data/index.html) BigScience Workshop Datasets (https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#datasets) Reddit Dataset (https://files.pushshift.io/reddit/) SEC's Electronic Data Gathering, Analysis, and Retrieval (EDGAR) (https://www.sec.gov/search-filings) Public Software Heritage S3 (https://docs.softwareheritage.org/devel/swh-export/graph/dataset.html#summary-of-dataset-versions) The Stack (https://huggingface.co/datasets/bigcode/the-stack) mC4 (https://huggingface.co/datasets/legacy-datasets/mc4) Advanced Mathematical Problem Solving (https://github.com/hendrycks/math?tab=readme-ov-file) MathPile (https://github.com/GAIR-NLP/MathPile/) NuminaMath CoT (https://huggingface.co/datasets/AI-MO/NuminaMath-CoT) PMC Article (https://pmc.ncbi.nlm.nih.gov/tools/textmining/) FLAN (https://github.com/google-research/FLAN) Advanced Reasoning Benchmark (https://github.com/TheDuckAI/arb) SciBench (https://github.com/mandyyyyii/scibench) WikiTableQuestions (https://huggingface.co/datasets/wikitablequestions) FinQA (https://finqasite.github.io/) Riddles (https://github.com/crawsome/riddles) Problems in Elementary Mathematics for Home Study (https://archive.org/details/AntonovVygodskyNikitinSankinProblemsInElementaryMathematicsForHomeStudyMir1982) MedMCQA (https://huggingface.co/datasets/openlifescienceai/medmcqa) Cosmos QA (https://huggingface.co/datasets/allenai/cosmos_qa) MCTest (https://huggingface.co/datasets/sagnikrayc/mctest) AI2's Reasoning Challenge (https://huggingface.co/datasets/ai2_arc) OpenBookQA (https://github.com/allenai/OpenBookQA) MMLU Auxiliary Train (https://huggingface.co/datasets/cais/mmlu/viewer/all/auxiliary_train) social-chemestry-101 (https://huggingface.co/datasets/tasksource/social-chemestry-101) Moral Stories (https://huggingface.co/datasets/demelin/moral_stories) The Common Pile v0.1 (https://huggingface.co/common-pile) FineMath (https://huggingface.co/datasets/HuggingFaceTB/finemath) MegaMath (https://huggingface.co/datasets/LLM360/MegaMath) FastChat (https://github.com/lm-sys/FastChat) Private Non-publicly Accessible Datasets of Third Parties ========================================================= Online Dataset Sources ====================== The English Common Crawl data was downloaded from the Common Crawl Foundation (see theirFAQfor details on their crawling) and includes the snapshots CC-MAIN-2013-20 through CC-MAIN-2025-13. The data was subsequently deduplicated and filtered in various ways described in theNemotron-CC paper. FAQ (https://commoncrawl.org/faq) Nemotron-CC paper (https://arxiv.org/abs/2412.02595) Additionally, we extracted data for fifteen languages from the following three Common Crawl snapshots: CC-MAIN-2024-51, CC-MAIN-2025-08, CC-MAIN-2025-18. The fifteen languages included were Arabic, Chinese, Danish, Dutch, French, German, Italian, Japanese, Korean, Polish, Portuguese, Russian, Spanish, Swedish, and Thai. As we did not have reliable multilingual model-based quality classifiers available, we applied just heuristic filtering instead—similar to what we did for lower quality English data in the Nemotron-CC pipeline, but selectively removing some filters for some languages that did not work well. Deduplication was done in the same way as for Nemotron-CC. The GitHub Crawl was collected using the GitHub REST API and the Amazon S3 API. Each crawl was operated in accordance with the rate limits set by its respective source, either GitHub or S3. We collect raw source code and subsequently remove any having a license which does not exist in our permissive-license set (for additional details, refer to the technical report). NVIDIA-Sourced Synthetic Datasets ================================= Art of Problem Solving (https://artofproblemsolving.com/company) American Mathematics Competitions 8 (https://artofproblemsolving.com/wiki/index.php/AMC_8_Problems_and_Solutions) American Mathematics Competitions 10 (https://artofproblemsolving.com/wiki/index.php/AMC_10_Problems_and_Solutions) DeepSeek-R1 (https://huggingface.co/deepseek-ai/DeepSeek-R1) social-chemestry-101 (https://huggingface.co/datasets/tasksource/social-chemestry-101) Moral Stories (https://huggingface.co/datasets/demelin/moral_stories) Mixtral-8x22B-v0.1 (https://huggingface.co/mistralai/Mixtral-8x22B-v0.1) OpenStax - CC BY-SA subset (https://openstax.org/) DeepSeek-V3 (https://huggingface.co/deepseek-ai/DeepSeek-V3) Mixtral-8x22B-v0.1 (https://huggingface.co/mistralai/Mixtral-8x22B-v0.1) Qwen2.5-72B (https://huggingface.co/Qwen/Qwen2.5-72B) OpenStax - CC BY-SA subset (https://openstax.org/) DeepSeek-V3 (https://huggingface.co/deepseek-ai/DeepSeek-V3) Mixtral-8x22B-v0.1 (https://huggingface.co/mistralai/Mixtral-8x22B-v0.1) Qwen2.5-72B (https://huggingface.co/Qwen/Qwen2.5-72B) OpenStax - CC BY-SA subset (https://openstax.org/) GSM8K (https://github.com/openai/grade-school-math) Open Textbook Library - CC BY-SA & GNU subset (https://open.umn.edu/opentextbooks/textbooks/) DeepSeek-R1 (https://huggingface.co/deepseek-ai/DeepSeek-R1) DeepSeek-V3 (https://huggingface.co/deepseek-ai/DeepSeek-V3) DeepSeek-V3-0324 (https://huggingface.co/deepseek-ai/DeepSeek-V3-0324) Qwen2.5-72B (https://huggingface.co/Qwen/Qwen2.5-72B) Nemotron-PrismMath (https://huggingface.co/datasets/nvidia/Nemotron-PrismMath) Big-Math-RL-Verified (https://huggingface.co/datasets/SynthLabsAI/Big-Math-RL-Verified) OpenR1-Math-220k (https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) Qwen2.5-0.5B-instruct (https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct) Qwen2.5-72B-Instruct (https://huggingface.co/Qwen/Qwen2.5-72B-Instruct) DeepSeek-R1-Distill-Qwen-32B (https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) arXiv (https://info.arxiv.org/help/bulk_data/index.html) National Institutes of Health ExPorter (https://www.nih.gov/) BioRxiv (https://www.biorxiv.org/tdm) PMC Article (https://pmc.ncbi.nlm.nih.gov/tools/textmining/) USPTO Backgrounds (https://data.uspto.gov/apis/transition-guide/bdss#pats) peS2o (https://huggingface.co/datasets/allenai/peS2o) CORE (https://core.ac.uk/documentation/dataset) PG-19 (https://github.com/google-deepmind/pg19) DOAB CC BY & CC BY-SA subset (https://www.doabooks.org/en) NDLTD (https://ndltd.org/thesis-resources/global-etd-search/) Qwen2.5-72B-Instruct (https://huggingface.co/Qwen/Qwen2.5-72B-Instruct) Common Crawl (https://commoncrawl.org/latest-crawl) DeepSeek-V3 (https://huggingface.co/deepseek-ai/DeepSeek-V3) Common Crawl (https://commoncrawl.org/latest-crawl) phi-4 (https://huggingface.co/microsoft/phi-4) Common Crawl (https://commoncrawl.org/latest-crawl) phi-4 (https://huggingface.co/microsoft/phi-4) Nemotron-MIND (https://huggingface.co/datasets/nvidia/Nemotron-MIND) Common Crawl (https://commoncrawl.org/latest-crawl) phi-4 (https://huggingface.co/microsoft/phi-4) Common Crawl (https://commoncrawl.org/latest-crawl) phi-4 (https://huggingface.co/microsoft/phi-4) Common Crawl (https://commoncrawl.org/latest-crawl) phi-4 (https://huggingface.co/microsoft/phi-4) Common Crawl (https://commoncrawl.org/latest-crawl) phi-4 (https://huggingface.co/microsoft/phi-4) Common Crawl (https://commoncrawl.org/latest-crawl) phi-4 (https://huggingface.co/microsoft/phi-4) AQUA-RAT (https://huggingface.co/datasets/deepmind/aqua_rat) LogiQA (https://huggingface.co/datasets/lucasmccabe/logiqa) AR-LSAT (https://github.com/zhongwanjun/AR-LSAT) DeepSeek-V3 (https://huggingface.co/deepseek-ai/DeepSeek-V3) DeepSeek-V3-0324 (https://huggingface.co/deepseek-ai/DeepSeek-V3-0324) AQUA-RAT (https://huggingface.co/datasets/deepmind/aqua_rat) LogiQA (https://huggingface.co/datasets/lucasmccabe/logiqa) AR-LSAT (https://github.com/zhongwanjun/AR-LSAT) Qwen3-30B-A3B (https://huggingface.co/Qwen/Qwen3-30B-A3B) Art of Problem Solving (https://artofproblemsolving.com/company) American Mathematics Competitions 8 (https://artofproblemsolving.com/wiki/index.php/AMC_8_Problems_and_Solutions) American Mathematics Competitions 10 (https://artofproblemsolving.com/wiki/index.php/AMC_10_Problems_and_Solutions) GSM8K (https://github.com/openai/grade-school-math) PRM800K (https://github.com/openai/prm800k) Qwen2.5-32B-Instruct (https://huggingface.co/Qwen/Qwen2.5-32B-Instruct) Qwen2.5-Math-72B (https://huggingface.co/Qwen/Qwen2.5-Math-72B) Qwen2.5-Math-7B (https://huggingface.co/Qwen/Qwen2.5-Math-7B) Qwen2.5-72B-Instruct (https://huggingface.co/Qwen/Qwen2.5-72B-Instruct) MMLU Auxiliary Train (https://huggingface.co/datasets/cais/mmlu/viewer/all/auxiliary_train) DeepSeek-R1 (https://huggingface.co/deepseek-ai/DeepSeek-R1) arXiv (https://info.arxiv.org/help/bulk_data/index.html) National Institutes of Health ExPorter (https://www.nih.gov/) BioRxiv (https://www.biorxiv.org/tdm) PMC Article (https://pmc.ncbi.nlm.nih.gov/tools/textmining/) USPTO Backgrounds (https://data.uspto.gov/apis/transition-guide/bdss#pats) peS2o (https://huggingface.co/datasets/allenai/peS2o) CORE (https://core.ac.uk/documentation/dataset) PG-19 (https://github.com/google-deepmind/pg19) DOAB CC BY & CC BY-SA subset (https://www.doabooks.org/en) NDLTD (https://ndltd.org/thesis-resources/global-etd-search/) Qwen2.5-72B-Instruct (https://huggingface.co/Qwen/Qwen2.5-72B-Instruct) Common Crawl (https://commoncrawl.org/) Qwen3-30B-A3B (https://huggingface.co/Qwen/Qwen3-30B-A3B) Mistral-NeMo-12B-Instruct (https://huggingface.co/nvidia/Mistral-NeMo-12B-Instruct) Common Crawl (https://commoncrawl.org/) Qwen3-30B-A3B (https://huggingface.co/Qwen/Qwen3-30B-A3B) Wikimedia (https://dumps.wikimedia.org/) Qwen3-30B-A3B (https://huggingface.co/Qwen/Qwen3-30B-A3B) OpenMathReasoning (https://huggingface.co/datasets/nvidia/OpenMathReasoning) DeepSeek-R1-0528 (https://huggingface.co/deepseek-ai/DeepSeek-R1-0528) OpenCodeReasoning (https://huggingface.co/datasets/nvidia/OpenCodeReasoning) DeepSeek-R1-0528 (https://huggingface.co/deepseek-ai/DeepSeek-R1-0528) Humanity's Last Exam (https://huggingface.co/datasets/cais/hle) DeepSeek-R1-0528 (https://huggingface.co/deepseek-ai/DeepSeek-R1-0528) ToolBench (https://github.com/OpenBMB/ToolBench) Qwen3-235B-A22B (https://huggingface.co/Qwen/Qwen3-235B-A22B) Nemotron Content Safety Dataset V2 (https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0) eval-safety (https://github.com/CrystalEye42/eval-safety/blob/main/malicious_tasks_dataset.yaml) Gretel Synthetic Safety Alignment (https://huggingface.co/datasets/gretelai/gretel-safety-alignment-en-v1) RedTeam_2K (https://huggingface.co/datasets/JailbreakV-28K/JailBreakV-28k/viewer/RedTeam_2K) DeepSeek-R1-0528 (https://huggingface.co/deepseek-ai/DeepSeek-R1-0528) HelpSteer3 (https://huggingface.co/datasets/nvidia/HelpSteer3) HelpSteer2 (https://huggingface.co/datasets/nvidia/HelpSteer2) Qwen3-235B-A22B (https://huggingface.co/Qwen/Qwen3-235B-A22B) HelpSteer2 (https://huggingface.co/datasets/nvidia/HelpSteer2) C4 (https://huggingface.co/datasets/allenai/c4) LMSYS-Chat-1M (https://huggingface.co/datasets/lmsys/lmsys-chat-1m) ShareGPT52K (https://huggingface.co/datasets/RyokoAI/ShareGPT52K) tigerbot-kaggle-leetcodesolutions-en-2k (https://huggingface.co/datasets/TigerResearch/tigerbot-kaggle-leetcodesolutions-en-2k) GSM8K (https://github.com/openai/grade-school-math) PRM800K (https://github.com/openai/prm800k) FinQA (https://finqasite.github.io/) WikiTableQuestions (https://huggingface.co/datasets/wikitablequestions) Riddles (https://github.com/crawsome/riddles) glaive-function-calling-v2 (https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) SciBench (https://github.com/mandyyyyii/scibench) OpenBookQA (https://github.com/allenai/OpenBookQA) Advanced Reasoning Benchmark (https://github.com/TheDuckAI/arb) Public Software Heritage S3 (https://docs.softwareheritage.org/devel/swh-export/graph/dataset.html#summary-of-dataset-versions) Khan Academy Math Keywords (https://www.khanacademy.org/math) Nemotron-4-340B-Base (https://huggingface.co/nvidia/Nemotron-4-340B-Base) Nemotron-4-340B-Instruct (https://huggingface.co/nvidia/Nemotron-4-340B-Instruct) Nemotron-4-340B-Reward (https://huggingface.co/nvidia/Nemotron-4-340B-Reward) Mixtral-8x7B-Instruct-v0.1 (https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) Mixtral-8x22B-Instruct-v0.1 (https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1) LMSYS-Chat-1M (https://huggingface.co/datasets/lmsys/lmsys-chat-1m) Qwen3-235B-A22B (https://huggingface.co/Qwen/Qwen3-235B-A22B) OpenMathReasoning (https://huggingface.co/datasets/nvidia/OpenMathReasoning) OpenCodeReasoning (https://huggingface.co/datasets/nvidia/OpenCodeReasoning) DeepSeek-R1-0528 (https://huggingface.co/deepseek-ai/DeepSeek-R1-0528) Qwen2.5-32B-Instruct-AWQ (https://huggingface.co/Qwen/Qwen2.5-32B-Instruct-AWQ) Qwen2.5-14B-Instruct (https://huggingface.co/Qwen/Qwen2.5-14B-Instruct) WildChat (https://huggingface.co/datasets/allenai/WildChat-1M) Qwen3-235B-A22B (https://huggingface.co/Qwen/Qwen3-235B-A22B) Gemma 3 PT 12B (https://huggingface.co/google/gemma-3-12b-it) Gemma 3 PT 27B (https://huggingface.co/google/gemma-3-27b-it) Evaluation Dataset: ------------------- • Data Collection Method by dataset: Hybrid: Human, Synthetic • Labeling Method by dataset: Hybrid: Automated, Human, Synthetic Inference ========= • Engines: vLLM Engines: vLLM • Test Hardware NVIDIA: 1*H100. Test Hardware NVIDIA: 1*H100. Ethical Considerations ====================== NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with ourTrustworthy AIterms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Trustworthy AI (https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fagreements%2Ftrustworthy-ai%2Fterms%2F&data=05%7C02%7Cbsimkin%40nvidia.com%7Cdb502dd5d52f4a725b6e08ddd9be6f84%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638906134164797524%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=2VL7oGfJbFtEnSoL4deMIQVwOCzzZ8bfm0wUDGjazk0%3D&reserved=0) terms of service (https://www.google.com/url?q=https://nam11.safelinks.protection.outlook.com/?url%3Dhttps%253A%252F%252Fwww.nvidia.com%252Fen-us%252Fagreements%252Ftrustworthy-ai%252Fterms%252F%26data%3D05%257C02%257Cbsimkin%2540nvidia.com%257Cdb502dd5d52f4a725b6e08ddd9be6f84%257C43083d15727340c1b7db39efd9ccc17a%257C0%257C0%257C638906134164797524%257CUnknown%257CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%253D%253D%257C0%257C%257C%257C%26sdata%3D2VL7oGfJbFtEnSoL4deMIQVwOCzzZ8bfm0wUDGjazk0%253D%26reserved%3D0&sa=D&source=docs&ust=1755037828403815&usg=AOvVaw1E6mrSVK3syZSu-5NhIR_l) For more detailed information on ethical considerations for this model, please see the Model Card++ Bias, Explainability, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "TRELLIS",
    "nvidia_url": "https://build.nvidia.com/microsoft/trellis",
    "scraped_on": "2025-10-15T21:38:27.696594",
    "tags": [
      "image-to-3d",
      "run-on-rtx",
      "text-to-3d"
    ],
    "model_card": "Overview ======== Description: ============ MSFT TRELLIS 3D is an asset generation model capable of producing detailed meshes directly from text prompts or images. With multiple size variants, TRELLIS offers options for users aiming to maximize quality and/or speed. This model is ready for non-commercial/commercial use. Third-Party Community Consideration: ==================================== This model is not owned or developed by NVIDIA. This model has been developed and built to a third-party’s requirements for this application and use case; see link to: • microsoft/TRELLIS-image-large Model Card microsoft/TRELLIS-image-large Model Card (https://huggingface.co/microsoft/TRELLIS-image-large) • microsoft/TRELLIS-text-large Model Card microsoft/TRELLIS-text-large Model Card (https://huggingface.co/microsoft/TRELLIS-text-large) License/Terms of Use -------------------- GOVERNING TERMS: This trial service is governed by theNVIDIA API Trial Terms of Service. Use of this model is governed by theNVIDIA Community Model License. Additional Information: MIT license. NVIDIA API Trial Terms of Service (https://assets.ngc.nvidia.com/products/api-catalog/legal/NVIDIA%20API%20Trial%20Terms%20of%20Service.pdf) NVIDIA Community Model License (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) Deployment Geography: --------------------- Global Use Case: ========= Creators and professionals can use this model to generate high-quality images from text prompts, simplifying visual communication. Release Date: ------------- • Build.Nvidia.com September 2, 2025 viahttps://build.nvidia.com/microsoft/trellis https://build.nvidia.com/microsoft/trellis • Huggingface December 2, 2024 viahttps://huggingface.co/microsoft/TRELLIS-image-large https://huggingface.co/microsoft/TRELLIS-image-large References ---------- • TRELLIS project page TRELLIS project page (https://microsoft.github.io/TRELLIS/) Model Architecture: =================== Architecture Type: TransformerNetwork Architecture: Sparse Flow TransformerNumber of model parameters: TRELLIS-image-large-1.2B, TRELLIS-text-base-342M, TRELLIS-text-large 1.1B. Architecture Type Network Architecture Number of model parameters Input: ====== Input Type: Text, ImageInput Parameters: Text: One-Dimensional (1D); Image: Two-Dimensional (2D)Input Format: Text: String. Image: Red, Green, Blue (RGB)Other Properties Related to Input: Steps, Classifier-Free Guidance Scale and Seed Input Type Input Parameters Input Format Other Properties Related to Input Output: ======= Output Type: 3D ObjectOutput Parameters: Three-Dimensional (3D)Output Format: Graphics Library Binary (GLB) Output Type Output Parameters Output Format Software Integration: ===================== Runtime Engines: • Pytorch Supported Hardware Platforms: Supported Hardware Platforms • NVIDIA Blackwell • NVIDIA Hopper • NVIDIA Lovelace Supported Operating Systems: Linux, Windows Subsystem for Linux Supported Operating Systems The integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment. Model Version(s): ================= • TRELLIS-image-large • TRELLIS-text-large Training, Testing, and Evaluation Datasets: =========================================== Training Dataset: ================= Link:https://huggingface.co/datasets/JeffreyXiang/TRELLIS-500K Link: https://huggingface.co/datasets/JeffreyXiang/TRELLIS-500K Data Modality:Image, Text, 3D Objects Data Modality: • Data Collection Method by dataset: Automated • Labeling Method by dataset: Automated Properties (Quantity, Dataset Descriptions, Sensor(s)):TRELLIS-500K is a dataset of 500K 3D assets curated from Objaverse(XL), ABO, 3D-FUTURE, HSSD, and Toys4k, filtered based on aesthetic scores. Properties (Quantity, Dataset Descriptions, Sensor(s)): Testing Dataset: ================ Link:https://huggingface.co/datasets/JeffreyXiang/TRELLIS-500K Link: https://huggingface.co/datasets/JeffreyXiang/TRELLIS-500K Data Modality:Image, Text, 3D Objects Data Modality: • Data Collection Method by dataset: Automated • Labeling Method by dataset: Automated Properties (Quantity, Dataset Descriptions, Sensor(s)):TRELLIS-500K is a dataset of 500K 3D assets curated from Objaverse(XL), ABO, 3D-FUTURE, HSSD, and Toys4k, filtered based on aesthetic scores. Properties (Quantity, Dataset Descriptions, Sensor(s)): Evaluation Dataset: =================== Link:https://huggingface.co/datasets/JeffreyXiang/TRELLIS-500K Link: https://huggingface.co/datasets/JeffreyXiang/TRELLIS-500K Data Modality:Image, Text, 3D Objects Data Modality: • Data Collection Method by dataset: Automated • Labeling Method by dataset: Automated Properties (Quantity, Dataset Descriptions, Sensor(s)):TRELLIS-500K is a dataset of 500K 3D assets curated from Objaverse(XL), ABO, 3D-FUTURE, HSSD, and Toys4k, filtered based on aesthetic scores. Properties (Quantity, Dataset Descriptions, Sensor(s)): Inference: ========== Engine: PytorchTest Hardware: L40S Engine Test Hardware Ethical Considerations: ======================= NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report model quality, risk, security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "parakeet-ctc-0.6b-vi",
    "nvidia_url": "https://build.nvidia.com/nvidia/parakeet-ctc-0_6b-vi",
    "scraped_on": "2025-10-15T21:38:24.482081",
    "tags": [
      "asr",
      "nvidia nim",
      "streaming",
      "vietnamese"
    ],
    "model_card": "Model Overview ============== RIVA Parakeet-CTC-0.6B-Unified ASR Vietnamese (around 600M parameters) [1] is trained on an ASR dataset with over 4,000 hours of Vietnamese (vi) speech. The model transcribes speech in Vietnamese, in upper case and lower case alphabets, along with punctuations (period, comma, and question mark), and spaces. This model is ready for commercial/non-commercial use. Terms of use ============ GOVERNING TERMS: Use of this model is governed by theNVIDIA Community Model License Agreement. NVIDIA Community Model License Agreement (https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/) References ========== • Fast Conformer Fast Conformer (https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) Model Architecture ================== Architecture Type:Parakeet-CTC (also known as FastConformer-CTC) which is an optimized version of Conformer model with 8x depthwise-separable convolutional downsampling with CTC loss.Network Architecture:Parakeet-CTC-0.6B Architecture Type: Network Architecture: Input ===== Input Type(s):Audio in Vietnamese + EnglishInput Format(s):Linear PCM 16-bit 1 channelInput Parameters:One-Dimensional (1D) Input Type(s): Input Format(s): Input Parameters: Output ====== Output Type(s):Text String in Vietnamese + English with TimestampsOutput Format(s):StringOutput Parameters:One-Dimensional (1D) Output Type(s): Output Format(s): Output Parameters: Supported Operating System(s): • Linux Inference ========= Engine:TritonTest Hardware: Engine: Test Hardware: • NVIDIA A2 • NVIDIA A10 • NVIDIA A16 • NVIDIA A30 • NVIDIA A40 • NVIDIA A100 • NVIDIA H100 • NVIDIA L4 • NVIDIA L40 • GeForce RTX 40xx • GeForce RTX 50xx • Blackwell RTX 60xx Ethical Considerations (For NVIDIA Models Only): ================================================ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  },
  {
    "name": "parakeet-ctc-0.6b-zh-cn",
    "nvidia_url": "https://build.nvidia.com/nvidia/parakeet-ctc-0_6b-zh-cn",
    "scraped_on": "2025-10-15T21:38:21.018236",
    "tags": [
      "asr",
      "mandarin",
      "nvidia nim",
      "speech-to-text",
      "streaming"
    ],
    "model_card": "Speech Recognition: Parakeet CTC 0.6b Mandarin English Code Switch Model ======================================================================== Description =========== RIVA Parakeet-CTC-XL-0.6B-Unified ASR Mandarin (around 600M parameters) [1] is trained on ASR Set with over 17,000 hours of Mandarin (zh-CN) and English (en-US) speech. The model transcribes speech in Mandarin and English, in upper case and lower case alphabets, along with punctuations (period, comma, and question mark), spaces, and apostrophes. This model is ready for commercial use. License/Terms of Use ==================== GOVERNING TERMS: The use of this model is governed by the NVIDIA Community Model License (found athttps://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/). https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/ References ========== [1]Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition[2]Fast-Conformer-CTC Model[3]Conformer: Convolution-augmented Transformer for Speech Recognition Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition (https://arxiv.org/abs/2305.05084) Fast-Conformer-CTC Model (https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html) Conformer: Convolution-augmented Transformer for Speech Recognition (https://arxiv.org/abs/2005.08100) Model Architecture ================== Architecture Type:Parakeet-CTC (also known as FastConformer-CTC) [1], [2] which is an optimized version of Conformer model [3] with 8x depthwise-separable convolutional downsampling with CTC lossNetwork Architecture:Parakeet-CTC-XL-0.6B Architecture Type: Network Architecture: Input ===== Input Type(s):AudioInput Format(s):wavInput Parameters:1-DimensionOther Properties Related to Input:Maximum Length in seconds specific to GPU Memory, No Pre-Processing Needed, Mono channel is required. Input Type(s): Input Format(s): Input Parameters: Other Properties Related to Input: Output ====== Output Type(s):TextOutput Format:String (in Mandarin and English)Output Parameters:1-DimensionOther Properties Related to Output:No Maximum Character Length, Does not handle special characters. Output Type(s): Output Format: Output Parameters: Other Properties Related to Output: Supported Operating System(s): • Linux Model Version(s): ================= Parakeet-CTC-XL-unified-0.6b_spe7k_zh-CN_3.0 Training & Evaluation ===================== Training Dataset ---------------- Data Collection Method by dataset • Human Labeling Method by dataset • Human Properties: This model is trained on over 17,000 hours of Mandarin (zh-CN) and English (en-US) speech, comprised of a dynamic blend of public and internal proprietary datasets normalized to have upper-cased, lower-cased, punctuated, and spoken forms in text. Evaluation Dataset ------------------ Data Collection Method by dataset • Human Labeling Method by dataset • Human Properties: A dynamic blend of public and internal proprietary datasets normalized to have upper-cased, lower-cased, punctuated, and spoken forms in text. Inference ========= Engine:TritonTest Hardware: Engine: Test Hardware: • NVIDIA A2 • NVIDIA A10 • NVIDIA A16 • NVIDIA A30 • NVIDIA A40 • NVIDIA A100 • NVIDIA H100 • NVIDIA L4 • NVIDIA L40 • GeForce RTX 40xx • GeForce RTX 50xx • Blackwell RTX 60xx Ethical Considerations (For NVIDIA Models Only): ================================================ NVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards. Please report security vulnerabilities or NVIDIA AI Concernshere. here (https://www.nvidia.com/en-us/support/submit-security-vulnerability/)"
  }
]